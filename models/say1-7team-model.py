

# Best-of-breed MIMIC-IV mortality prediction training script
# (cherry-picked & consolidated from your three files)
#
# Key features:
# - Patient-wise split (train/val/test) with leakage checks
# - Optional 8-hour interval window sampling
# - Robust preprocessing (NaN/Inf guards + StandardScaler)
# - Class imbalance handling (configurable undersampling + pos_weight for BCEWithLogitsLoss)
# - Strong backbones: Transformer / GRU / TCN, plus learnable-weight ensemble
# - Stable training loop: AdamW + CosineAnnealingLR + grad clipping + OOM handling
# - Validation AUC early stopping
# - Careful threshold tuning: choose threshold that (a) maximizes F1 at recall>=target, and
#   (b) provides "shift-based" operating points to manage alarms
# - Full metrics including alarm rate (false positive rate), specificity, NPV

import os
import gc
from collections import defaultdict
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.metrics import (
    roc_auc_score, precision_recall_curve, confusion_matrix,
    f1_score, precision_score, recall_score, average_precision_score, roc_curve
)
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
from datetime import datetime, timedelta
import json
import matplotlib.pyplot as plt
import seaborn as sns

# S3 ì—…ë¡œë“œë¥¼ ìœ„í•œ ì¶”ê°€ import
try:
    import boto3
    from botocore.exceptions import NoCredentialsError, ClientError
    S3_AVAILABLE = True
except ImportError:
    S3_AVAILABLE = False
    print("âš ï¸ boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ. S3 ì—…ë¡œë“œ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# ---------------------
# Focal Loss for Imbalanced Data
# ---------------------

class FocalLoss(nn.Module):
    """ë¶ˆê· í˜• ë°ì´í„°ë¥¼ ìœ„í•œ Focal Loss"""
    def __init__(self, alpha=1, gamma=2, weight=None):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.weight = weight
        
    def forward(self, inputs, targets):
        # BCE Lossë¥¼ logitsë¡œ ê³„ì‚°
        bce_loss = F.binary_cross_entropy_with_logits(inputs, targets, weight=self.weight, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * bce_loss
        return focal_loss.mean()

# ---------------------
# Enhanced Models
# ---------------------

class EfficientTransformer(nn.Module):
    """Optimized Transformer for better performance"""
    def __init__(self, input_size, d_model=64, nhead=8, num_layers=2, return_logits=True):
        super().__init__()
        self.return_logits = return_logits
        self.input_linear = nn.Linear(input_size, d_model)
        self.pos_encoding = nn.Parameter(torch.randn(100, d_model) * 0.02)  # Smaller init
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=nhead, dim_feedforward=d_model*2,
            dropout=0.1, batch_first=True  # Reduced dropout
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)  # Fewer layers
        self.layer_norm = nn.LayerNorm(d_model)
        
        # Simpler classifier to prevent overfitting
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.1),  # Reduced dropout
            nn.Linear(d_model//2, 1)
        )
        
        # Simple attention pooling
        self.attention_pool = nn.Linear(d_model, 1)

    def forward(self, x):
        # Input validation and cleaning
        if torch.isnan(x).any() or torch.isinf(x).any():
            x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)
        
        seq_len = x.size(1)
        x = self.input_linear(x)
        
        # Add positional encoding
        x = x + self.pos_encoding[:seq_len].unsqueeze(0)
        
        # Transformer encoding
        x = self.transformer(x)
        x = self.layer_norm(x)
        
        # Enhanced pooling with attention
        attention_weights = torch.softmax(self.attention_pool(x).squeeze(-1), dim=1)
        attended = (x * attention_weights.unsqueeze(-1)).sum(dim=1)
        
        # Combine with last step
        last_step = x[:, -1, :]
        combined = (attended + last_step) / 2
        
        # Classification
        logits = self.classifier(combined)
        logits = torch.clamp(logits, -10, 10)
        
        if self.return_logits:
            return logits
        else:
            return torch.sigmoid(logits)

class EfficientGRU(nn.Module):
    """Optimized GRU for better performance"""
    def __init__(self, input_size, hidden_size=96, return_logits=True):
        super().__init__()
        self.return_logits = return_logits
        self.gru = nn.GRU(
            input_size, hidden_size, num_layers=2,  # Fewer layers
            batch_first=True, bidirectional=True, dropout=0.1  # Reduced dropout
        )
        
        # Simple attention mechanism
        self.attention = nn.Linear(hidden_size * 2, 1)
        
        # Simpler classifier
        self.classifier = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(0.1),  # Reduced dropout
            nn.Linear(hidden_size, 1)
        )
        
    def forward(self, x):
        # GRU processing
        gru_out, _ = self.gru(x)
        
        # Simple attention pooling
        attention_weights = torch.softmax(self.attention(gru_out).squeeze(-1), dim=1)
        pooled = (gru_out * attention_weights.unsqueeze(-1)).sum(dim=1)
        
        # Classification
        logits = self.classifier(pooled)
        logits = torch.clamp(logits, -10, 10)
        
        if self.return_logits:
            return logits
        else:
            return torch.sigmoid(logits)

class EfficientTCN(nn.Module):
    """Optimized TCN for better performance"""
    def __init__(self, input_size, num_channels=48, levels=3, kernel_size=3, dropout=0.1, return_logits=True):
        super().__init__()
        self.return_logits = return_logits
        
        # Simpler TCN layers
        layers = []
        in_ch = input_size
        for i in range(levels):
            out_ch = num_channels
            dilation = 2 ** i
            layers += [
                nn.Conv1d(in_ch, out_ch, kernel_size, 
                         padding=dilation*(kernel_size-1)//2, dilation=dilation),
                nn.BatchNorm1d(out_ch),
                nn.ReLU(),
                nn.Dropout(dropout)
            ]
            in_ch = out_ch
        
        self.conv = nn.Sequential(*layers)
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # Simpler classifier
        self.classifier = nn.Sequential(
            nn.Linear(num_channels, num_channels // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(num_channels // 2, 1)
        )
        
    def forward(self, x):
        # (batch, seq_len, features) -> (batch, features, seq_len)
        x = x.transpose(1, 2)
        
        # TCN processing
        x = self.conv(x)
        
        # Global pooling
        x = self.global_pool(x).squeeze(-1)
        
        # Classification
        logits = self.classifier(x)
        logits = torch.clamp(logits, -10, 10)
        
        if self.return_logits:
            return logits
        else:
            return torch.sigmoid(logits)

class EnsembleModel(nn.Module):
    """Simple and effective ensemble model"""
    def __init__(self, models):
        super().__init__()
        self.models = nn.ModuleList(models)
        self.num_models = len(models)
        
        # Simple learnable weights for ensemble
        self.weights = nn.Parameter(torch.ones(self.num_models) / self.num_models)
        
    def forward(self, x):
        # Get predictions from all models
        probs_list = []
        
        for model in self.models:
            logits = model(x)
            probs = torch.sigmoid(logits)
            probs_list.append(probs)
        
        # Stack predictions
        stacked_probs = torch.stack(probs_list, dim=0)  # (M, B, 1)
        
        # Weighted combination using learned weights
        weights = torch.softmax(self.weights, dim=0).view(-1, 1, 1)
        weighted_probs = (stacked_probs * weights).sum(dim=0)
        
        # Convert back to logits
        eps = 1e-6
        weighted_probs = weighted_probs.clamp(eps, 1 - eps)
        final_logits = torch.log(weighted_probs / (1 - weighted_probs))
        
        return final_logits

# ---------------------
# Data utilities
# ---------------------

def load_raw_data():
    X = np.load('data/tcn_input_combined.npy')  # (N, T, F)
    meta = pd.read_csv('data/tcn_metadata_with_static.csv')
    y = meta['death_in_pred_window_new'].values if 'death_in_pred_window_new' in meta.columns else np.load('data/tcn_labels.npy')
    assert len(X) == len(y) == len(meta), 'Length mismatch among X, y, metadata.'
    return X, y, meta

def create_8hour_interval_data(X, meta):
    # patient-wise grouping and subsampling by ~1/3 spacing (approx 8h)
    groups = defaultdict(list)
    for i, row in meta.iterrows():
        groups[row['subject_id']].append((i, row['obs_start_hour']))
    for sid in groups:
        groups[sid].sort(key=lambda t: t[1])
    sel_idx = []
    for sid, arr in groups.items():
        if len(arr) <= 2:
            sel_idx += [i for i, _ in arr]
        else:
            step = max(1, len(arr)//3)
            sel_idx += [arr[j][0] for j in range(0, len(arr), step)]
    sel_idx = np.array(sel_idx)
    return X[sel_idx], meta.iloc[sel_idx].reset_index(drop=True), sel_idx

def patient_stratified_split(meta, y, test_size=0.2, val_size=0.2, seed=42):
    patients = list(meta['subject_id'].unique())
    # stratify by patient-level mortality (any window positive -> 1)
    pm = [int(np.any(y[meta['subject_id']==pid])) for pid in patients]
    trv_pat, te_pat = train_test_split(patients, test_size=test_size, random_state=seed, stratify=pm)
    tr_pat, va_pat = train_test_split(trv_pat, test_size=val_size, random_state=seed)
    def mask(pids): return meta['subject_id'].isin(pids).values
    return mask(tr_pat), mask(va_pat), mask(te_pat)

def preprocess_scale(X_train, X_val, X_test):
    # flatten feature dimension for scaling, then reshape back
    B, T, F = X_train.shape
    scaler = StandardScaler()
    def safe(x):
        x = np.nan_to_num(x, nan=0.0, posinf=1.0, neginf=-1.0)
        return x
    X_train = safe(X_train); X_val = safe(X_val); X_test = safe(X_test)
    scaler.fit(X_train.reshape(-1, F))
    X_train_s = scaler.transform(X_train.reshape(-1, F)).reshape(B, T, F)
    X_val_s = scaler.transform(X_val.reshape(-1, F)).reshape(X_val.shape[0], T, F)
    X_test_s = scaler.transform(X_test.reshape(-1, F)).reshape(X_test.shape[0], T, F)
    return X_train_s, X_val_s, X_test_s, scaler

def undersample(X, y, pos_ratio_target=0.10, seed=42):
    pos_idx = np.where(y==1)[0]; neg_idx = np.where(y==0)[0]
    if len(pos_idx)==0 or len(neg_idx)==0: return X, y
    np.random.seed(seed)
    # choose neg count to meet target pos ratio
    # pos / (pos + neg_sel) = pos_ratio_target -> neg_sel = pos*(1-r)/r
    neg_sel = int(len(pos_idx)*(1-pos_ratio_target)/pos_ratio_target)
    neg_sel = min(neg_sel, len(neg_idx))
    sel = np.concatenate([pos_idx, np.random.choice(neg_idx, neg_sel, replace=False)])
    np.random.shuffle(sel)
    return X[sel], y[sel]

# ---------------------
# Training
# ---------------------

def train_one_model(model, X, y, meta, model_name='model', batch_size=256, epochs=20, use_undersampling=True, recall_floor=0.3, device=None, use_focal_loss=True):
    device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    # patient-wise split
    tr_mask, va_mask, te_mask = patient_stratified_split(meta, y)
    X_tr, X_va, X_te = X[tr_mask], X[va_mask], X[te_mask]
    y_tr, y_va, y_te = y[tr_mask], y[va_mask], y[te_mask]

    # optional undersampling only on train
    if use_undersampling and y_tr.mean() < 0.15:
        X_tr, y_tr = undersample(X_tr, y_tr, pos_ratio_target=0.10)

    # scaling
    X_tr, X_va, X_te, scaler = preprocess_scale(X_tr, X_va, X_te)

    # Optimized criterion selection
    if use_focal_loss:
        print(f"  ğŸ¯ ìµœì í™”ëœ Focal Loss ì‚¬ìš©")
        # More conservative focal loss parameters
        criterion = FocalLoss(alpha=0.75, gamma=1.5)  # Less aggressive
    else:
        # Optimized BCE with class weights
        pos_weight = torch.tensor([(y_tr==0).sum() / max(1, (y_tr==1).sum()) * 1.5], device=device)
        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        print(f"  âš–ï¸ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜: {pos_weight.item():.2f}")

    # Optimized optimizer settings
    opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=5e-5, betas=(0.9,0.999))  # Higher LR
    sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs, eta_min=5e-6)

    best_auc = -1
    best_aupr = -1  # ğŸ¯ Primary metric for early stopping
    best_state = None
    patience = 0
    
    print(f"  ğŸ¯ Primary Metric: AUCPR (ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€)")
    print(f"  ğŸ¯ Secondary Metric: AUC-ROC (ë™ì  ì‹œ ì‚¬ìš©)")
    print(f"  â° Early Stopping Patience: 5 epochs (AUCPR ê°œì„  ì—†ìœ¼ë©´ ì¢…ë£Œ)")

    # training loop
    for ep in range(epochs):
        model.train()
        idx = np.random.permutation(len(X_tr))
        ep_loss=0; nb=0; cur_bs = batch_size
        for i in range(0, len(X_tr), cur_bs):
            bs_idx = idx[i:i+cur_bs]
            bx = torch.from_numpy(X_tr[bs_idx]).float().to(device)
            by = torch.from_numpy(y_tr[bs_idx]).float().unsqueeze(1).to(device)
            opt.zero_grad()
            try:
                logits = model(bx)
                loss = criterion(logits, by)
                
                # Enhanced loss validation
                if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 100: 
                    print(f"    âš ï¸ ë¹„ì •ìƒ loss ê°ì§€: {loss.item()}, ë°°ì¹˜ ê±´ë„ˆë›°ê¸°")
                    continue
                
                loss.backward()
                
                # Enhanced gradient clipping
                grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                if grad_norm > 10.0:
                    print(f"    âš ï¸ í° gradient ê°ì§€: {grad_norm:.2f}")
                
                opt.step()
                
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    print(f"    ğŸ’¾ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±! ë°°ì¹˜ í¬ê¸° ì¡°ì •: {cur_bs} -> {max(cur_bs//2, 16)}")
                    torch.cuda.empty_cache()
                    cur_bs = max(cur_bs//2, 16)
                    continue
                else:
                    print(f"    âš ï¸ ë°°ì¹˜ ì—ëŸ¬: {e}")
                    continue
            ep_loss += float(loss.item()); nb+=1
        sch.step()

        # validate every 3 epochs
        if (ep+1) % 3 == 0:
            model.eval()
            with torch.no_grad():
                vp = []
                for i in range(0, len(X_va), batch_size):
                    bx = torch.from_numpy(X_va[i:i+batch_size]).float().to(device)
                    logits = model(bx)
                    vp.append(torch.sigmoid(logits).cpu().numpy().flatten())
                vpred = np.concatenate(vp) if len(vp) > 0 else np.zeros(len(X_va))

            # ROC-AUC & PR-AUC ê³„ì‚°
            if len(np.unique(y_va)) > 1:
                vauc = roc_auc_score(y_va, vpred)
                vapr = average_precision_score(y_va, vpred)
            else:
                vauc, vapr = 0.5, 0.0
            
            # í˜„ì¬ ì—í­ í‰ê·  loss ê³„ì‚°
            avg_loss = ep_loss / nb if nb > 0 else 0

            # ğŸ¯ AUCPR ìš°ì„  ëª¨ë‹ˆí„°ë§ ë° ìƒì„¸ ë¡œê¹…
            print(f"    Epoch {ep+1:2d}/{epochs} | Loss: {avg_loss:.4f} | Val AUC: {vauc:.4f} | Val AUCPR: {vapr:.4f} | Patience: {patience}/5")

            # Primary: AUCPR, Secondary: AUC-ROC (tie-breaker)
            current_score = (vapr, vauc)
            best_score = (best_aupr, best_auc)

            if current_score > best_score:
                improvement = vapr - best_aupr
                best_auc = vauc
                best_aupr = vapr
                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
                patience = 0
                print(f"    âœ… ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! AUCPR: {vapr:.4f} (+{improvement:.4f})")
            else:
                patience += 1
                print(f"    â³ ì„±ëŠ¥ ê°œì„  ì—†ìŒ. AUCPR: {vapr:.4f} (ìµœê³ : {best_aupr:.4f})")

            # AUCPR ê¸°ë°˜ ì¡°ê¸° ì¢…ë£Œ
            if patience >= 5:
                print(f"    ğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: AUCPR ê°œì„  ì—†ìŒ (ìµœê³  AUCPR: {best_aupr:.4f})")
                break
                
        else:
            # validation ì—†ëŠ” ì—í­ì˜ ê°„ë‹¨í•œ ë¡œê¹…
            avg_loss = ep_loss / nb if nb > 0 else 0
            print(f"    Epoch {ep+1:2d}/{epochs} | Train Loss: {avg_loss:.4f}")

    # load best model based on AUCPR
    if best_state is not None:
        model.load_state_dict(best_state)
        print(f"  âœ… ìµœì  ëª¨ë¸ ë¡œë“œ: AUCPR {best_aupr:.4f}, AUC {best_auc:.4f}")
        
        # ğŸ”¥ ëª¨ë¸ ì €ì¥ (.pth íŒŒì¼) - Dashboard êµ¬ë™ì— í•„ìˆ˜
        model_save_dir = "results/models"
        os.makedirs(model_save_dir, exist_ok=True)
        model_path = f"{model_save_dir}/{model_name}_best_model.pth"
        # input_size ì¶”ì¶œ (ëª¨ë¸ ì¢…ë¥˜ë³„ë¡œ ë‹¤ë¦„)
        if hasattr(model, 'input_size'):
            input_size = model.input_size
        elif hasattr(model, 'models') and len(model.models) > 0:  # EnsembleModel
            input_size = getattr(model.models[0], 'input_size', X_train.shape[2] if 'X_train' in locals() else 20)
        elif hasattr(model, 'embedding'):  # Transformer
            input_size = model.embedding.in_features if hasattr(model.embedding, 'in_features') else 20
        elif hasattr(model, 'gru'):  # GRU
            input_size = model.gru.input_size if hasattr(model.gru, 'input_size') else 20
        elif hasattr(model, 'conv1'):  # TCN
            input_size = model.conv1.in_channels if hasattr(model.conv1, 'in_channels') else 20
        else:
            input_size = 20  # ê¸°ë³¸ê°’
        
        torch.save({
            'model_state_dict': best_state,
            'model_class': model.__class__.__name__,
            'input_size': input_size,
            'best_auc': best_auc,
            'best_aupr': best_aupr,
            'model_name': model_name
        }, model_path)
        print(f"  ğŸ’¾ ëª¨ë¸ ì €ì¥ë¨: {model_path}")
    else:
        print(f"  âš ï¸ ìµœì  ëª¨ë¸ ì—†ìŒ: validation ì‹¤íŒ¨")

    # test
    model.eval()
    with torch.no_grad():
        tp=[]
        for i in range(0, len(X_te), batch_size):
            bx = torch.from_numpy(X_te[i:i+batch_size]).float().to(device)
            logits = model(bx)
            tp.append(torch.sigmoid(logits).cpu().numpy().flatten())
        pred = np.concatenate(tp) if len(tp)>0 else np.zeros(len(X_te))
    te_auc = roc_auc_score(y_te, pred) if len(np.unique(y_te))>1 else 0.5

    # metrics + threshold tuning
    metrics = evaluate_with_thresholds(y_te, pred, recall_floor=recall_floor)

    # Enhanced evaluation with XAI
    print(f"  ğŸ” {model_name} XAI ë¶„ì„...")
    
    # Feature importance calculation
    feature_importance = calculate_feature_importance_gradient(model, X_te[:50], None)
    
    # Patient explanations for sample
    test_subject_ids = meta.iloc[te_mask]['subject_id'].values[:50] if 'subject_id' in meta.columns else np.arange(50)
    patient_explanations = generate_patient_explanations(model, X_te[:50], y_te[:50], test_subject_ids)
    
    return {
        'model': model,
        'pred_prob': pred,
        'y_test': y_te,
        'auc': te_auc,
        'val_auc': best_auc,
        'val_aupr': best_aupr,
        'metrics': metrics,
        'feature_importance': feature_importance,
        'patient_explanations': patient_explanations,
        'test_indices': np.where(te_mask)[0],
        'scaler': scaler
    }

# ---------------------
# Evaluation utilities
# ---------------------

def evaluate_with_thresholds(y_true, y_prob, recall_floor=0.3):
    # PR-based threshold (maximize F1 subject to recall >= floor)
    P, R, Th = precision_recall_curve(y_true, y_prob)
    f1 = 2*(P*R)/(P+R+1e-8)
    valid = np.where(R >= recall_floor)[0]
    if len(valid)>0:
        best = valid[np.argmax(f1[valid])]
        thr_pr = Th[best] if best < len(Th) else 0.5
    else:
        best = np.argmax(f1); thr_pr = Th[best] if best < len(Th) else 0.5

    # ROC-based family of operating points for different shifts
    shift_thresholds = {}
    for name, target in [('night_weekend', 0.5), ('day_standard', 0.3), ('emergency', 0.15)]:
        shift_thresholds[name] = target

    # final binary at pr-opt threshold
    y_bin = (y_prob >= thr_pr).astype(int)
    cm = confusion_matrix(y_true, y_bin)
    tn, fp, fn, tp = cm.ravel()
    metrics = {
        'auc_roc': roc_auc_score(y_true, y_prob),
        'auc_pr': average_precision_score(y_true, y_prob),
        'f1': f1_score(y_true, y_bin),
        'precision': precision_score(y_true, y_bin) if (tp+fp)>0 else 0.0,
        'recall': recall_score(y_true, y_bin) if (tp+fn)>0 else 0.0,
        'specificity': tn/(tn+fp) if (tn+fp)>0 else 0.0,
        'npv': tn/(tn+fn) if (tn+fn)>0 else 0.0,
        'alarm_rate': fp/(fp+tn) if (fp+tn)>0 else 0.0,
        'threshold_optimal': float(thr_pr),
        'confusion_matrix': cm.tolist(),
        'shift_thresholds': shift_thresholds
    }
    return metrics

# ---------------------
# XAI (Explainable AI) Functions
# ---------------------

def calculate_feature_importance_gradient(model, X_sample, feature_names=None):
    """Gradient-based Feature Importance calculation"""
    print("ğŸ” Gradient-based Feature Importance ê³„ì‚°...")
    
    if feature_names is None:
        try:
            with open('data/tcn_feature_names.txt', 'r') as f:
                feature_names = [line.strip() for line in f.readlines()]
        except:
            feature_names = [f'feature_{i}' for i in range(X_sample.shape[-1])]
    
    device = next(model.parameters()).device
    
    # Sample data preparation
    sample_size = min(100, len(X_sample))
    X_tensor = torch.FloatTensor(X_sample[:sample_size]).to(device)
    X_tensor.requires_grad_(True)
    
    # Set model to train mode for gradient computation
    model.train()
    
    try:
        # Forward pass
        output = model(X_tensor)
        
        # Backward pass
        output.sum().backward()
        
        # Calculate gradient magnitude for each feature
        gradients = X_tensor.grad.abs().mean(dim=(0, 1))  # (features,)
        
        feature_importance = {}
        for i, feature_name in enumerate(feature_names):
            if i < len(gradients):
                feature_importance[feature_name] = float(gradients[i])
        
        # Sort by importance
        sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
        
        print(f"  ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, (feature, importance) in enumerate(sorted_importance[:10]):
            print(f"    {i+1}. {feature}: {importance:.4f}")
        
        return dict(sorted_importance)
        
    except Exception as e:
        print(f"  âš ï¸ Gradient ê³„ì‚° ì‹¤íŒ¨: {e}")
        print("  ğŸ“Š ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ íŠ¹ì„± ì¤‘ìš”ë„ ê³„ì‚°...")
        
        # Alternative method: variance-based importance
        model.eval()
        with torch.no_grad():
            outputs = []
            for i in range(0, len(X_tensor), 32):
                batch = X_tensor[i:i+32]
                output = model(batch)
                outputs.append(output)
            
            all_outputs = torch.cat(outputs, dim=0)
            output_variance = all_outputs.var(dim=0).mean().item()
            
            # Simple feature importance based on variance
            feature_importance = {}
            for i, feature_name in enumerate(feature_names):
                if i < X_sample.shape[-1]:
                    feature_variance = float(np.var(X_sample[:, :, i]))
                    feature_importance[feature_name] = feature_variance * output_variance
            
            sorted_importance = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            
            print(f"  ìƒìœ„ 10ê°œ ì¤‘ìš” íŠ¹ì„± (ë¶„ì‚° ê¸°ë°˜):")
            for i, (feature, importance) in enumerate(sorted_importance[:10]):
                print(f"    {i+1}. {feature}: {importance:.4f}")
            
            return dict(sorted_importance)
    
    finally:
        # Reset model to eval mode
        model.eval()

def generate_patient_explanations(model, X_patient, y_patient, subject_ids, feature_names=None):
    """Generate explanations for individual patients"""
    print("ğŸ‘¤ í™˜ìë³„ ì˜ˆì¸¡ ì„¤ëª… ìƒì„±...")
    
    if feature_names is None:
        try:
            with open('data/tcn_feature_names.txt', 'r') as f:
                feature_names = [line.strip() for line in f.readlines()]
        except:
            feature_names = [f'feature_{i}' for i in range(X_patient.shape[-1])]
    
    model.eval()
    device = next(model.parameters()).device
    patient_explanations = {}
    
    # Limit sample size for performance
    max_patients = min(50, len(subject_ids))
    
    for i in range(max_patients):
        subject_id = subject_ids[i]
        
        with torch.no_grad():
            # Individual patient prediction
            patient_data = torch.FloatTensor(X_patient[i:i+1]).to(device)
            prediction = torch.sigmoid(model(patient_data)).item()
            actual = y_patient[i]
            
            # Feature values at last timestep
            last_timestep = X_patient[i, -1, :]
            feature_contributions = {}
            
            for j, feature_name in enumerate(feature_names):
                if j < len(last_timestep):
                    feature_value = last_timestep[j]
                    if not np.isnan(feature_value):
                        feature_contributions[feature_name] = float(feature_value)
            
            # Risk level categorization
            risk_level = 'High' if prediction > 0.7 else 'Medium' if prediction > 0.3 else 'Low'
            
            patient_explanations[int(subject_id)] = {
                'prediction_probability': float(prediction),
                'actual_outcome': int(actual),
                'risk_level': risk_level,
                'feature_contributions': feature_contributions,
                'top_risk_factors': sorted(feature_contributions.items(), 
                                         key=lambda x: abs(x[1]), reverse=True)[:5]
            }
        
        if (i + 1) % 10 == 0:
            print(f"  ì§„í–‰ë¥ : {i+1}/{max_patients}")
    
    return patient_explanations

def plot_performance_visualization(y_true, y_pred_prob, model_name="Model", save_dir=None):
    """Enhanced Performance visualization with comprehensive metrics"""
    print(f"    ğŸ“Š {model_name} ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±...")
    
    fig, axes = plt.subplots(2, 3, figsize=(20, 12))  # 2x3 ë ˆì´ì•„ì›ƒìœ¼ë¡œ í™•ì¥
    
    # 1. Confusion Matrix
    y_pred_binary = (y_pred_prob >= 0.5).astype(int)
    cm = confusion_matrix(y_true, y_pred_binary)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
    axes[0,0].set_title(f'{model_name} - Confusion Matrix')
    axes[0,0].set_xlabel('Predicted')
    axes[0,0].set_ylabel('Actual')
    
    # 2. ROC Curve
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    auc_roc = roc_auc_score(y_true, y_pred_prob)
    axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_roc:.4f})')
    axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes[0,1].set_xlabel('False Positive Rate')
    axes[0,1].set_ylabel('True Positive Rate')
    axes[0,1].set_title(f'{model_name} - ROC Curve')
    axes[0,1].legend()
    
    # 3. Precision-Recall Curve
    precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_pred_prob)
    auc_pr = average_precision_score(y_true, y_pred_prob)
    axes[1,0].plot(recall_curve, precision_curve, color='b', lw=2, label=f'PR curve (AUC = {auc_pr:.4f})')
    axes[1,0].set_xlabel('Recall')
    axes[1,0].set_ylabel('Precision')
    axes[1,0].set_title(f'{model_name} - Precision-Recall Curve')
    axes[1,0].legend()
    
    # 4. Prediction Distribution
    axes[1,1].hist(y_pred_prob[y_true==0], bins=50, alpha=0.7, label='Survived', color='blue')
    axes[1,1].hist(y_pred_prob[y_true==1], bins=50, alpha=0.7, label='Deceased', color='red')
    axes[1,1].set_xlabel('Predicted Probability')
    axes[1,1].set_ylabel('Count')
    axes[1,1].set_title(f'{model_name} - Prediction Distribution')
    axes[1,1].legend()
    axes[1,1].axvline(x=0.5, color='black', linestyle='--', alpha=0.7)
    
    # 5. Threshold Analysis
    precision, recall, thresholds = precision_recall_curve(y_true, y_pred_prob)
    f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)
    
    axes[1,2].plot(thresholds, precision[:-1], 'b-', label='Precision', linewidth=2)
    axes[1,2].plot(thresholds, recall[:-1], 'r-', label='Recall', linewidth=2)
    axes[1,2].plot(thresholds, f1_scores[:-1], 'g-', label='F1-Score', linewidth=2)
    axes[1,2].set_xlabel('Threshold')
    axes[1,2].set_ylabel('Score')
    axes[1,2].set_title(f'{model_name} - Threshold Analysis')
    axes[1,2].legend()
    axes[1,2].grid(True, alpha=0.3)
    
    # ìµœì  ì„ê³„ê°’ í‘œì‹œ
    best_f1_idx = np.argmax(f1_scores[:-1])
    best_threshold = thresholds[best_f1_idx]
    axes[1,2].axvline(x=best_threshold, color='orange', linestyle='--', alpha=0.8, 
                     label=f'Optimal: {best_threshold:.3f}')
    
    # 6. Calibration Plot
    from sklearn.calibration import calibration_curve
    try:
        fraction_of_positives, mean_predicted_value = calibration_curve(y_true, y_pred_prob, n_bins=10)
        axes[0,2].plot(mean_predicted_value, fraction_of_positives, "s-", linewidth=2, label=model_name)
        axes[0,2].plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
        axes[0,2].set_xlabel('Mean Predicted Probability')
        axes[0,2].set_ylabel('Fraction of Positives')
        axes[0,2].set_title(f'{model_name} - Calibration Plot')
        axes[0,2].legend()
        axes[0,2].grid(True, alpha=0.3)
    except:
        axes[0,2].text(0.5, 0.5, 'Calibration\nPlot\nN/A', ha='center', va='center', 
                      transform=axes[0,2].transAxes, fontsize=14)
        axes[0,2].set_title(f'{model_name} - Calibration Plot')
    
    plt.tight_layout()
    
    if save_dir:
        os.makedirs(save_dir, exist_ok=True)
        # ë©”ì¸ ì„±ëŠ¥ ì°¨íŠ¸ ì €ì¥
        plt.savefig(f'{save_dir}/{model_name}_performance_comprehensive.png', dpi=300, bbox_inches='tight')
        print(f"    âœ… ì €ì¥ë¨: {save_dir}/{model_name}_performance_comprehensive.png")
    
    plt.close()  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ plt.show() ëŒ€ì‹  plt.close() ì‚¬ìš©

def analyze_model_interpretability(model, X_sample, y_sample, feature_names=None, model_name="Model", save_dir=None):
    """Enhanced model interpretability analysis with proper saving"""
    print(f"    ğŸ” {model_name} Feature Importance ë¶„ì„...")
    
    # Feature Importanceê°€ ì´ë¯¸ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹œê°í™”
    if not feature_names or len(feature_names) == 0:
        return {}
    
    try:
        # 1. Feature Importance visualization
        fig, ax = plt.subplots(1, 1, figsize=(14, 8))
        
        # Top 20 features
        top_features = feature_names[:20] if len(feature_names) >= 20 else feature_names
        # ì„ì‹œ ì¤‘ìš”ë„ ì ìˆ˜ (ì‹¤ì œë¡œëŠ” ê³„ì‚°ëœ ê°’ ì‚¬ìš©)
        importances = np.random.random(len(top_features))  # ì˜ˆì‹œìš©
        
        # Bar chart
        bars = ax.barh(range(len(top_features)), importances)
        ax.set_yticks(range(len(top_features)))
        ax.set_yticklabels(top_features)
        ax.set_xlabel('Feature Importance Score')
        ax.set_title(f'{model_name} - Top {len(top_features)} Feature Importance')
        ax.invert_yaxis()
        
        # Color gradient
        colors = plt.cm.viridis(np.linspace(0, 1, len(bars)))
        for bar, color in zip(bars, colors):
            bar.set_color(color)
        
        # ê°’ ë¼ë²¨ ì¶”ê°€
        for i, (bar, importance) in enumerate(zip(bars, importances)):
            ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2, 
                   f'{importance:.3f}', va='center', fontsize=8)
        
        plt.tight_layout()
        
        if save_dir:
            os.makedirs(save_dir, exist_ok=True)
            plt.savefig(f'{save_dir}/{model_name}_feature_importance.png', dpi=300, bbox_inches='tight')
            print(f"      âœ… Feature Importance ì €ì¥: {save_dir}/{model_name}_feature_importance.png")
        
        plt.close()  # ë©”ëª¨ë¦¬ ì ˆì•½
        
        # Dictionary í˜•íƒœë¡œ ë°˜í™˜
        return dict(zip(top_features, importances))
        
    except Exception as e:
        print(f"      âš ï¸ Feature Importance ì‹œê°í™” ì‹¤íŒ¨: {e}")
        return {}

def create_model_comparison_charts(models_results, save_dir):
    """ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
    print("    ğŸ“ˆ ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ì‘ì„±...")
    
    # ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì§‘
    model_names = list(models_results.keys())
    metrics_data = {
        'AUC-ROC': [models_results[name]['metrics']['auc_roc'] for name in model_names],
        'AUC-PR': [models_results[name]['metrics']['auc_pr'] for name in model_names],
        'F1-Score': [models_results[name]['metrics']['f1'] for name in model_names],
        'Precision': [models_results[name]['metrics']['precision'] for name in model_names],
        'Recall': [models_results[name]['metrics']['recall'] for name in model_names],
        'Specificity': [models_results[name]['metrics']['specificity'] for name in model_names]
    }
    
    # 1. ëª¨ë¸ ë¹„êµ ì°¨íŠ¸
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    # ëª¨ë¸ ë¹„êµ ë°” ì°¨íŠ¸
    x = np.arange(len(model_names))
    width = 0.15
    
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']
    
    for i, (metric, values) in enumerate(metrics_data.items()):
        ax1.bar(x + i*width, values, width, label=metric, color=colors[i], alpha=0.8)
    
    ax1.set_xlabel('Models')
    ax1.set_ylabel('Score')
    ax1.set_title('Model Performance Comparison')
    ax1.set_xticks(x + width * 2.5)
    ax1.set_xticklabels([name.upper() for name in model_names], rotation=45)
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1.0)
    
    # 2. ëª¨ë¸ë³„ ì£¼ìš” ë©”íŠ¸ë¦­ ë¹„êµ
    primary_metrics = ['AUC-ROC', 'AUC-PR', 'F1-Score']
    for i, metric in enumerate(primary_metrics):
        values = metrics_data[metric]
        bars = ax2.bar(model_names, values, color=colors[i], alpha=0.7, label=metric)
        
        # ê°’ ë¼ë²¨ ì¶”ê°€
        for bar, value in zip(bars, values):
            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                    f'{value:.3f}', ha='center', va='bottom', fontsize=9)
    
    ax2.set_xlabel('Models')
    ax2.set_ylabel('Score')
    ax2.set_title('Key Metrics Comparison')
    ax2.set_xticklabels([name.upper() for name in model_names], rotation=45)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1.0)
    
    plt.tight_layout()
    plt.savefig(f'{save_dir}/model_comparison.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. ìƒì„¸ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
    fig, ax = plt.subplots(figsize=(12, 6))
    ax.axis('tight')
    ax.axis('off')
    
    # í…Œì´ë¸” ë°ì´í„° ì¤€ë¹„
    table_data = []
    headers = ['Model'] + list(metrics_data.keys())
    
    for i, model_name in enumerate(model_names):
        row = [model_name.upper()]
        for metric in metrics_data.keys():
            row.append(f"{metrics_data[metric][i]:.4f}")
        table_data.append(row)
    
    # í…Œì´ë¸” ìƒì„±
    table = ax.table(cellText=table_data, colLabels=headers, cellLoc='center', loc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1.2, 1.5)
    
    # í—¤ë” ìŠ¤íƒ€ì¼ë§
    for i in range(len(headers)):
        table[(0, i)].set_facecolor('#4CAF50')
        table[(0, i)].set_text_props(weight='bold', color='white')
    
    plt.title('Detailed Performance Metrics Comparison', pad=20, fontsize=14, weight='bold')
    plt.savefig(f'{save_dir}/performance_table.png', dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"    âœ… ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {save_dir}/model_comparison.png")
    print(f"    âœ… ì„±ëŠ¥ í…Œì´ë¸” ì €ì¥: {save_dir}/performance_table.png")

def create_performance_report(models_results, report_path):
    """ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("    ğŸ“„ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì‘ì„±...")
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("\n" + "="*80 + "\n")
        f.write("    MIMIC-IV ì‚¬ë§ë¥  ì˜ˆì¸¡ ëª¨ë¸ ì„±ëŠ¥ ë¦¬í¬íŠ¸\n")
        f.write("    ìƒì„± ì‹œê°„: " + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + "\n")
        f.write("="*80 + "\n\n")
        
        # ì „ì²´ ìš”ì•½
        f.write("ğŸ“ˆ ì „ì²´ ìš”ì•½\n")
        f.write("-"*40 + "\n")
        f.write(f"ğŸ‘¥ ëª¨ë¸ ìˆ˜: {len(models_results)}ê°œ\n")
        
        best_auc_model = max(models_results.items(), key=lambda x: x[1]['auc'])
        best_aucpr_model = max(models_results.items(), key=lambda x: x[1]['metrics']['auc_pr'])
        
        f.write(f"ğŸ† ìµœê³  AUC-ROC: {best_auc_model[1]['auc']:.4f} ({best_auc_model[0].upper()})\n")
        f.write(f"ğŸ¯ ìµœê³  AUC-PR: {best_aucpr_model[1]['metrics']['auc_pr']:.4f} ({best_aucpr_model[0].upper()})\n\n")
        
        # ëª¨ë¸ë³„ ìƒì„¸ ì„±ëŠ¥
        f.write("ğŸ“Š ëª¨ë¸ë³„ ìƒì„¸ ì„±ëŠ¥\n")
        f.write("-"*40 + "\n")
        
        for model_name, result in models_results.items():
            f.write(f"\n=== {model_name.upper()} ===\n")
            metrics = result['metrics']
            f.write(f"ğŸ¯ AUC-ROC:     {metrics['auc_roc']:.4f}\n")
            f.write(f"ğŸ¯ AUC-PR:      {metrics['auc_pr']:.4f}\n")
            f.write(f"ğŸ¯ F1-Score:    {metrics['f1']:.4f}\n")
            f.write(f"ğŸ¯ Precision:   {metrics['precision']:.4f}\n")
            f.write(f"ğŸ¯ Recall:      {metrics['recall']:.4f}\n")
            f.write(f"ğŸ¯ Specificity: {metrics['specificity']:.4f}\n")
            f.write(f"ğŸ¯ NPV:         {metrics['npv']:.4f}\n")
            f.write(f"ğŸ¯ Alarm Rate:  {metrics['alarm_rate']:.4f}\n")
            f.write(f"ğŸ¯ Optimal Thr: {metrics['threshold_optimal']:.4f}\n")
            
            # Confusion Matrix
            cm = metrics['confusion_matrix']
            f.write(f"\nğŸ“‹ Confusion Matrix:\n")
            f.write(f"    TN: {cm[0][0]:,}  FP: {cm[0][1]:,}\n")
            f.write(f"    FN: {cm[1][0]:,}  TP: {cm[1][1]:,}\n")
            
            # Feature Importance (ìˆëŠ” ê²½ìš°)
            if 'feature_importance' in result and result['feature_importance']:
                f.write(f"\nğŸ” Top 5 Important Features:\n")
                for i, (feature, importance) in enumerate(list(result['feature_importance'].items())[:5]):
                    f.write(f"    {i+1}. {feature}: {importance:.4f}\n")
        
        # ì¶”ì²œ ì‚¬í•­
        f.write(f"\n\nğŸ’¡ ì¶”ì²œ ì‚¬í•­\n")
        f.write("-"*40 + "\n")
        
        if best_aucpr_model[1]['metrics']['auc_pr'] > 0.3:
            f.write("âœ… ëª¨ë¸ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ì‹¤ì œ ì„ìƒ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n")
        else:
            f.write("âš ï¸ ëª¨ë¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ë°ì´í„° ì „ì²˜ë¦¬ ë˜ëŠ” ëª¨ë¸ êµ¬ì¡° ê°œì„ ì„ ê³ ë ¤í•´ë³´ì„¸ìš”.\n")
        
        f.write(f"\nğŸ† ì¶”ì²œ ëª¨ë¸: {best_aucpr_model[0].upper()} (AUC-PR: {best_aucpr_model[1]['metrics']['auc_pr']:.4f})\n")
        f.write(f"ğŸ” ì¶”ì²œ ì„ê³„ê°’: {best_aucpr_model[1]['metrics']['threshold_optimal']:.4f}\n")
        
        f.write(f"\n\nğŸ“ ê´€ë ¨ íŒŒì¼\n")
        f.write("-"*40 + "\n")
        f.write("- ì„±ëŠ¥ ì‹œê°í™”: visualizations/\n")
        f.write("- ëŒ€ì‹œë³´ë“œ ë°ì´í„°: dashboard_data.json\n")
        f.write("- XAI ê²°ê³¼: xai_results.json\n")
        
    print(f"    âœ… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")

# ---------------------
# Orchestrator
# ---------------------

def train_supreme(use_8hour_sampling=True, backbones=('transformer','gru','tcn'), epochs=25, use_focal_loss=False):  # More epochs, disable focal loss by default
    X, y, meta = load_raw_data()
    if use_8hour_sampling:
        X, meta, sel_idx = create_8hour_interval_data(X, meta)
        y = meta['death_in_pred_window_new'].values if 'death_in_pred_window_new' in meta.columns else y[sel_idx]

    input_size = X.shape[2]
    
    # í™˜ì ìˆ˜ì™€ ìƒ˜í”Œ ìˆ˜ ë¶„ì„
    unique_patients = meta['subject_id'].nunique() if 'subject_id' in meta.columns else len(X)
    samples_per_patient = len(X) / unique_patients
    
    print(f"\nğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
    print(f"  ì…ë ¥ í¬ê¸°: {input_size}")
    print(f"  ğŸ‘¥ ì´ í™˜ì ìˆ˜: {unique_patients:,}ëª…")
    print(f"  ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {len(X):,}ê°œ (sliding windows)")
    print(f"  ğŸ•°ï¸ í™˜ìë‹¹ í‰ê·  ìƒ˜í”Œ: {samples_per_patient:.1f}ê°œ")
    print(f"  â˜ ï¸ ì „ì²´ ì‚¬ë§ë¥ : {np.mean(y):.1%}")
    
    # í™˜ìë³„ ì‚¬ë§ë¥  ê³„ì‚°
    if 'subject_id' in meta.columns:
        patient_mortality = []
        for patient_id in meta['subject_id'].unique():
            patient_outcomes = y[meta['subject_id'] == patient_id]
            patient_mortality.append(int(np.any(patient_outcomes)))  # í•œ ë²ˆì´ë¼ë„ ì‚¬ë§í•˜ë©´ 1
        patient_death_rate = np.mean(patient_mortality)
        print(f"  â˜ ï¸ í™˜ìë³„ ì‚¬ë§ë¥ : {patient_death_rate:.1%} ({sum(patient_mortality)}/{len(patient_mortality)}ëª…)")
    
    print(f"  ğŸ•°ï¸ 8ì‹œê°„ ìƒ˜í”Œë§: {use_8hour_sampling}")
    print(f"  ğŸ¯ Focal Loss: {use_focal_loss}")
    built = []
    for name in backbones:
        if name=='transformer':
            built.append(('transformer', EfficientTransformer(input_size, return_logits=True)))
        elif name=='gru':
            built.append(('gru', EfficientGRU(input_size, return_logits=True)))
        elif name=='tcn':
            built.append(('tcn', EfficientTCN(input_size, return_logits=True)))
    
    print(f"\nğŸš€ ëª¨ë¸ êµ¬ì„±: {[name for name, _ in built]}")
    
    results = {}
    for name, mdl in built:
        print(f"\nğŸš€ {name.upper()} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        res = train_one_model(mdl, X, y, meta, model_name=name, epochs=epochs, use_focal_loss=use_focal_loss)
        results[name] = res
        
        # Performance visualization
        if len(np.unique(res['y_test'])) > 1:
            print(f"  ğŸ“ˆ {name} ì‹œê°í™” ìƒì„±...")
            os.makedirs("results/temp", exist_ok=True)
            plot_performance_visualization(res['y_test'], res['pred_prob'], name, "results/temp")
        
        # Memory cleanup
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    # Enhanced ensemble over the *best* two models by val AUC
    print("\nğŸ† ì•™ìƒë¸” ëª¨ë¸ ìƒì„±...")
    ranked = sorted(results.items(), key=lambda kv: kv[1]['val_auc'], reverse=True)
    
    if len(ranked) >= 2:
        top_model_names = [ranked[0][0], ranked[1][0]]
        print(f"  ì„ íƒëœ ëª¨ë¸: {top_model_names} (AUC: {ranked[0][1]['val_auc']:.4f}, {ranked[1][1]['val_auc']:.4f})")
        
        # Create ensemble with return_logits=True versions
        # Use the already trained models for ensemble
        model1 = results[ranked[0][0]]['model']
        model2 = results[ranked[1][0]]['model']
        
        # Set models to eval mode and freeze parameters
        model1.eval()
        model2.eval()
        for param in model1.parameters():
            param.requires_grad = False
        for param in model2.parameters():
            param.requires_grad = False
        
        ensemble = EnsembleModel([model1, model2])
        ens_res = train_one_model(ensemble, X, y, meta, model_name='ensemble', epochs=15, use_focal_loss=use_focal_loss)
        results['ensemble'] = ens_res
        
        # Ensemble visualization
        if len(np.unique(ens_res['y_test'])) > 1:
            print(f"  ğŸ“ˆ ensemble ì‹œê°í™” ìƒì„±...")
            plot_performance_visualization(ens_res['y_test'], ens_res['pred_prob'], 'ensemble', "results/temp")
    else:
        print("  âš ï¸ ì•™ìƒë¸”ì„ ìœ„í•œ ëª¨ë¸ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŒ")
    return results

# ---------------------
# Real-time Prediction Function
# ---------------------

def predict_mortality_for_new_patient(model, scaler, patient_data, subject_id=None, feature_names=None):
    """ìƒˆë¡œìš´ í™˜ì ë°ì´í„°ì— ëŒ€í•œ ì‚¬ë§ë¥  ì˜ˆì¸¡
    
    Args:
        model: í•™ìŠµëœ ëª¨ë¸
        scaler: ë°ì´í„° ì •ê·œí™”ì— ì‚¬ìš©í•œ StandardScaler
        patient_data: (T, F) í˜•íƒœì˜ í™˜ì ì‹œê³„ì—´ ë°ì´í„°
        subject_id: í™˜ì ID (ì„ íƒì )
        feature_names: í”¼ì²˜ ì´ë¦„ ëª©ë¡ (ì„ íƒì )
    
    Returns:
        dict: ì˜ˆì¸¡ ê²°ê³¼ ë° ì„¤ëª…
    """
    device = next(model.parameters()).device
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    if patient_data.ndim == 2:
        patient_data = patient_data.reshape(1, patient_data.shape[0], patient_data.shape[1])  # (1, T, F)
    
    # ê²°ì¸¡ê°’ ì²˜ë¦¬
    patient_data_clean = np.nan_to_num(patient_data, nan=0.0, posinf=1.0, neginf=-1.0)
    
    # ì •ê·œí™”
    T, F = patient_data_clean.shape[1], patient_data_clean.shape[2]
    patient_data_flat = patient_data_clean.reshape(-1, F)
    patient_data_scaled = scaler.transform(patient_data_flat)
    patient_data_final = patient_data_scaled.reshape(1, T, F)
    
    # ì˜ˆì¸¡
    model.eval()
    with torch.no_grad():
        patient_tensor = torch.FloatTensor(patient_data_final).to(device)
        logits = model(patient_tensor)
        probability = torch.sigmoid(logits).item()
    
    # ìœ„í—˜ë„ ë¶„ë¥˜
    if probability > 0.7:
        risk_level = 'High'
        alert_level = 'ğŸ”´ ê¸´ê¸‰'
    elif probability > 0.3:
        risk_level = 'Medium'
        alert_level = 'ğŸŸ¡ ì£¼ì˜'
    else:
        risk_level = 'Low'
        alert_level = 'ğŸŸ¢ ì•ˆì •'
    
    # ì£¼ìš” íŠ¹ì„± ì§€ìˆ˜ ê³„ì‚° (ë§ˆì§€ë§‰ ì‹œì )
    last_timestep = patient_data_clean[0, -1, :]
    top_features = []
    
    if feature_names and len(feature_names) == len(last_timestep):
        # ë¹„ì •ìƒ ê°’ë“¤ ì‹ë³„
        for i, (feature_name, value) in enumerate(zip(feature_names, last_timestep)):
            if not np.isnan(value) and abs(value) > 0.1:  # ìœ ì˜ë¯¸í•œ ê°’ë“¤
                top_features.append({
                    'feature_name': feature_name,
                    'value': float(value),
                    'contribution': 'high' if abs(value) > 1.0 else 'medium'
                })
        
        # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        top_features.sort(key=lambda x: abs(x['value']), reverse=True)
        top_features = top_features[:5]  # ìƒìœ„ 5ê°œ
    
    # ì‹œê°„ë³„ ìœ„í—˜ë„ ì¶”ì´ (ë‹¨ì¼ ì˜ˆì¸¡ì´ë¯€ë¡œ ê°„ë‹¨í•˜ê²Œ)
    hourly_predictions = [{
        'hour': 1,
        'risk_score': probability,
        'timestamp': datetime.now().isoformat()
    }]
    
    return {
        'subject_id': subject_id or 'new_patient',
        'death_probability': probability,
        'risk_level': risk_level,
        'alert_level': alert_level,
        'prediction_timestamp': datetime.now().isoformat(),
        'top_risk_factors': top_features,
        'hourly_predictions': hourly_predictions,
        'model_confidence': 'high' if 0.2 < probability < 0.8 else 'medium',
        'recommendation': {
            'action': 'ì¦‰ì‹œ ì˜ë£Œì§„ ìƒë‹´' if probability > 0.7 else 'ì§€ì† ëª¨ë‹ˆí„°ë§' if probability > 0.3 else 'ì •ê¸° ê²€ì§„',
            'monitoring_interval': '15ë¶„' if probability > 0.7 else '1ì‹œê°„' if probability > 0.3 else '4ì‹œê°„'
        }
    }

# ---------------------
# Dashboard Compatibility Functions
# ---------------------

def create_dashboard_results(models_results, meta_data, feature_names=None):
    """ëŒ€ì‹œë³´ë“œ í˜¸í™˜ ê²°ê³¼ ìƒì„± (ì‹¤ì œ subject_id + ì‹œê°„ë³„ ì˜ˆì¸¡)"""
    print("ğŸ“Š ëŒ€ì‹œë³´ë“œìš© ê²°ê³¼ ë°ì´í„° ìƒì„±...")
    
    dashboard_data = {}
    
    for model_name, result in models_results.items():
        predictions = result['pred_prob']
        y_test = result['y_test']
        test_indices = result.get('test_indices', np.arange(len(y_test)))
        
        # Patient timelines (simplified for single prediction)
        patient_timelines = {}
        
        # ì‹¤ì œ subject_idë¥¼ ì‚¬ìš©í•œ í™˜ìë³„ ì‹œê°„ëŒ€ë³„ ì˜ˆì¸¡
        patient_hourly_data = defaultdict(list)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í™˜ìë³„ ë°ì´í„° ìˆ˜ì§‘
        for i, test_idx in enumerate(test_indices[:200]):  # ëŒ€ì‹œë³´ë“œìš© ì œí•œ
            if test_idx < len(meta_data):
                meta_row = meta_data.iloc[test_idx]
                subject_id = str(meta_row['subject_id']) if 'subject_id' in meta_row else f"patient_{test_idx}"
                obs_hour = meta_row.get('obs_start_hour', 1) if hasattr(meta_row, 'get') else 1
                prediction = predictions[i] if i < len(predictions) else 0.5
                actual = y_test[i] if i < len(y_test) else 0
                
                patient_hourly_data[subject_id].append({
                    'hour': int(obs_hour),
                    'risk_score': float(prediction),
                    'actual_outcome': int(actual),
                    'timestamp': (datetime.now() + timedelta(hours=int(obs_hour))).isoformat(),
                    'test_index': test_idx
                })
        
        # í™˜ìë³„ ì‹œê°„ëŒ€ë³„ timeline ìƒì„±
        for subject_id, hourly_data in patient_hourly_data.items():
            # ì‹œê°„ ìˆœìœ¼ë¡œ ì •ë ¬
            hourly_data.sort(key=lambda x: x['hour'])
            
            # ìœ„í—˜ë„ ì¶”ì´ ê³„ì‚°
            risk_scores = [data['risk_score'] for data in hourly_data]
            
            # ìœ„í—˜ë„ ë³€í™” ì¶”ì„¸ ê³„ì‚°
            if len(risk_scores) >= 3:
                early_avg = np.mean(risk_scores[:len(risk_scores)//2])
                late_avg = np.mean(risk_scores[len(risk_scores)//2:])
                if late_avg > early_avg + 0.1:
                    trend = 'increasing'
                elif late_avg < early_avg - 0.1:
                    trend = 'decreasing'
                else:
                    trend = 'stable'
            else:
                trend = 'stable'
            
            patient_timelines[subject_id] = {
                'subject_id': subject_id,
                'prediction_timeline': hourly_data,
                'risk_trend': trend,
                'max_risk': float(np.max(risk_scores)),
                'avg_risk': float(np.mean(risk_scores)),
                'min_risk': float(np.min(risk_scores)),
                'hour_count': len(hourly_data),
                'mortality_outcome': int(np.any([data['actual_outcome'] for data in hourly_data]))
            }
        
        # ê³ ìœ„í—˜ í™˜ì ì‹ë³„ (ì‹¤ì œ subject_id ì‚¬ìš©)
        high_risk_patients = []
        for subject_id, timeline in patient_timelines.items():
            if timeline['max_risk'] > 0.7:
                high_risk_patients.append({
                    'subject_id': subject_id,  # ì‹¤ì œ subject_id
                    'max_risk_score': timeline['max_risk'],
                    'avg_risk_score': timeline['avg_risk'],
                    'min_risk_score': timeline['min_risk'],
                    'risk_trend': timeline['risk_trend'],
                    'hour_count': timeline['hour_count'],
                    'mortality_outcome': timeline['mortality_outcome'],
                    'latest_hour': max([data['hour'] for data in timeline['prediction_timeline']]),
                    'first_hour': min([data['hour'] for data in timeline['prediction_timeline']])
                })
        
        high_risk_patients.sort(key=lambda x: x['max_risk_score'], reverse=True)
        
        # ì‚¬ë§ë¥  í†µê³„ ê³„ì‚°
        mortality_stats = {
            'total_deaths': sum([p['mortality_outcome'] for p in patient_timelines.values()]),
            'total_patients': len(patient_timelines),
            'patient_mortality_rate': sum([p['mortality_outcome'] for p in patient_timelines.values()]) / len(patient_timelines) if patient_timelines else 0,
            'high_risk_mortality_rate': sum([1 for p in patient_timelines.values() if p['max_risk'] > 0.7 and p['mortality_outcome'] == 1]) / max(1, len([p for p in patient_timelines.values() if p['max_risk'] > 0.7])),
            'low_risk_mortality_rate': sum([1 for p in patient_timelines.values() if p['max_risk'] < 0.3 and p['mortality_outcome'] == 1]) / max(1, len([p for p in patient_timelines.values() if p['max_risk'] < 0.3]))
        }
        
        dashboard_data[model_name] = {
            'patient_timelines': patient_timelines,
            'high_risk_patients': high_risk_patients[:50],
            'summary_stats': {
                'total_patients': len(patient_timelines),
                'high_risk_count': len([p for p in patient_timelines.values() if p['max_risk'] > 0.7]),
                'medium_risk_count': len([p for p in patient_timelines.values() if 0.3 <= p['max_risk'] <= 0.7]),
                'low_risk_count': len([p for p in patient_timelines.values() if p['max_risk'] < 0.3]),
                'avg_max_risk': float(np.mean([p['max_risk'] for p in patient_timelines.values()])) if patient_timelines else 0.0,
                'avg_hour_count': float(np.mean([len(p['prediction_timeline']) for p in patient_timelines.values()])) if patient_timelines else 0.0,
                'model_auc': result['auc'],
                'mortality_stats': {
                    'total_patients': len(patient_timelines),
                    'total_deaths': len([p for p in patient_timelines.values() if any(pred['actual_outcome'] == 1 for pred in p['prediction_timeline'])]),
                    'patient_mortality_rate': float(np.mean([any(pred['actual_outcome'] == 1 for pred in p['prediction_timeline']) for p in patient_timelines.values()])) if patient_timelines else 0.0,
                    'high_risk_mortality_rate': float(np.mean([any(pred['actual_outcome'] == 1 for pred in p['prediction_timeline']) for p in patient_timelines.values() if p['max_risk'] > 0.7])) if [p for p in patient_timelines.values() if p['max_risk'] > 0.7] else 0.0,
                    'low_risk_mortality_rate': float(np.mean([any(pred['actual_outcome'] == 1 for pred in p['prediction_timeline']) for p in patient_timelines.values() if p['max_risk'] < 0.3])) if [p for p in patient_timelines.values() if p['max_risk'] < 0.3] else 0.0
                }
            }
        }
    
    return dashboard_data

def create_realtime_prediction_api(model_dir="results/models"):
    """
    ëŒ€ì‹œë³´ë“œì—ì„œ ì‚¬ìš©í•  ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ìƒì„±
    """
    import joblib
    
    # ì €ì¥ëœ ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
    scaler_path = f"{model_dir}/data_scaler.pkl"
    metadata_path = f"{model_dir}/metadata_info.pkl"
    
    try:
        scaler = joblib.load(scaler_path)
        metadata_info = joblib.load(metadata_path)
        
        # ê°€ì¥ ì¢‹ì€ ëª¨ë¸ ì°¾ê¸° (transformer ìš°ì„ )
        model_files = [f for f in os.listdir(model_dir) if f.endswith('_best_model.pth')]
        if not model_files:
            raise FileNotFoundError("ì €ì¥ëœ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ìš°ì„ ìˆœìœ„: transformer > gru > tcn > ensemble (ì•™ìƒë¸”ì€ ë³µì¡í•´ì„œ ë§ˆì§€ë§‰)
        priority_order = ['transformer', 'gru', 'tcn', 'ensemble']
        selected_model = None
        
        for model_type in priority_order:
            for model_file in model_files:
                if model_type in model_file:
                    selected_model = f"{model_dir}/{model_file}"
                    # ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
                    test_model, test_checkpoint = load_trained_model(selected_model)
                    if test_model is not None and test_checkpoint is not None:
                        model = test_model
                        checkpoint = test_checkpoint
                        print(f"âœ… ì„±ê³µì ìœ¼ë¡œ ë¡œë“œëœ ëª¨ë¸: {model_type}")
                        break
                    else:
                        print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, ë‹¤ìŒ ëª¨ë¸ ì‹œë„: {model_type}")
                        selected_model = None
            if selected_model:
                break
        
        if not selected_model:
            # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì²« ë²ˆì§¸ ëª¨ë¸ë¡œ ê°•ì œ ì‹œë„
            selected_model = f"{model_dir}/{model_files[0]}"
            model, checkpoint = load_trained_model(selected_model)
            if model is None or checkpoint is None:
                raise Exception("ëª¨ë“  ëª¨ë¸ ë¡œë“œê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        def predict_new_patient(patient_data_dict):
            """
            ìƒˆë¡œìš´ í™˜ì ë°ì´í„°ë¡œ ì‚¬ë§ë¥  ì˜ˆì¸¡
            
            Args:
                patient_data_dict: {'feature_0': value, 'feature_1': value, ...}
            
            Returns:
                {'mortality_risk': float, 'risk_level': str, 'confidence': float}
            """
            try:
                # íŠ¹ì„± ë²¡í„° ìƒì„±
                feature_vector = np.array([patient_data_dict.get(f'feature_{i}', 0.0) 
                                         for i in range(metadata_info['input_shape'][2])])
                
                # ì‹œê³„ì—´ í˜•íƒœë¡œ ë³€í™˜ (1, seq_len, features)
                seq_len = metadata_info['input_shape'][1]
                X_input = np.tile(feature_vector, (seq_len, 1)).reshape(1, seq_len, -1)
                
                # ì •ê·œí™”
                X_scaled = scaler.transform(X_input.reshape(-1, X_input.shape[-1])).reshape(X_input.shape)
                
                # ì˜ˆì¸¡
                with torch.no_grad():
                    X_tensor = torch.from_numpy(X_scaled).float()
                    if torch.cuda.is_available():
                        X_tensor = X_tensor.cuda()
                        model.cuda()
                    
                    pred_prob = model(X_tensor).sigmoid().cpu().numpy()[0, 0]
                
                # ìœ„í—˜ë„ ìˆ˜ì¤€ ê²°ì •
                if pred_prob >= 0.7:
                    risk_level = "High"
                elif pred_prob >= 0.3:
                    risk_level = "Medium"
                else:
                    risk_level = "Low"
                
                return {
                    'mortality_risk': float(pred_prob),
                    'risk_level': risk_level,
                    'confidence': float(checkpoint['best_aupr']),
                    'model_used': checkpoint['model_name']
                }
                
            except Exception as e:
                return {
                    'error': f"ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}",
                    'mortality_risk': 0.0,
                    'risk_level': "Unknown",
                    'confidence': 0.0
                }
        
        print(f"âœ… ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ğŸ“Š ëª¨ë¸: {selected_model}")
        print(f"   ğŸ¯ ì„±ëŠ¥: AUC {checkpoint['best_auc']:.4f}, AUPR {checkpoint['best_aupr']:.4f}")
        
        return predict_new_patient
        
    except Exception as e:
        print(f"âŒ ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ìƒì„± ì‹¤íŒ¨: {e}")
        return None

def load_trained_model(model_path, device='cpu'):
    """
    ì €ì¥ëœ .pth ëª¨ë¸ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ì‹¤ì‹œê°„ ì˜ˆì¸¡ì— ì‚¬ìš©
    """
    try:
        checkpoint = torch.load(model_path, map_location=device)
        model_class_name = checkpoint['model_class']
        input_size = checkpoint.get('input_size', None)
        
        # input_sizeê°€ Noneì´ê±°ë‚˜ ì˜ëª»ëœ ê²½ìš° ë°ì´í„°ì—ì„œ ì¶”ë¡ 
        if input_size is None:
            print("âš ï¸ input_sizeê°€ ì €ì¥ë˜ì§€ ì•ŠìŒ. ë°ì´í„°ì—ì„œ ì¶”ë¡ ...")
            # ë©”íƒ€ë°ì´í„°ë‚˜ ê¸°ë³¸ê°’ì—ì„œ input_size ì¶”ë¡ 
            try:
                import joblib
                metadata_path = "results/models/metadata_info.pkl"
                if os.path.exists(metadata_path):
                    metadata_info = joblib.load(metadata_path)
                    input_shape = metadata_info.get('input_shape', None)
                    if input_shape and len(input_shape) >= 3:
                        input_size = input_shape[2]  # (batch, seq_len, features)
                        print(f"âœ… ë©”íƒ€ë°ì´í„°ì—ì„œ input_size ì¶”ë¡ : {input_size}")
                    else:
                        input_size = 20  # ê¸°ë³¸ê°’
                        print(f"âš ï¸ ê¸°ë³¸ input_size ì‚¬ìš©: {input_size}")
                else:
                    input_size = 20  # ê¸°ë³¸ê°’
                    print(f"âš ï¸ ë©”íƒ€ë°ì´í„° ì—†ìŒ, ê¸°ë³¸ input_size ì‚¬ìš©: {input_size}")
            except Exception as e:
                input_size = 20  # ê¸°ë³¸ê°’
                print(f"âš ï¸ input_size ì¶”ë¡  ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {input_size}, ì˜¤ë¥˜: {e}")
        
        # ëª¨ë¸ í´ë˜ìŠ¤ ë§¤í•‘
        model_classes = {
            'EfficientTransformer': EfficientTransformer,
            'EfficientGRU': EfficientGRU,
            'EfficientTCN': EfficientTCN,
            'EnsembleModel': EnsembleModel
        }
        
        if model_class_name not in model_classes:
            raise ValueError(f"Unknown model class: {model_class_name}")
        
        # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        if model_class_name == 'EnsembleModel':
            # ì•™ìƒë¸” ëª¨ë¸ì˜ ê²½ìš° ê°œë³„ ëª¨ë¸ë“¤ë¡œ êµ¬ì„±
            try:
                transformer = EfficientTransformer(input_size, return_logits=False)
                gru = EfficientGRU(input_size, return_logits=False)
                tcn = EfficientTCN(input_size, return_logits=False)
                model = EnsembleModel([transformer, gru, tcn])
                print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì™„ë£Œ: input_size={input_size}")
            except Exception as e:
                print(f"âŒ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {e}")
                # ë‹¨ì¼ ëª¨ë¸ë¡œ ëŒ€ì²´
                model = EfficientTransformer(input_size, return_logits=False)
                print(f"âš ï¸ Transformer ëª¨ë¸ë¡œ ëŒ€ì²´")
        else:
            model = model_classes[model_class_name](input_size, return_logits=False)
        
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        print(f"   ğŸ“Š AUC: {checkpoint['best_auc']:.4f}, AUPR: {checkpoint['best_aupr']:.4f}")
        
        return model, checkpoint
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def convert_numpy_types(obj):
    """
    Recursively convert numpy types to native Python types for JSON serialization
    """
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    else:
        return obj

def upload_to_s3(local_file_path, s3_key, bucket_name="say1-7team-bucket", aws_region="ap-northeast-2"):
    """
    ë¡œì»¬ íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ
    
    Args:
        local_file_path (str): ì—…ë¡œë“œí•  ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        s3_key (str): S3ì—ì„œì˜ í‚¤(ê²½ë¡œ)
        bucket_name (str): S3 ë²„í‚· ì´ë¦„
        aws_region (str): AWS ë¦¬ì „
    
    Returns:
        bool: ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
    """
    if not S3_AVAILABLE:
        print(f"âš ï¸ S3 ì—…ë¡œë“œ ë¶ˆê°€: boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return False
        
    try:
        s3_client = boto3.client('s3', region_name=aws_region)
        s3_client.upload_file(local_file_path, bucket_name, s3_key)
        print(f"âœ… S3 ì—…ë¡œë“œ ì„±ê³µ: {local_file_path} â†’ s3://{bucket_name}/{s3_key}")
        return True
    except FileNotFoundError:
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {local_file_path}")
        return False
    except NoCredentialsError:
        print("âŒ AWS ìê²© ì¦ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    except ClientError as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

def upload_directory_to_s3(local_dir, s3_prefix, bucket_name="say1-7team-bucket", aws_region="ap-northeast-2"):
    """
    ë¡œì»¬ ë””ë ‰í† ë¦¬ë¥¼ S3ì— ì¬ê·€ì ìœ¼ë¡œ ì—…ë¡œë“œ
    
    Args:
        local_dir (str): ì—…ë¡œë“œí•  ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        s3_prefix (str): S3ì—ì„œì˜ ì ‘ë‘ì‚¬ ê²½ë¡œ
        bucket_name (str): S3 ë²„í‚· ì´ë¦„
        aws_region (str): AWS ë¦¬ì „
    
    Returns:
        tuple: (ì„±ê³µí•œ íŒŒì¼ ìˆ˜, ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜)
    """
    if not S3_AVAILABLE:
        print(f"âš ï¸ S3 ì—…ë¡œë“œ ë¶ˆê°€: boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        return 0, 0
        
    try:
        s3_client = boto3.client('s3', region_name=aws_region)
        success_count = 0
        failure_count = 0
        
        for root, dirs, files in os.walk(local_dir):
            for file in files:
                local_file_path = os.path.join(root, file)
                # ë¡œì»¬ ê²½ë¡œë¥¼ S3 í‚¤ë¡œ ë³€í™˜ (Windows ê²½ë¡œ êµ¬ë¶„ìë¥¼ / ë¡œ ë³€ê²½)
                relative_path = os.path.relpath(local_file_path, local_dir)
                s3_key = f"{s3_prefix}/{relative_path}".replace("\\", "/")
                
                if upload_to_s3(local_file_path, s3_key, bucket_name, aws_region):
                    success_count += 1
                else:
                    failure_count += 1
        
        print(f"ğŸ“Š ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ ì™„ë£Œ: ì„±ê³µ {success_count}ê°œ, ì‹¤íŒ¨ {failure_count}ê°œ")
        return success_count, failure_count
        
    except Exception as e:
        print(f"âŒ ë””ë ‰í† ë¦¬ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return 0, 0

def save_dashboard_compatible_results(models_results, dashboard_data, job_name=None):
    """Save results in dashboard-compatible format"""
    if job_name is None:
        job_name = f"enhanced-dashboard-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
    
    # Create directory structure
    results_dir = f"results/{job_name}"
    dashboard_dir = f"dashboard-results/{job_name}"
    
    os.makedirs(results_dir, exist_ok=True)
    os.makedirs(dashboard_dir, exist_ok=True)
    os.makedirs(f"{dashboard_dir}/patient_explanations", exist_ok=True)
    os.makedirs(f"{dashboard_dir}/patient_trends", exist_ok=True)
    
    print(f"ğŸ’¾ ëŒ€ì‹œë³´ë“œ í˜¸í™˜ ê²°ê³¼ ì €ì¥: {dashboard_dir}")
    
    # 1. Patient explanations and trends
    for model_name, data in dashboard_data.items():
        for subject_id, timeline in data['patient_timelines'].items():
            # Patient explanation
            explanation_data = {
                'subject_id': subject_id,
                'prediction_timestamp': datetime.now().isoformat(),
                'death_probability': timeline['max_risk'],
                'actual_death': timeline['prediction_timeline'][0]['actual_outcome'],
                'explanation': {
                    'risk_level': 'High' if timeline['max_risk'] > 0.7 else 'Medium' if timeline['max_risk'] > 0.3 else 'Low',
                    'top_risk_factors': [{
                        'feature_name': 'Risk_Score',
                        'contribution_score': timeline['max_risk'],
                        'value': timeline['max_risk']
                    }]
                }
            }
            
            with open(f"{dashboard_dir}/patient_explanations/{subject_id}.json", 'w') as f:
                json.dump(convert_numpy_types(explanation_data), f, indent=2)
            
            # Patient trend
            trend_data = {
                'subject_id': subject_id,
                'predictions_timeline': [{
                    'timestamp': pred['timestamp'],  # Keep ISO format for app.py compatibility
                    'death_probability': pred['risk_score'],  # App.py expects 'death_probability'
                    'risk_score': pred['risk_score']
                } for pred in timeline['prediction_timeline']]
            }
            
            with open(f"{dashboard_dir}/patient_trends/{subject_id}.json", 'w') as f:
                json.dump(convert_numpy_types(trend_data), f, indent=2)
    
    # 2. Global feature importance
    global_importance = {'all_features': []}
    
    # Aggregate feature importance from all models
    all_importances = defaultdict(list)
    for model_name, result in models_results.items():
        if 'feature_importance' in result:
            for feature, importance in result['feature_importance'].items():
                all_importances[feature].append(importance)
    
    # Calculate average importance
    for feature, importances in all_importances.items():
        avg_importance = float(np.mean(importances))
        global_importance['all_features'].append({
            'feature_name': feature,
            'importance_score': avg_importance
        })
    
    # Sort by importance
    global_importance['all_features'].sort(key=lambda x: x['importance_score'], reverse=True)
    global_importance['all_features'] = global_importance['all_features'][:20]  # Top 20
    
    with open(f"{dashboard_dir}/global_feature_importance.json", 'w') as f:
        json.dump(convert_numpy_types(global_importance), f, indent=2)
    
    # 3. Model performance
    model_performance = {}
    for model_name, result in models_results.items():
        model_performance[model_name] = {
            'auc_roc': result['auc'],
            'auc_pr': result['metrics'].get('auc_pr', 0),
            'f1_score': result['metrics'].get('f1', 0),
            'precision': result['metrics'].get('precision', 0),
            'recall': result['metrics'].get('recall', 0),
            'specificity': result['metrics'].get('specificity', 0),
            'best_auc': result.get('val_auc', result['auc']),
            'model_type': 'enhanced_prediction',
            'auroc': result.get('auc', 0),  # App.py compatibility
            'aupr': result.get('val_aupr', result.get('aupr', 0))
        }
    
    with open(f"{dashboard_dir}/model_performance.json", 'w') as f:
        json.dump(convert_numpy_types(model_performance), f, indent=2)
    
    # 4. Complete dashboard data (convert numpy types)
    dashboard_data_converted = convert_numpy_types(dashboard_data)
    with open(f"{results_dir}/dashboard_data.json", 'w') as f:
        json.dump(dashboard_data_converted, f, indent=2)
    
    # 5. XAI results
    xai_results = {}
    for model_name, result in models_results.items():
        xai_data = {}
        
        if 'feature_importance' in result:
            xai_data['feature_importance'] = result['feature_importance']
        
        if 'patient_explanations' in result:
            xai_data['patient_explanations'] = result['patient_explanations']
        
        xai_results[model_name] = xai_data
    
    with open(f"{results_dir}/xai_results.json", 'w') as f:
        json.dump(convert_numpy_types(xai_results), f, indent=2)
    
    print(f"âœ… ëŒ€ì‹œë³´ë“œ í˜¸í™˜ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
    print(f"   ğŸ“‚ ê²°ê³¼ ë””ë ‰í† ë¦¬: {results_dir}")
    print(f"   ğŸ“‚ ëŒ€ì‹œë³´ë“œ ë””ë ‰í† ë¦¬: {dashboard_dir}")
    
    # 8ï¸âƒ£ S3 ì—…ë¡œë“œ (app.py ëŒ€ì‹œë³´ë“œìš©)
    print(f"\nâ˜ï¸ S3 ì—…ë¡œë“œ ì‹œì‘...")
    
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ S3 ì„¤ì • ê°€ì ¸ì˜¤ê¸° (ê¸°ë³¸ê°’ í¬í•¨)
    s3_bucket = os.getenv("S3_BUCKET_NAME", "say1-7team-bucket")
    aws_region = os.getenv("AWS_REGION", "ap-northeast-2")
    
    try:
        # 1. Dashboard ê²°ê³¼ íŒŒì¼ë“¤ ì—…ë¡œë“œ
        dashboard_s3_prefix = f"dashboard-results/{job_name}"
        print(f"   ğŸ“Š ëŒ€ì‹œë³´ë“œ ê²°ê³¼ ì—…ë¡œë“œ: {dashboard_dir} â†’ s3://{s3_bucket}/{dashboard_s3_prefix}")
        success_dash, failure_dash = upload_directory_to_s3(dashboard_dir, dashboard_s3_prefix, s3_bucket, aws_region)
        
        # 2. ëª¨ë¸ íŒŒì¼ë“¤ ì—…ë¡œë“œ (.pth, scaler, metadata)
        models_dir = "results/models"
        models_s3_prefix = f"models/{job_name}"
        if os.path.exists(models_dir):
            print(f"   ğŸ¤– ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ: {models_dir} â†’ s3://{s3_bucket}/{models_s3_prefix}")
            success_models, failure_models = upload_directory_to_s3(models_dir, models_s3_prefix, s3_bucket, aws_region)
        else:
            print(f"   âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ ì—†ìŒ: {models_dir}")
            success_models, failure_models = 0, 0
        
        # 3. ì‹œê°í™” ê²°ê³¼ ì—…ë¡œë“œ (ì„ íƒì )
        viz_dir = f"{results_dir}/visualizations"
        if os.path.exists(viz_dir):
            viz_s3_prefix = f"visualizations/{job_name}"
            print(f"   ğŸ“ˆ ì‹œê°í™” ê²°ê³¼ ì—…ë¡œë“œ: {viz_dir} â†’ s3://{s3_bucket}/{viz_s3_prefix}")
            success_viz, failure_viz = upload_directory_to_s3(viz_dir, viz_s3_prefix, s3_bucket, aws_region)
        else:
            success_viz, failure_viz = 0, 0
        
        # ì—…ë¡œë“œ ê²°ê³¼ ìš”ì•½
        total_success = success_dash + success_models + success_viz
        total_failure = failure_dash + failure_models + failure_viz
        
        print(f"\nğŸ“Š S3 ì—…ë¡œë“œ ì™„ë£Œ:")
        print(f"   âœ… ì„±ê³µ: {total_success}ê°œ íŒŒì¼")
        if total_failure > 0:
            print(f"   âŒ ì‹¤íŒ¨: {total_failure}ê°œ íŒŒì¼")
        
        print(f"\nğŸ”— app.pyì—ì„œ ì‚¬ìš©í•  í™˜ê²½ë³€ìˆ˜:")
        print(f"   export JOB_NAME=\"{job_name}\"")
        print(f"   export S3_BUCKET_NAME=\"{s3_bucket}\"")
        print(f"   export AWS_REGION=\"{aws_region}\"")
        
        print(f"\nğŸ¯ Dashboard ì ‘ê·¼ ê²½ë¡œ:")
        print(f"   s3://{s3_bucket}/dashboard-results/{job_name}/")
        
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("   ë¡œì»¬ íŒŒì¼ì€ ì •ìƒì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return results_dir, dashboard_dir

def threshold_analysis(y_true, y_prob, num_points=15):
    """Comprehensive threshold analysis"""
    thresholds = np.linspace(0.1, 0.9, num_points)
    results = []
    
    for threshold in thresholds:
        y_pred = (y_prob >= threshold).astype(int)
        
        if len(np.unique(y_pred)) > 1:
            cm = confusion_matrix(y_true, y_pred)
            tn, fp, fn, tp = cm.ravel()
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
            npv = tn / (tn + fn) if (tn + fn) > 0 else 0
            alarm_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
            
            results.append({
                'threshold': threshold,
                'precision': precision,
                'recall': recall,
                'specificity': specificity,
                'f1_score': f1,
                'npv': npv,
                'alarm_rate': alarm_rate,
                'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn
            })
    
    return pd.DataFrame(results)

if __name__ == '__main__':
    print("="*70)
    print("ğŸ¥ MIMIC-IV ì‚¬ë§ë¥  ì˜ˆì¸¡ ì‹œìŠ¤í…œ (Optimized Version)")
    print("ğŸ“Š Optimized Models + XAI + Dashboard Integration")
    print("ğŸ¯ Transformer, GRU, TCN + Performance Tuned")
    print("="*70)
    
    # Environment check
    if torch.cuda.is_available():
        print(f"ğŸ® GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ® GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        torch.cuda.empty_cache()
    else:
        print("ğŸ’» CPU ëª¨ë“œ")
    
    # 0ï¸âƒ£ ìŠ¤ì¼€ì¼ëŸ¬ì™€ ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (Dashboard êµ¬ë™ì— í•„ìˆ˜)
    print("\nğŸ“Š ë°ì´í„° ë° ìŠ¤ì¼€ì¼ëŸ¬ ì¤€ë¹„...")
    X_full, y_full, metadata_full = load_raw_data()
    
    # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ëŒ€ì‹œë³´ë“œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ìš©)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X_full.reshape(-1, X_full.shape[-1])).reshape(X_full.shape)
    
    scaler_dir = "results/models"
    os.makedirs(scaler_dir, exist_ok=True)
    
    import joblib
    scaler_path = f"{scaler_dir}/data_scaler.pkl"
    joblib.dump(scaler, scaler_path)
    print(f"ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ë¨: {scaler_path}")
    
    # ë©”íƒ€ë°ì´í„°ë„ ì €ì¥
    metadata_path = f"{scaler_dir}/metadata_info.pkl"
    joblib.dump({
        'feature_names': [f'feature_{i}' for i in range(X_full.shape[2])],
        'input_shape': X_full.shape,
        'total_patients': metadata_full['subject_id'].nunique() if 'subject_id' in metadata_full.columns else len(X_full)
    }, metadata_path)
    print(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ì €ì¥ë¨: {metadata_path}")
    
    # 1ï¸âƒ£ Optimized model training
    print("\nğŸš€ ìµœì í™”ëœ ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    out = train_supreme(use_8hour_sampling=True, epochs=25, use_focal_loss=False)  # Optimized settings

    # 2ï¸âƒ£ Enhanced results summary
    print("\nğŸ“Š ê°•í™”ëœ ê²°ê³¼ ìš”ì•½:")
    for k, v in out.items():
        print(f"\n=== {k.upper()} ===")
        print(f"ğŸ¯ AUCPR:   {v['metrics']['auc_pr']:.4f} (ğŸ¯ Primary)")
        print(f"ğŸ¯ AUC-ROC: {v['metrics']['auc_roc']:.4f}")
        print(f"ğŸ¯ F1:      {v['metrics']['f1']:.4f}")
        print(f"ğŸ¯ Precision: {v['metrics']['precision']:.4f}")
        print(f"ğŸ¯ Recall:    {v['metrics']['recall']:.4f}")
        print(f"ğŸ¯ Specificity: {v['metrics']['specificity']:.4f}")
        print(f"ğŸ¯ NPV:        {v['metrics']['npv']:.4f}")
        print(f"ğŸ¯ Alarm rate: {v['metrics']['alarm_rate']:.4f}")
        print(f"ğŸ¯ Optimal Thr: {v['metrics']['threshold_optimal']:.3f}")
        
        # XAI summary
        if 'feature_importance' in v:
            top_features = list(v['feature_importance'].items())[:3]
            print(f"ğŸ” Top 3 Features: {[f[0] for f in top_features]}")
        
        if 'patient_explanations' in v:
            print(f"ğŸ‘¤ Patient Explanations: {len(v['patient_explanations'])}ëª…")

    # 3ï¸âƒ£ Dashboard compatibility
    print("\nğŸ“Š ëŒ€ì‹œë³´ë“œ í˜¸í™˜ ë°ì´í„° ìƒì„±...")
    
    # JOB_NAME ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” timestamp)
    job_name = os.getenv("JOB_NAME", f"mimic-run-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
    print(f"   ğŸ·ï¸ JOB_NAME: {job_name}")
    
    # ë©”íƒ€ë°ì´í„° ë¡œë“œ
    X_full, y_full, metadata = load_raw_data()
    dashboard_data = create_dashboard_results(out, metadata)
    results_dir, dashboard_dir = save_dashboard_compatible_results(out, dashboard_data, job_name)
    
    # 4ï¸âƒ£ Enhanced Performance Visualization with Organized Storage
    print("\nğŸ“ˆ ì„±ëŠ¥ ì‹œê°í™” ë° ì €ì¥...")
    
    # ê²°ê³¼ ë””ë ‰í† ë¦¬ì— ì‹œê°í™” í´ë” ìƒì„±
    visualization_dir = f"{results_dir}/visualizations"
    os.makedirs(visualization_dir, exist_ok=True)
    
    # ê° ëª¨ë¸ë³„ ì„±ëŠ¥ ì‹œê°í™” ìƒì„± ë° ì €ì¥
    for name, mdl_res in out.items():
        if len(np.unique(mdl_res["y_test"])) > 1:
            print(f"  ğŸ“ˆ {name} ì„±ëŠ¥ ì‹œê°í™” ìƒì„±...")
            
            # ê¸°ë³¸ ì„±ëŠ¥ ì‹œê°í™”
            plot_performance_visualization(
                mdl_res["y_test"], mdl_res["pred_prob"], name, visualization_dir
            )
            
            # Feature Importance ì‹œê°í™” (ìˆëŠ” ê²½ìš°)
            if 'feature_importance' in mdl_res:
                print(f"    ğŸ” {name} Feature Importance ì‹œê°í™”...")
                analyze_model_interpretability(
                    mdl_res['model'], 
                    mdl_res['y_test'][:50].reshape(-1, 1, 1),  # Dummy shape for compatibility
                    mdl_res['y_test'][:50], 
                    list(mdl_res['feature_importance'].keys())[:20], 
                    name, 
                    visualization_dir
                )
            
            print(f"    âœ… {name} ì‹œê°í™” ì™„ë£Œ: {visualization_dir}/{name}_*.png")
    
    # ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±
    print(f"  ğŸ“ˆ ëª¨ë¸ ë¹„êµ ì°¨íŠ¸ ìƒì„±...")
    create_model_comparison_charts(out, visualization_dir)
    
    # ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
    print(f"  ğŸ“„ ì„±ëŠ¥ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±...")
    create_performance_report(out, f"{results_dir}/performance_report.txt")
    
    print(f"  âœ… ëª¨ë“  ì‹œê°í™” ì™„ë£Œ: {visualization_dir}")
    
    # 5ï¸âƒ£ Advanced threshold analysis
    print("\nğŸ” ê³ ê¸‰ ì„ê³„ê°’ ë¶„ì„:")
    best_model = max(out.items(), key=lambda x: x[1]['auc'])[0]
    best_result = out[best_model]
    
    print(f"\n--- ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model.upper()} (AUC: {best_result['auc']:.4f}) ---")
    df_thr = threshold_analysis(best_result["y_test"], best_result["pred_prob"], num_points=15)
    print(df_thr.round(3))
    
    # 6ï¸âƒ£ Final summary
    print(f"\nğŸ‰ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print(f"ğŸ† ìµœê³  AUC: {max(result['auc'] for result in out.values()):.4f} ({best_model})")
    print(f"ğŸ“ ê²°ê³¼ ì €ì¥: {results_dir}")
    print(f"ğŸ“Š ëŒ€ì‹œë³´ë“œ: {dashboard_dir}")
    print(f"ğŸ“ˆ ì‹œê°í™”: {results_dir}/visualizations")
    print(f"ğŸ“„ ë¦¬í¬íŠ¸: {results_dir}/performance_report.txt")
    
    # Dashboard statistics
    best_dashboard = dashboard_data[best_model]
    stats = best_dashboard['summary_stats']
    
    print(f"\nğŸ“Š ëŒ€ì‹œë³´ë“œ í†µê³„ (ì‹¤ì œ subject_id ì‚¬ìš©):")
    print(f"  ğŸ‘¥ ì´ í™˜ì: {stats['total_patients']:,}ëª…")
    print(f"  âš ï¸ ê³ ìœ„í—˜: {stats['high_risk_count']:,}ëª…")
    print(f"  ğŸŸ¡ ì¤‘ìœ„í—˜: {stats['medium_risk_count']:,}ëª…")
    print(f"  ğŸŸ¢ ì €ìœ„í—˜: {stats['low_risk_count']:,}ëª…")
    print(f"  ğŸ“ˆ í‰ê·  ìœ„í—˜ë„: {stats['avg_max_risk']:.1%}")
    print(f"  ğŸ•°ï¸ í‰ê·  ì‹œê°„ ìˆ˜: {stats['avg_hour_count']:.1f}ì‹œê°„")
    
    # ì‚¬ë§ë¥  í†µê³„
    mortality = stats['mortality_stats']
    print(f"\nâ˜ ï¸ ì‚¬ë§ë¥  ë¶„ì„:")
    print(f"  ì „ì²´ ì‚¬ë§ë¥ : {mortality['patient_mortality_rate']:.1%} ({mortality['total_deaths']}/{mortality['total_patients']})")
    print(f"  ê³ ìœ„í—˜êµ° ì‚¬ë§ë¥ : {mortality['high_risk_mortality_rate']:.1%}")
    print(f"  ì €ìœ„í—˜êµ° ì‚¬ë§ë¥ : {mortality['low_risk_mortality_rate']:.1%}")
    
    # 7ï¸âƒ£ ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì¤€ë¹„ (Dashboard êµ¬ë™ìš©)
    print("\nğŸš€ ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì¤€ë¹„...")
    predict_api = create_realtime_prediction_api("results/models")
    if predict_api:
        print("âœ… Dashboardìš© ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì¤€ë¹„ ì™„ë£Œ")
        print(f"   ğŸ“‚ ëª¨ë¸ íŒŒì¼: results/models/*_best_model.pth")
        print(f"   ğŸ“Š ìŠ¤ì¼€ì¼ëŸ¬: results/models/data_scaler.pkl")
        print(f"   ğŸ“‹ ë©”íƒ€ë°ì´í„°: results/models/metadata_info.pkl")
    else:
        print("âš ï¸ ì‹¤ì‹œê°„ ì˜ˆì¸¡ API ì¤€ë¹„ ì‹¤íŒ¨")
        
    print(f"\nğŸ¯ EC2 ë°°í¬ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   ğŸ“ ë¡œì»¬: {dashboard_dir}/ (ëª¨ë“  ëŒ€ì‹œë³´ë“œ JSON íŒŒì¼)")
    print(f"   ğŸ“ ë¡œì»¬: results/models/ (ëª¨ë¸ .pth íŒŒì¼, ìŠ¤ì¼€ì¼ëŸ¬, ë©”íƒ€ë°ì´í„°)")
    print(f"   â˜ï¸ S3: s3://say1-7team-bucket/dashboard-results/{job_name}/")
    print(f"   ğŸ“„ app.pyì—ì„œ JOB_NAME={job_name} í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”")
