{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d4b7fb",
   "metadata": {},
   "source": [
    "# 1.sedatives_id_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1982de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>linksto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>225150</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>229420</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>221623</td>\n",
       "      <td>Diazepam (Valium)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>227212</td>\n",
       "      <td>Etomidate (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>221744</td>\n",
       "      <td>Fentanyl</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>225942</td>\n",
       "      <td>Fentanyl (Concentrate)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>225972</td>\n",
       "      <td>Fentanyl (Push)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>221712</td>\n",
       "      <td>Ketamine</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>227211</td>\n",
       "      <td>Ketamine (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>221385</td>\n",
       "      <td>Lorazepam (Ativan)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>221668</td>\n",
       "      <td>Midazolam (Versed)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>225154</td>\n",
       "      <td>Morphine Sulfate</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>222168</td>\n",
       "      <td>Propofol</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>227210</td>\n",
       "      <td>Propofol (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>226224</td>\n",
       "      <td>Propofol Ingredient</td>\n",
       "      <td>ingredientevents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid                       label           linksto\n",
       "1106  225150  Dexmedetomidine (Precedex)       inputevents\n",
       "3599  229420  Dexmedetomidine (Precedex)       inputevents\n",
       "291   221623           Diazepam (Valium)       inputevents\n",
       "2147  227212      Etomidate (Intubation)       chartevents\n",
       "298   221744                    Fentanyl       inputevents\n",
       "1646  225942      Fentanyl (Concentrate)       inputevents\n",
       "1668  225972             Fentanyl (Push)       inputevents\n",
       "296   221712                    Ketamine       inputevents\n",
       "2146  227211       Ketamine (Intubation)       chartevents\n",
       "285   221385          Lorazepam (Ativan)       inputevents\n",
       "294   221668          Midazolam (Versed)       inputevents\n",
       "1110  225154            Morphine Sulfate       inputevents\n",
       "316   222168                    Propofol       inputevents\n",
       "2145  227210       Propofol (Intubation)       chartevents\n",
       "1802  226224         Propofol Ingredient  ingredientevents"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz Î°úÎî© (Ï≤òÏùå Ìïú Î≤àÎßå)\n",
    "import pandas as pd\n",
    "\n",
    "d_items = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "\n",
    "# ÏßÑÏ†ïÏ†ú ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏\n",
    "sedation_keywords = [\n",
    "    \"midazolam\", \"propofol\", \"dexmedetomidine\", \"lorazepam\", \n",
    "    \"fentanyl\", \"etomidate\", \"diazepam\", \"ketamine\", \"morphine\"\n",
    "]\n",
    "\n",
    "# label ÎòêÎäî drugnameÏóêÏÑú ÏßÑÏ†ïÏ†ú ÌïÑÌÑ∞ÎßÅ (ÎåÄÏÜåÎ¨∏Ïûê Î¨¥Ïãú)\n",
    "sedatives = d_items[\n",
    "    d_items['label'].str.lower().str.contains('|'.join(sedation_keywords), na=False)\n",
    "]\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "sedatives[['itemid', 'label', 'linksto']].sort_values(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fbb7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>linksto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>225150</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>229420</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>227212</td>\n",
       "      <td>Etomidate (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>221744</td>\n",
       "      <td>Fentanyl</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>225942</td>\n",
       "      <td>Fentanyl (Concentrate)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>225972</td>\n",
       "      <td>Fentanyl (Push)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>221668</td>\n",
       "      <td>Midazolam (Versed)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>222168</td>\n",
       "      <td>Propofol</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>227210</td>\n",
       "      <td>Propofol (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>226224</td>\n",
       "      <td>Propofol Ingredient</td>\n",
       "      <td>ingredientevents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid                       label           linksto\n",
       "1106  225150  Dexmedetomidine (Precedex)       inputevents\n",
       "3599  229420  Dexmedetomidine (Precedex)       inputevents\n",
       "2147  227212      Etomidate (Intubation)       chartevents\n",
       "298   221744                    Fentanyl       inputevents\n",
       "1646  225942      Fentanyl (Concentrate)       inputevents\n",
       "1668  225972             Fentanyl (Push)       inputevents\n",
       "294   221668          Midazolam (Versed)       inputevents\n",
       "316   222168                    Propofol       inputevents\n",
       "2145  227210       Propofol (Intubation)       chartevents\n",
       "1802  226224         Propofol Ingredient  ingredientevents"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz Î°úÎî© (Ï≤òÏùå Ìïú Î≤àÎßå)\n",
    "import pandas as pd\n",
    "\n",
    "d_items = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "\n",
    "# ÏßÑÏ†ïÏ†ú ÌÇ§ÏõåÎìú Î¶¨Ïä§Ìä∏\n",
    "sedation_keywords = [\n",
    "    \"midazolam\", \"propofol\", \"dexmedetomidine\",\n",
    "    \"fentanyl\", \"remifentanil\", \"etomidate\"\n",
    "]\n",
    "\n",
    "# label ÎòêÎäî drugnameÏóêÏÑú ÏßÑÏ†ïÏ†ú ÌïÑÌÑ∞ÎßÅ (ÎåÄÏÜåÎ¨∏Ïûê Î¨¥Ïãú)\n",
    "sedatives = d_items[\n",
    "    d_items['label'].str.lower().str.contains('|'.join(sedation_keywords), na=False)\n",
    "]\n",
    "\n",
    "# Í≤∞Í≥º ÌôïÏù∏\n",
    "sedatives[['itemid', 'label', 'linksto']].sort_values(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d687466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>linksto</th>\n",
       "      <th>category</th>\n",
       "      <th>unitname</th>\n",
       "      <th>param_type</th>\n",
       "      <th>lownormalvalue</th>\n",
       "      <th>highnormalvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [itemid, label, abbreviation, linksto, category, unitname, param_type, lownormalvalue, highnormalvalue]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz ÏóêÏÑú Remifentanil Í¥ÄÎ†® label Í≤ÄÏÉâ -> ÎùºÎ≤® ÏóÜÏùåÏùÑ ÌôïÏù∏Ìï®\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "df[df[\"label\"].str.lower().str.contains(\"remifentanil|ultiva\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eea4cd",
   "metadata": {},
   "source": [
    "# PySpark Í∏∞Î∞ò ÏßÑÏ†ïÏ†ú Ìà¨ÏïΩ ÏãúÏ†ê Ï∂îÏ∂ú + GCSÏôÄ Îß§Ïπ≠ (Ïä§ÌÉÄÌÑ∞ ÏΩîÎìú)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b7edd",
   "metadata": {},
   "source": [
    "### ‚ë† ÏÉà ÏΩîÌò∏Ìä∏ Í∏∞Ï§Ä ÌïÑÌÑ∞ÎßÅ Ï∂îÍ∞Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769043dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort*\")\n",
    "print(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c7b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 16:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/06 16:33:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Ïù¥ÎØ∏ spark ÏÑ∏ÏÖòÏù¥ ÏûàÏúºÎ©¥ Ïû¨ÏÇ¨Ïö©, ÏóÜÏúºÎ©¥ ÏÉàÎ°ú ÏÉùÏÑ±\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# üîÅ ÏΩîÌò∏Ìä∏ Î®ºÏ†Ä Î∂àÎü¨ÏôÄÏÑú stay_id ÌïÑÌÑ∞ÎßÅ Í∏∞Ï§Ä ÎßåÎì§Í∏∞\n",
    "cohort = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\", header=True, inferSchema=True)\n",
    "cohort_ids = cohort.select(\"stay_id\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a408bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|sedated_flag|  count|\n",
      "+------------+-------+\n",
      "|           1| 619690|\n",
      "|           0|3161090|\n",
      "+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "|gcs_itemid|  total|non_null|\n",
      "+----------+-------+--------+\n",
      "|    223900|1260658| 1260658|\n",
      "|    220739|1262538| 1262538|\n",
      "|    223901|1257584| 1257584|\n",
      "+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, when, lit, max as spark_max, udf\n",
    "from pyspark.sql.types import TimestampType, StructType, StructField, IntegerType\n",
    "import datetime\n",
    "\n",
    "# 1. Spark ÏÑ∏ÏÖò ÏãúÏûë\n",
    "spark = SparkSession.builder.appName(\"GCS Sedation Flag\").getOrCreate()\n",
    "\n",
    "# 2. ÏΩîÌò∏Ìä∏ Î°úÎî© Î∞è stay_id Ï∂îÏ∂ú\n",
    "cohort = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "cohort_ids = cohort.select(\"stay_id\").distinct()\n",
    "\n",
    "# 3. ÏßÑÏ†ïÏ†ú itemid Î∞è ÏïΩÌö® ÏßÄÏÜç ÏãúÍ∞Ñ Ï†ïÏùò (ÏãúÍ∞Ñ Îã®ÏúÑ)\n",
    "sedative_itemids = [\n",
    "    (221319, 4),  # Midazolam (Versed)\n",
    "    (221668, 4),  # Midazolam\n",
    "    (222168, 1),  # Propofol\n",
    "    (221744, 2),  # Fentanyl\n",
    "    (225942, 2),  # Fentanyl (Concentrate)\n",
    "    (225972, 2),  # Fentanyl (Push)\n",
    "    (221320, 4),  # Dexmedetomidine\n",
    "    (229420, 4),  # Dexmedetomidine (Precedex)\n",
    "    (225150, 4),  # Dexmedetomidine (Precedex)\n",
    "    (221195, 4),  # Lorazepam\n",
    "    (227212, 1)   # Etomidate\n",
    "]\n",
    "sedative_ids_only = [x[0] for x in sedative_itemids]\n",
    "\n",
    "# 4. itemid + ÏßÄÏÜçÏãúÍ∞ÑÏùÑ PySpark DataFrameÏúºÎ°ú Î≥ÄÌôò\n",
    "schema = StructType([\n",
    "    StructField(\"itemid\", IntegerType(), False),\n",
    "    StructField(\"window_hours\", IntegerType(), False)\n",
    "])\n",
    "sedative_windows = spark.createDataFrame(sedative_itemids, schema)\n",
    "\n",
    "# 5. ÏßÑÏ†ïÏ†ú Ìà¨ÏïΩ Ïù¥Î≤§Ìä∏ Î°úÎî© Î∞è ÌïÑÌÑ∞ÎßÅ\n",
    "inputevents = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/icu/inputevents.csv.gz\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "sedation_events = inputevents.filter(col(\"itemid\").isin(sedative_ids_only)) \\\n",
    "    .join(cohort_ids, on=\"stay_id\", how=\"inner\") \\\n",
    "    .withColumn(\"starttime\", to_timestamp(\"starttime\")) \\\n",
    "    .join(sedative_windows, on=\"itemid\", how=\"left\")\n",
    "\n",
    "# 6. UDFÎ°ú ÏßÑÏ†ïÏ†ú Ìà¨ÏïΩ Ï¢ÖÎ£å ÏãúÏ†ê(endtime) Í≥ÑÏÇ∞\n",
    "def add_hours(starttime, hours):\n",
    "    if starttime is None or hours is None:\n",
    "        return None\n",
    "    return starttime + datetime.timedelta(hours=int(hours))\n",
    "add_hours_udf = udf(add_hours, TimestampType())\n",
    "\n",
    "sedation_events = sedation_events.withColumn(\n",
    "    \"endtime\", add_hours_udf(col(\"starttime\"), col(\"window_hours\"))\n",
    ").select(\"stay_id\", \"starttime\", \"endtime\")\n",
    "\n",
    "# 7. GCS Í¥ÄÏ∏°Ïπò Î°úÎî© Î∞è ÌïÑÌÑ∞ÎßÅ\n",
    "chartevents = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "gcs_itemids = [220739, 223900, 223901]  # GCS Eye, Verbal, Motor\n",
    "gcs = chartevents.filter(col(\"itemid\").isin(gcs_itemids)) \\\n",
    "    .join(cohort_ids, on=\"stay_id\", how=\"inner\") \\\n",
    "    .withColumn(\"charttime\", to_timestamp(\"charttime\")) \\\n",
    "    .filter(col(\"valuenum\").isNotNull()) \\\n",
    "    .select(\"stay_id\", \"charttime\", \"itemid\", \"valuenum\")\n",
    "\n",
    "# 8. Ï∫êÏãúÎ°ú ÏÑ±Îä• ÏµúÏ†ÅÌôî\n",
    "gcs.cache()\n",
    "sedation_events.cache()\n",
    "\n",
    "# 9. Ï°∞Ïù∏ ÌõÑ ÏßÑÏ†ïÏ†ú Ìà¨ÏïΩ Ïó¨Î∂Ä (sedated_flag) Î∂ÄÏó¨\n",
    "sedated_flagged = gcs.join(\n",
    "    sedation_events,\n",
    "    on=\"stay_id\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"time_diff\",\n",
    "    expr(\"abs(unix_timestamp(charttime) - unix_timestamp(starttime))\")\n",
    ").withColumn(\n",
    "    \"temp_flag\",\n",
    "    when((col(\"charttime\") >= col(\"starttime\")) & (col(\"charttime\") <= col(\"endtime\")), 1).otherwise(0)\n",
    ").groupBy(\"stay_id\", \"charttime\", \"itemid\", \"valuenum\").agg(\n",
    "    expr(\"min(time_diff) as min_time_diff\"),\n",
    "    spark_max(\"temp_flag\").alias(\"sedated_flag\")  # Îã§Ï§ë Ìà¨ÏïΩ Í≥†Î†§\n",
    ")\n",
    "\n",
    "# 10. Ïª¨ÎüºÎ™Ö Ï†ïÎ¶¨\n",
    "sedated_flagged = sedated_flagged.withColumnRenamed(\"itemid\", \"gcs_itemid\")\n",
    "\n",
    "# 11. Í≤∞Í≥º Ï†ÄÏû• (Parquet + ÌååÌã∞ÏÖîÎãù)\n",
    "sedated_flagged.select(\n",
    "    \"stay_id\", \"charttime\", \"gcs_itemid\", \"valuenum\", \"sedated_flag\"\n",
    ").write.mode(\"overwrite\") \\\n",
    "  .partitionBy(\"stay_id\") \\\n",
    "  .parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "\n",
    "# 12. Í∞ÑÎã®Ìïú Í≤ÄÏ¶ù Ï∂úÎ†•\n",
    "sedated_flagged.groupBy(\"sedated_flag\").count().show()\n",
    "sedated_flagged.groupBy(\"gcs_itemid\").agg(\n",
    "    expr(\"count(*) as total\"),\n",
    "    expr(\"count(valuenum) as non_null\")\n",
    ").show()\n",
    "\n",
    "# 13. Spark Ï¢ÖÎ£å\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc1ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 89:>                                                       (0 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4161.759s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 89.0 (TID 38018): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:42:57 ERROR Executor: Exception in task 4.0 in stage 89.0 (TID 38018)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "25/08/06 17:42:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 89.0 (TID 38018),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "25/08/06 17:42:57 ERROR TaskSetManager: Task 4 in stage 89.0 failed 1 times; aborting job\n",
      "25/08/06 17:42:57 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 0.0 in stage 89.0 (TID 38014) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 3.0 in stage 89.0 (TID 38017) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 7.0 in stage 89.0 (TID 38021) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 5.0 in stage 89.0 (TID 38019) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 2.0 in stage 89.0 (TID 38016) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 9.0 in stage 89.0 (TID 38023) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 8.0 in stage 89.0 (TID 38022) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 10.0 in stage 89.0 (TID 38024) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 6.0 in stage 89.0 (TID 38020) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 1.0 in stage 89.0 (TID 38015) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o877.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m gcs_summary_exp3\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/exp3_gcs_masked\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 13. Í≤∞Ï∏° ÌôïÏù∏\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[43mgcs_summary_exp3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(*) as total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(gcs_partial_mean) as non_null_gcs_partial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(verbal_mean) as non_null_verbal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     95\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 14. Spark Ï¢ÖÎ£å\u001b[39;00m\n\u001b[1;32m     98\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o877.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, max as spark_max, min as spark_min, stddev, expr\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Spark ÏÑ∏ÏÖò ÏãúÏûë\n",
    "spark = SparkSession.builder.appName(\"GCS Preprocessing\").getOrCreate()\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "gcs_data = spark.read.parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "\n",
    "# 3. sedated_flagÎ•º charttime Îã®ÏúÑÎ°ú ÏßëÍ≥Ñ\n",
    "sed_flag_summary = gcs_data.groupBy(\"stay_id\", \"charttime\").agg(\n",
    "    spark_max(\"sedated_flag\").alias(\"sedated_flag\")\n",
    ")\n",
    "\n",
    "# 4. GCS ÌîºÎ≤ó\n",
    "gcs_pivoted = gcs_data.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\").agg(spark_max(\"valuenum\")).fillna(0)\n",
    "\n",
    "# 5. GCS Ï¥ùÏ†ê Í≥ÑÏÇ∞ Î∞è sedated_flag Ï∂îÍ∞Ä\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ").join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\")\n",
    "\n",
    "# 6. ÏõêÎ≥∏ Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî (PandasÎ°ú)\n",
    "gcs_total_pd = gcs_pivoted.select(\"gcs_total\", \"sedated_flag\").toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=gcs_total_pd, x=\"gcs_total\", hue=\"sedated_flag\", bins=13, multiple=\"stack\")\n",
    "plt.title(\"GCS Total Distribution by Sedation Flag\")\n",
    "plt.xlabel(\"GCS Total\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"outputs/gcs_total_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. Ïã§Ìóò 1: Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "gcs_summary_exp1 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp1.write.mode(\"overwrite\").parquet(\"outputs/exp1_gcs_summary\")\n",
    "\n",
    "# 8. Ïã§Ìóò 2: GCS + sedation ÎπÑÏú®\n",
    "gcs_summary_exp2 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"sedated_flag\").alias(\"sedated_ratio\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp2.write.mode(\"overwrite\").parquet(\"outputs/exp2_gcs_flagged_summary\")\n",
    "\n",
    "# 9. Ïã§Ìóò 3: sedation Ïãú verbal Ï†úÍ±∞ (ÎßàÏä§ÌÇπ)\n",
    "gcs_masked = gcs_data.withColumn(\n",
    "    \"valuenum_masked\",\n",
    "    when((col(\"sedated_flag\") == 1) & (col(\"gcs_itemid\") == 223900), None).otherwise(col(\"valuenum\"))\n",
    ")\n",
    "\n",
    "# 10. ÌîºÎ≤ó\n",
    "gcs_masked_pivot = gcs_masked.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\").agg(spark_max(\"valuenum_masked\")).fillna(0)\n",
    "\n",
    "# 11. ÎßàÏä§ÌÇπ Ï¥ùÏ†ê Í≥ÑÏÇ∞\n",
    "gcs_masked_pivot = gcs_masked_pivot.join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    "    .withColumn(\n",
    "        \"gcs_partial\",\n",
    "        when(col(\"sedated_flag\") == 1, col(\"223901\"))  # sedation Ïãú motorÎßå\n",
    "        .otherwise(col(\"220739\") + col(\"223900\") + col(\"223901\"))  # ÏïÑÎãàÎ©¥ Ï†ÑÏ≤¥\n",
    "    )\n",
    "\n",
    "# 12. Ïã§Ìóò 3 ÏöîÏïΩ Ï†ÄÏû•\n",
    "gcs_summary_exp3 = gcs_masked_pivot.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_partial\").alias(\"gcs_partial_mean\"),\n",
    "    spark_max(\"223901\").alias(\"motor_max\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\"),\n",
    "    stddev(\"223901\").alias(\"motor_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\")\n",
    ")\n",
    "gcs_summary_exp3.write.mode(\"overwrite\").parquet(\"outputs/exp3_gcs_masked\")\n",
    "\n",
    "# 13. Í≤∞Ï∏° ÌôïÏù∏\n",
    "gcs_summary_exp3.selectExpr(\n",
    "    \"count(*) as total\",\n",
    "    \"count(gcs_partial_mean) as non_null_gcs_partial\",\n",
    "    \"count(verbal_mean) as non_null_verbal\"\n",
    ").show()\n",
    "\n",
    "# 14. Spark Ï¢ÖÎ£å\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a554ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 20:52:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3780780\n",
      "Schema:\n",
      "root\n",
      " |-- charttime: timestamp (nullable = true)\n",
      " |-- gcs_itemid: integer (nullable = true)\n",
      " |-- valuenum: double (nullable = true)\n",
      " |-- sedated_flag: integer (nullable = true)\n",
      " |-- stay_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved successfully\n",
      "Running Experiment 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1 completed: 47606 patients\n",
      "Running Experiment 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp2 completed: 47606 patients\n",
      "Running Experiment 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp3 completed: 47606 patients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "- Total patients: 47606\n",
      "- Non-null GCS partial: 47606\n",
      "- Non-null verbal: 47606\n",
      "Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, max as spark_max, min as spark_min, stddev, expr, isnan, isnull\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Spark ÏÑ∏ÏÖò ÏãúÏûë (Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Preprocessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è Í∏∞Î≥∏ Í≤ÄÏ¶ù\n",
    "gcs_data = spark.read.parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "print(f\"Total records: {gcs_data.count()}\")\n",
    "print(\"Schema:\")\n",
    "gcs_data.printSchema()\n",
    "\n",
    "# 3. Îç∞Ïù¥ÌÑ∞ ÌÉÄÏûÖ Î™ÖÏãúÏ†Å Î≥ÄÌôò (ÌÉÄÏûÖ ÏïàÏ†ïÏÑ± ÌôïÎ≥¥)\n",
    "gcs_data = gcs_data.withColumn(\"valuenum\", col(\"valuenum\").cast(DoubleType())) \\\n",
    "    .withColumn(\"sedated_flag\", col(\"sedated_flag\").cast(IntegerType()))\n",
    "\n",
    "# 4. sedated_flagÎ•º charttime Îã®ÏúÑÎ°ú ÏßëÍ≥Ñ (Ï∫êÏã± Ï∂îÍ∞Ä)\n",
    "sed_flag_summary = gcs_data.groupBy(\"stay_id\", \"charttime\").agg(\n",
    "    spark_max(\"sedated_flag\").alias(\"sedated_flag\")\n",
    ").cache()\n",
    "\n",
    "# 5. GCS ÌîºÎ≤ó (ÏïàÏ†ÑÌïú ÌÉÄÏûÖ Ï≤òÎ¶¨)\n",
    "gcs_pivoted = gcs_data.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\") \\\n",
    "    .agg(spark_max(\"valuenum\")) \\\n",
    "    .fillna(0.0)  # fillnaÏóê Î™ÖÏãúÏ†Å ÌÉÄÏûÖ ÏßÄÏ†ï\n",
    "\n",
    "# Ïª¨ÎüºÎ™Ö ÌôïÏù∏ Î∞è Ï°¥Ïû¨ Ïó¨Î∂Ä Ï≤¥ÌÅ¨\n",
    "pivot_columns = gcs_pivoted.columns\n",
    "expected_cols = [\"220739\", \"223900\", \"223901\"]\n",
    "missing_cols = [col for col in expected_cols if col not in pivot_columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns {missing_cols}\")\n",
    "    for col_name in missing_cols:\n",
    "        gcs_pivoted = gcs_pivoted.withColumn(col_name, lit(0.0))\n",
    "\n",
    "# 6. GCS Ïú†Ìö®ÏÑ± Í≤ÄÏ¶ù Î∞è Ïù¥ÏÉÅÏπò ÎßàÏä§ÌÇπ Ï≤òÎ¶¨ (Validation & Masking)\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ").join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    ".fillna({\"sedated_flag\": 0})  # null sedated_flag Ï≤òÎ¶¨\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# GCS Íµ¨ÏÑ± ÏöîÏÜåÎ≥Ñ Ï†ïÏÉÅ Î≤îÏúÑ:\n",
    "# - Eye:    1~4\n",
    "# - Verbal: 1~5\n",
    "# - Motor:  1~6\n",
    "# ‚Üí Ïú†Ìö® Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇú Í∞íÏùÄ None (null)ÏúºÎ°ú ÎßàÏä§ÌÇπ Ï≤òÎ¶¨\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"220739\", when((col(\"220739\") >= 1) & (col(\"220739\") <= 4), col(\"220739\")).otherwise(None)  # Eye\n",
    ").withColumn(\n",
    "    \"223900\", when((col(\"223900\") >= 1) & (col(\"223900\") <= 5), col(\"223900\")).otherwise(None)  # Verbal\n",
    ").withColumn(\n",
    "    \"223901\", when((col(\"223901\") >= 1) & (col(\"223901\") <= 6), col(\"223901\")).otherwise(None)  # Motor\n",
    ")\n",
    "\n",
    "# ÎßàÏä§ÌÇπ Ï≤òÎ¶¨Îêú Í∞íÎì§ÏùÑ Ïù¥Ïö©Ìï¥ GCS Ï¥ùÏ†ê Ïû¨Í≥ÑÏÇ∞\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Ï∫êÏã±ÏúºÎ°ú ÏÑ±Îä• ÏµúÏ†ÅÌôî\n",
    "gcs_pivoted.cache()\n",
    "\n",
    "# 7. ÏãúÍ∞ÅÌôî (ÏÉòÌîåÎßÅÏúºÎ°ú Î©îÎ™®Î¶¨ Ï†àÏïΩ)\n",
    "try:\n",
    "    gcs_sample = gcs_pivoted.sample(0.1).select(\"gcs_total\", \"sedated_flag\").toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=gcs_sample, x=\"gcs_total\", hue=\"sedated_flag\", bins=13, multiple=\"stack\")\n",
    "    plt.title(\"GCS Total Distribution by Sedation Flag (Sample)\")\n",
    "    plt.xlabel(\"GCS Total\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(\"outputs/gcs_total_distribution.png\")\n",
    "    plt.close()\n",
    "    print(\"Visualization saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {e}\")\n",
    "\n",
    "# 8. Ïã§Ìóò 1: Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©\n",
    "print(\"Running Experiment 1...\")\n",
    "gcs_summary_exp1 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp1.write.mode(\"overwrite\").parquet(\"outputs/exp1_gcs_summary\")\n",
    "print(f\"Exp1 completed: {gcs_summary_exp1.count()} patients\")\n",
    "\n",
    "# 9. Ïã§Ìóò 2: GCS + sedation ÎπÑÏú®\n",
    "print(\"Running Experiment 2...\")\n",
    "gcs_summary_exp2 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"sedated_flag\").alias(\"sedated_ratio\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp2.write.mode(\"overwrite\").parquet(\"outputs/exp2_gcs_flagged_summary\")\n",
    "print(f\"Exp2 completed: {gcs_summary_exp2.count()} patients\")\n",
    "\n",
    "# 10. Ïã§Ìóò 3: sedation Ïãú verbal ÎßàÏä§ÌÇπ (ÏïàÏ†ÑÌïú Ï≤òÎ¶¨)\n",
    "print(\"Running Experiment 3...\")\n",
    "gcs_masked = gcs_data.withColumn(\n",
    "    \"valuenum_masked\",\n",
    "    when((col(\"sedated_flag\") == 1) & (col(\"gcs_itemid\") == 223900), None)\n",
    "    .otherwise(col(\"valuenum\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# 11. ÎßàÏä§ÌÇπÎêú Îç∞Ïù¥ÌÑ∞ ÌîºÎ≤ó (ÌÉÄÏûÖ ÏïàÏ†ïÏÑ±)\n",
    "gcs_masked_pivot = gcs_masked.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\") \\\n",
    "    .agg(spark_max(\"valuenum_masked\")) \\\n",
    "    .fillna(0.0)\n",
    "\n",
    "# Ïª¨Îüº Ï°¥Ïû¨ ÌôïÏù∏\n",
    "if all(col in gcs_masked_pivot.columns for col in expected_cols):\n",
    "    # 12. ÎßàÏä§ÌÇπ Ï¥ùÏ†ê Í≥ÑÏÇ∞\n",
    "    gcs_masked_pivot = gcs_masked_pivot.join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    "        .fillna({\"sedated_flag\": 0}) \\\n",
    "        .withColumn(\n",
    "            \"gcs_partial\",\n",
    "            when(col(\"sedated_flag\") == 1, col(\"223901\"))  # sedation Ïãú motorÎßå\n",
    "            .otherwise(col(\"220739\") + col(\"223900\") + col(\"223901\"))  # ÏïÑÎãàÎ©¥ Ï†ÑÏ≤¥\n",
    "        )\n",
    "    \n",
    "    # 13. Ïã§Ìóò 3 ÏöîÏïΩ Ï†ÄÏû•\n",
    "    gcs_summary_exp3 = gcs_masked_pivot.groupBy(\"stay_id\").agg(\n",
    "        mean(\"gcs_partial\").alias(\"gcs_partial_mean\"),\n",
    "        spark_max(\"223901\").alias(\"motor_max\"),\n",
    "        mean(\"223901\").alias(\"motor_mean\"),\n",
    "        stddev(\"223901\").alias(\"motor_std\"),\n",
    "        mean(\"220739\").alias(\"eye_mean\"),\n",
    "        mean(\"223900\").alias(\"verbal_mean\")\n",
    "    )\n",
    "    gcs_summary_exp3.write.mode(\"overwrite\").parquet(\"outputs/exp3_gcs_masked\")\n",
    "    print(f\"Exp3 completed: {gcs_summary_exp3.count()} patients\")\n",
    "    \n",
    "    # 14. ÏïàÏ†ÑÌïú Í≤∞Ï∏° ÌôïÏù∏\n",
    "    try:\n",
    "        result = gcs_summary_exp3.agg(\n",
    "            expr(\"count(*) as total\"),\n",
    "            expr(\"count(gcs_partial_mean) as non_null_gcs_partial\"),\n",
    "            expr(\"count(verbal_mean) as non_null_verbal\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"Results Summary:\")\n",
    "        print(f\"- Total patients: {result['total']}\")\n",
    "        print(f\"- Non-null GCS partial: {result['non_null_gcs_partial']}\")\n",
    "        print(f\"- Non-null verbal: {result['non_null_verbal']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Result summary failed: {e}\")\n",
    "else:\n",
    "    print(\"Error: Required columns missing in masked pivot\")\n",
    "\n",
    "# 15. Ï∫êÏãú Ìï¥Ï†ú\n",
    "gcs_pivoted.unpersist()\n",
    "sed_flag_summary.unpersist()\n",
    "\n",
    "# 16. Spark Ï¢ÖÎ£å\n",
    "spark.stop()\n",
    "print(\"Processing completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ca997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
