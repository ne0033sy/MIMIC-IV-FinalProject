{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47d4b7fb",
   "metadata": {},
   "source": [
    "# 1.sedatives_id_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1982de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>linksto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>225150</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>229420</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>221623</td>\n",
       "      <td>Diazepam (Valium)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>227212</td>\n",
       "      <td>Etomidate (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>221744</td>\n",
       "      <td>Fentanyl</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>225942</td>\n",
       "      <td>Fentanyl (Concentrate)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>225972</td>\n",
       "      <td>Fentanyl (Push)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>221712</td>\n",
       "      <td>Ketamine</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2146</th>\n",
       "      <td>227211</td>\n",
       "      <td>Ketamine (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>221385</td>\n",
       "      <td>Lorazepam (Ativan)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>221668</td>\n",
       "      <td>Midazolam (Versed)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>225154</td>\n",
       "      <td>Morphine Sulfate</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>222168</td>\n",
       "      <td>Propofol</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>227210</td>\n",
       "      <td>Propofol (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>226224</td>\n",
       "      <td>Propofol Ingredient</td>\n",
       "      <td>ingredientevents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid                       label           linksto\n",
       "1106  225150  Dexmedetomidine (Precedex)       inputevents\n",
       "3599  229420  Dexmedetomidine (Precedex)       inputevents\n",
       "291   221623           Diazepam (Valium)       inputevents\n",
       "2147  227212      Etomidate (Intubation)       chartevents\n",
       "298   221744                    Fentanyl       inputevents\n",
       "1646  225942      Fentanyl (Concentrate)       inputevents\n",
       "1668  225972             Fentanyl (Push)       inputevents\n",
       "296   221712                    Ketamine       inputevents\n",
       "2146  227211       Ketamine (Intubation)       chartevents\n",
       "285   221385          Lorazepam (Ativan)       inputevents\n",
       "294   221668          Midazolam (Versed)       inputevents\n",
       "1110  225154            Morphine Sulfate       inputevents\n",
       "316   222168                    Propofol       inputevents\n",
       "2145  227210       Propofol (Intubation)       chartevents\n",
       "1802  226224         Propofol Ingredient  ingredientevents"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz 로딩 (처음 한 번만)\n",
    "import pandas as pd\n",
    "\n",
    "d_items = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "\n",
    "# 진정제 키워드 리스트\n",
    "sedation_keywords = [\n",
    "    \"midazolam\", \"propofol\", \"dexmedetomidine\", \"lorazepam\", \n",
    "    \"fentanyl\", \"etomidate\", \"diazepam\", \"ketamine\", \"morphine\"\n",
    "]\n",
    "\n",
    "# label 또는 drugname에서 진정제 필터링 (대소문자 무시)\n",
    "sedatives = d_items[\n",
    "    d_items['label'].str.lower().str.contains('|'.join(sedation_keywords), na=False)\n",
    "]\n",
    "\n",
    "# 결과 확인\n",
    "sedatives[['itemid', 'label', 'linksto']].sort_values(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fbb7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>linksto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1106</th>\n",
       "      <td>225150</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>229420</td>\n",
       "      <td>Dexmedetomidine (Precedex)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2147</th>\n",
       "      <td>227212</td>\n",
       "      <td>Etomidate (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>221744</td>\n",
       "      <td>Fentanyl</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>225942</td>\n",
       "      <td>Fentanyl (Concentrate)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668</th>\n",
       "      <td>225972</td>\n",
       "      <td>Fentanyl (Push)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>221668</td>\n",
       "      <td>Midazolam (Versed)</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>222168</td>\n",
       "      <td>Propofol</td>\n",
       "      <td>inputevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>227210</td>\n",
       "      <td>Propofol (Intubation)</td>\n",
       "      <td>chartevents</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1802</th>\n",
       "      <td>226224</td>\n",
       "      <td>Propofol Ingredient</td>\n",
       "      <td>ingredientevents</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      itemid                       label           linksto\n",
       "1106  225150  Dexmedetomidine (Precedex)       inputevents\n",
       "3599  229420  Dexmedetomidine (Precedex)       inputevents\n",
       "2147  227212      Etomidate (Intubation)       chartevents\n",
       "298   221744                    Fentanyl       inputevents\n",
       "1646  225942      Fentanyl (Concentrate)       inputevents\n",
       "1668  225972             Fentanyl (Push)       inputevents\n",
       "294   221668          Midazolam (Versed)       inputevents\n",
       "316   222168                    Propofol       inputevents\n",
       "2145  227210       Propofol (Intubation)       chartevents\n",
       "1802  226224         Propofol Ingredient  ingredientevents"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz 로딩 (처음 한 번만)\n",
    "import pandas as pd\n",
    "\n",
    "d_items = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "\n",
    "# 진정제 키워드 리스트\n",
    "sedation_keywords = [\n",
    "    \"midazolam\", \"propofol\", \"dexmedetomidine\",\n",
    "    \"fentanyl\", \"remifentanil\", \"etomidate\"\n",
    "]\n",
    "\n",
    "# label 또는 drugname에서 진정제 필터링 (대소문자 무시)\n",
    "sedatives = d_items[\n",
    "    d_items['label'].str.lower().str.contains('|'.join(sedation_keywords), na=False)\n",
    "]\n",
    "\n",
    "# 결과 확인\n",
    "sedatives[['itemid', 'label', 'linksto']].sort_values(by='label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d687466f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>label</th>\n",
       "      <th>abbreviation</th>\n",
       "      <th>linksto</th>\n",
       "      <th>category</th>\n",
       "      <th>unitname</th>\n",
       "      <th>param_type</th>\n",
       "      <th>lownormalvalue</th>\n",
       "      <th>highnormalvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [itemid, label, abbreviation, linksto, category, unitname, param_type, lownormalvalue, highnormalvalue]\n",
       "Index: []"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d_items.csv.gz 에서 Remifentanil 관련 label 검색 -> 라벨 없음을 확인함\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/skku_aws165/Documents/MIMIC/icu/d_items.csv.gz\", compression=\"gzip\", low_memory=False)\n",
    "df[df[\"label\"].str.lower().str.contains(\"remifentanil|ultiva\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eea4cd",
   "metadata": {},
   "source": [
    "# PySpark 기반 진정제 투약 시점 추출 + GCS와 매칭 (스타터 코드)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273b7edd",
   "metadata": {},
   "source": [
    "### ① 새 코호트 기준 필터링 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "769043dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "paths = glob.glob(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort*\")\n",
    "print(paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c7b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 16:33:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/06 16:33:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 이미 spark 세션이 있으면 재사용, 없으면 새로 생성\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# 🔁 코호트 먼저 불러와서 stay_id 필터링 기준 만들기\n",
    "cohort = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\", header=True, inferSchema=True)\n",
    "cohort_ids = cohort.select(\"stay_id\").distinct()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a408bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:17:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:17:54 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|sedated_flag|  count|\n",
      "+------------+-------+\n",
      "|           1| 619690|\n",
      "|           0|3161090|\n",
      "+------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------+\n",
      "|gcs_itemid|  total|non_null|\n",
      "+----------+-------+--------+\n",
      "|    223900|1260658| 1260658|\n",
      "|    220739|1262538| 1262538|\n",
      "|    223901|1257584| 1257584|\n",
      "+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, when, lit, max as spark_max, udf\n",
    "from pyspark.sql.types import TimestampType, StructType, StructField, IntegerType\n",
    "import datetime\n",
    "\n",
    "# 1. Spark 세션 시작\n",
    "spark = SparkSession.builder.appName(\"GCS Sedation Flag\").getOrCreate()\n",
    "\n",
    "# 2. 코호트 로딩 및 stay_id 추출\n",
    "cohort = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "cohort_ids = cohort.select(\"stay_id\").distinct()\n",
    "\n",
    "# 3. 진정제 itemid 및 약효 지속 시간 정의 (시간 단위)\n",
    "sedative_itemids = [\n",
    "    (221319, 4),  # Midazolam (Versed)\n",
    "    (221668, 4),  # Midazolam\n",
    "    (222168, 1),  # Propofol\n",
    "    (221744, 2),  # Fentanyl\n",
    "    (225942, 2),  # Fentanyl (Concentrate)\n",
    "    (225972, 2),  # Fentanyl (Push)\n",
    "    (221320, 4),  # Dexmedetomidine\n",
    "    (229420, 4),  # Dexmedetomidine (Precedex)\n",
    "    (225150, 4),  # Dexmedetomidine (Precedex)\n",
    "    (221195, 4),  # Lorazepam\n",
    "    (227212, 1)   # Etomidate\n",
    "]\n",
    "sedative_ids_only = [x[0] for x in sedative_itemids]\n",
    "\n",
    "# 4. itemid + 지속시간을 PySpark DataFrame으로 변환\n",
    "schema = StructType([\n",
    "    StructField(\"itemid\", IntegerType(), False),\n",
    "    StructField(\"window_hours\", IntegerType(), False)\n",
    "])\n",
    "sedative_windows = spark.createDataFrame(sedative_itemids, schema)\n",
    "\n",
    "# 5. 진정제 투약 이벤트 로딩 및 필터링\n",
    "inputevents = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/icu/inputevents.csv.gz\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "sedation_events = inputevents.filter(col(\"itemid\").isin(sedative_ids_only)) \\\n",
    "    .join(cohort_ids, on=\"stay_id\", how=\"inner\") \\\n",
    "    .withColumn(\"starttime\", to_timestamp(\"starttime\")) \\\n",
    "    .join(sedative_windows, on=\"itemid\", how=\"left\")\n",
    "\n",
    "# 6. UDF로 진정제 투약 종료 시점(endtime) 계산\n",
    "def add_hours(starttime, hours):\n",
    "    if starttime is None or hours is None:\n",
    "        return None\n",
    "    return starttime + datetime.timedelta(hours=int(hours))\n",
    "add_hours_udf = udf(add_hours, TimestampType())\n",
    "\n",
    "sedation_events = sedation_events.withColumn(\n",
    "    \"endtime\", add_hours_udf(col(\"starttime\"), col(\"window_hours\"))\n",
    ").select(\"stay_id\", \"starttime\", \"endtime\")\n",
    "\n",
    "# 7. GCS 관측치 로딩 및 필터링\n",
    "chartevents = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "gcs_itemids = [220739, 223900, 223901]  # GCS Eye, Verbal, Motor\n",
    "gcs = chartevents.filter(col(\"itemid\").isin(gcs_itemids)) \\\n",
    "    .join(cohort_ids, on=\"stay_id\", how=\"inner\") \\\n",
    "    .withColumn(\"charttime\", to_timestamp(\"charttime\")) \\\n",
    "    .filter(col(\"valuenum\").isNotNull()) \\\n",
    "    .select(\"stay_id\", \"charttime\", \"itemid\", \"valuenum\")\n",
    "\n",
    "# 8. 캐시로 성능 최적화\n",
    "gcs.cache()\n",
    "sedation_events.cache()\n",
    "\n",
    "# 9. 조인 후 진정제 투약 여부 (sedated_flag) 부여\n",
    "sedated_flagged = gcs.join(\n",
    "    sedation_events,\n",
    "    on=\"stay_id\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"time_diff\",\n",
    "    expr(\"abs(unix_timestamp(charttime) - unix_timestamp(starttime))\")\n",
    ").withColumn(\n",
    "    \"temp_flag\",\n",
    "    when((col(\"charttime\") >= col(\"starttime\")) & (col(\"charttime\") <= col(\"endtime\")), 1).otherwise(0)\n",
    ").groupBy(\"stay_id\", \"charttime\", \"itemid\", \"valuenum\").agg(\n",
    "    expr(\"min(time_diff) as min_time_diff\"),\n",
    "    spark_max(\"temp_flag\").alias(\"sedated_flag\")  # 다중 투약 고려\n",
    ")\n",
    "\n",
    "# 10. 컬럼명 정리\n",
    "sedated_flagged = sedated_flagged.withColumnRenamed(\"itemid\", \"gcs_itemid\")\n",
    "\n",
    "# 11. 결과 저장 (Parquet + 파티셔닝)\n",
    "sedated_flagged.select(\n",
    "    \"stay_id\", \"charttime\", \"gcs_itemid\", \"valuenum\", \"sedated_flag\"\n",
    ").write.mode(\"overwrite\") \\\n",
    "  .partitionBy(\"stay_id\") \\\n",
    "  .parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "\n",
    "# 12. 간단한 검증 출력\n",
    "sedated_flagged.groupBy(\"sedated_flag\").count().show()\n",
    "sedated_flagged.groupBy(\"gcs_itemid\").agg(\n",
    "    expr(\"count(*) as total\"),\n",
    "    expr(\"count(valuenum) as non_null\")\n",
    ").show()\n",
    "\n",
    "# 13. Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dc1ad4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:39:08 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 17:40:22 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "[Stage 89:>                                                       (0 + 10) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4161.759s][warning][gc,alloc] Executor task launch worker for task 4.0 in stage 89.0 (TID 38018): Retried waiting for GCLocker too often allocating 131074 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 17:42:57 ERROR Executor: Exception in task 4.0 in stage 89.0 (TID 38018)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "25/08/06 17:42:57 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 4.0 in stage 89.0 (TID 38018),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "25/08/06 17:42:57 ERROR TaskSetManager: Task 4 in stage 89.0 failed 1 times; aborting job\n",
      "25/08/06 17:42:57 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 0.0 in stage 89.0 (TID 38014) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 3.0 in stage 89.0 (TID 38017) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 7.0 in stage 89.0 (TID 38021) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 5.0 in stage 89.0 (TID 38019) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 2.0 in stage 89.0 (TID 38016) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 9.0 in stage 89.0 (TID 38023) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 8.0 in stage 89.0 (TID 38022) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 10.0 in stage 89.0 (TID 38024) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 6.0 in stage 89.0 (TID 38020) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/08/06 17:42:57 WARN TaskSetManager: Lost task 1.0 in stage 89.0 (TID 38015) (172.16.0.199 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n",
      "\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n",
      "\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n",
      "\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n",
      "\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n",
      "\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n",
      "\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o877.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m gcs_summary_exp3\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs/exp3_gcs_masked\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# 13. 결측 확인\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[43mgcs_summary_exp3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselectExpr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(*) as total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(gcs_partial_mean) as non_null_gcs_partial\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcount(verbal_mean) as non_null_verbal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     95\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# 14. Spark 종료\u001b[39;00m\n\u001b[1;32m     98\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/sql/classic/dataframe.py:285\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/sql/classic/dataframe.py:303\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    298\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    299\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    300\u001b[0m     )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/MIMIC/.venv/lib/python3.9/site-packages/py4j/protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o877.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 89.0 failed 1 times, most recent failure: Lost task 4.0 in stage 89.0 (TID 38018) (172.16.0.199 executor driver): java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\n\tat scala.Option.getOrElse(Option.scala:201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1295)\n\tat scala.Option.foreach(Option.scala:437)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1295)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3207)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\nCaused by: java.lang.OutOfMemoryError: Java heap space\n\tat java.base/java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:64)\n\tat java.base/java.nio.ByteBuffer.allocate(ByteBuffer.java:363)\n\tat org.apache.spark.io.ReadAheadInputStream.<init>(ReadAheadInputStream.java:111)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillReader.<init>(UnsafeSorterSpillReader.java:78)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeSorterSpillWriter.getReader(UnsafeSorterSpillWriter.java:159)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.advanceToNextPage(BytesToBytesMap.java:287)\n\tat org.apache.spark.unsafe.map.BytesToBytesMap$MapIterator.next(BytesToBytesMap.java:315)\n\tat org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap$1.next(UnsafeFixedWidthAggregationMap.java:175)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:196)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.<init>(TungstenAggregationIterator.scala:369)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1(HashAggregateExec.scala:126)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.$anonfun$doExecute$1$adapted(HashAggregateExec.scala:100)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec$$Lambda$5933/0x000000f8024ae270.apply(Unknown Source)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:918)\n\tat org.apache.spark.rdd.RDD$$Lambda$3062/0x000000f801edd260.apply(Unknown Source)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$3017/0x000000f801ec0ba8.apply(Unknown Source)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, max as spark_max, min as spark_min, stddev, expr\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Spark 세션 시작\n",
    "spark = SparkSession.builder.appName(\"GCS Preprocessing\").getOrCreate()\n",
    "\n",
    "# 2. 데이터 로딩\n",
    "gcs_data = spark.read.parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "\n",
    "# 3. sedated_flag를 charttime 단위로 집계\n",
    "sed_flag_summary = gcs_data.groupBy(\"stay_id\", \"charttime\").agg(\n",
    "    spark_max(\"sedated_flag\").alias(\"sedated_flag\")\n",
    ")\n",
    "\n",
    "# 4. GCS 피벗\n",
    "gcs_pivoted = gcs_data.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\").agg(spark_max(\"valuenum\")).fillna(0)\n",
    "\n",
    "# 5. GCS 총점 계산 및 sedated_flag 추가\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ").join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\")\n",
    "\n",
    "# 6. 원본 분포 시각화 (Pandas로)\n",
    "gcs_total_pd = gcs_pivoted.select(\"gcs_total\", \"sedated_flag\").toPandas()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=gcs_total_pd, x=\"gcs_total\", hue=\"sedated_flag\", bins=13, multiple=\"stack\")\n",
    "plt.title(\"GCS Total Distribution by Sedation Flag\")\n",
    "plt.xlabel(\"GCS Total\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.savefig(\"outputs/gcs_total_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "# 7. 실험 1: 그대로 사용\n",
    "gcs_summary_exp1 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp1.write.mode(\"overwrite\").parquet(\"outputs/exp1_gcs_summary\")\n",
    "\n",
    "# 8. 실험 2: GCS + sedation 비율\n",
    "gcs_summary_exp2 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"sedated_flag\").alias(\"sedated_ratio\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp2.write.mode(\"overwrite\").parquet(\"outputs/exp2_gcs_flagged_summary\")\n",
    "\n",
    "# 9. 실험 3: sedation 시 verbal 제거 (마스킹)\n",
    "gcs_masked = gcs_data.withColumn(\n",
    "    \"valuenum_masked\",\n",
    "    when((col(\"sedated_flag\") == 1) & (col(\"gcs_itemid\") == 223900), None).otherwise(col(\"valuenum\"))\n",
    ")\n",
    "\n",
    "# 10. 피벗\n",
    "gcs_masked_pivot = gcs_masked.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\").agg(spark_max(\"valuenum_masked\")).fillna(0)\n",
    "\n",
    "# 11. 마스킹 총점 계산\n",
    "gcs_masked_pivot = gcs_masked_pivot.join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    "    .withColumn(\n",
    "        \"gcs_partial\",\n",
    "        when(col(\"sedated_flag\") == 1, col(\"223901\"))  # sedation 시 motor만\n",
    "        .otherwise(col(\"220739\") + col(\"223900\") + col(\"223901\"))  # 아니면 전체\n",
    "    )\n",
    "\n",
    "# 12. 실험 3 요약 저장\n",
    "gcs_summary_exp3 = gcs_masked_pivot.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_partial\").alias(\"gcs_partial_mean\"),\n",
    "    spark_max(\"223901\").alias(\"motor_max\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\"),\n",
    "    stddev(\"223901\").alias(\"motor_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\")\n",
    ")\n",
    "gcs_summary_exp3.write.mode(\"overwrite\").parquet(\"outputs/exp3_gcs_masked\")\n",
    "\n",
    "# 13. 결측 확인\n",
    "gcs_summary_exp3.selectExpr(\n",
    "    \"count(*) as total\",\n",
    "    \"count(gcs_partial_mean) as non_null_gcs_partial\",\n",
    "    \"count(verbal_mean) as non_null_verbal\"\n",
    ").show()\n",
    "\n",
    "# 14. Spark 종료\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a554ae00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 20:52:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 3780780\n",
      "Schema:\n",
      "root\n",
      " |-- charttime: timestamp (nullable = true)\n",
      " |-- gcs_itemid: integer (nullable = true)\n",
      " |-- valuenum: double (nullable = true)\n",
      " |-- sedated_flag: integer (nullable = true)\n",
      " |-- stay_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization saved successfully\n",
      "Running Experiment 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/06 20:54:38 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp1 completed: 47606 patients\n",
      "Running Experiment 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp2 completed: 47606 patients\n",
      "Running Experiment 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp3 completed: 47606 patients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Summary:\n",
      "- Total patients: 47606\n",
      "- Non-null GCS partial: 47606\n",
      "- Non-null verbal: 47606\n",
      "Processing completed successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, mean, max as spark_max, min as spark_min, stddev, expr, isnan, isnull\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Spark 세션 시작 (메모리 최적화)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GCS Preprocessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. 데이터 로딩 및 기본 검증\n",
    "gcs_data = spark.read.parquet(\"outputs/gcs_with_sedation_flag\")\n",
    "print(f\"Total records: {gcs_data.count()}\")\n",
    "print(\"Schema:\")\n",
    "gcs_data.printSchema()\n",
    "\n",
    "# 3. 데이터 타입 명시적 변환 (타입 안정성 확보)\n",
    "gcs_data = gcs_data.withColumn(\"valuenum\", col(\"valuenum\").cast(DoubleType())) \\\n",
    "    .withColumn(\"sedated_flag\", col(\"sedated_flag\").cast(IntegerType()))\n",
    "\n",
    "# 4. sedated_flag를 charttime 단위로 집계 (캐싱 추가)\n",
    "sed_flag_summary = gcs_data.groupBy(\"stay_id\", \"charttime\").agg(\n",
    "    spark_max(\"sedated_flag\").alias(\"sedated_flag\")\n",
    ").cache()\n",
    "\n",
    "# 5. GCS 피벗 (안전한 타입 처리)\n",
    "gcs_pivoted = gcs_data.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\") \\\n",
    "    .agg(spark_max(\"valuenum\")) \\\n",
    "    .fillna(0.0)  # fillna에 명시적 타입 지정\n",
    "\n",
    "# 컬럼명 확인 및 존재 여부 체크\n",
    "pivot_columns = gcs_pivoted.columns\n",
    "expected_cols = [\"220739\", \"223900\", \"223901\"]\n",
    "missing_cols = [col for col in expected_cols if col not in pivot_columns]\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing columns {missing_cols}\")\n",
    "    for col_name in missing_cols:\n",
    "        gcs_pivoted = gcs_pivoted.withColumn(col_name, lit(0.0))\n",
    "\n",
    "# 6. GCS 유효성 검증 및 이상치 마스킹 처리 (Validation & Masking)\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ").join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    ".fillna({\"sedated_flag\": 0})  # null sedated_flag 처리\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# GCS 구성 요소별 정상 범위:\n",
    "# - Eye:    1~4\n",
    "# - Verbal: 1~5\n",
    "# - Motor:  1~6\n",
    "# → 유효 범위를 벗어난 값은 None (null)으로 마스킹 처리\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"220739\", when((col(\"220739\") >= 1) & (col(\"220739\") <= 4), col(\"220739\")).otherwise(None)  # Eye\n",
    ").withColumn(\n",
    "    \"223900\", when((col(\"223900\") >= 1) & (col(\"223900\") <= 5), col(\"223900\")).otherwise(None)  # Verbal\n",
    ").withColumn(\n",
    "    \"223901\", when((col(\"223901\") >= 1) & (col(\"223901\") <= 6), col(\"223901\")).otherwise(None)  # Motor\n",
    ")\n",
    "\n",
    "# 마스킹 처리된 값들을 이용해 GCS 총점 재계산\n",
    "gcs_pivoted = gcs_pivoted.withColumn(\n",
    "    \"gcs_total\",\n",
    "    col(\"220739\") + col(\"223900\") + col(\"223901\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 캐싱으로 성능 최적화\n",
    "gcs_pivoted.cache()\n",
    "\n",
    "# 7. 시각화 (샘플링으로 메모리 절약)\n",
    "try:\n",
    "    gcs_sample = gcs_pivoted.sample(0.1).select(\"gcs_total\", \"sedated_flag\").toPandas()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=gcs_sample, x=\"gcs_total\", hue=\"sedated_flag\", bins=13, multiple=\"stack\")\n",
    "    plt.title(\"GCS Total Distribution by Sedation Flag (Sample)\")\n",
    "    plt.xlabel(\"GCS Total\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.savefig(\"outputs/gcs_total_distribution.png\")\n",
    "    plt.close()\n",
    "    print(\"Visualization saved successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Visualization failed: {e}\")\n",
    "\n",
    "# 8. 실험 1: 그대로 사용\n",
    "print(\"Running Experiment 1...\")\n",
    "gcs_summary_exp1 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp1.write.mode(\"overwrite\").parquet(\"outputs/exp1_gcs_summary\")\n",
    "print(f\"Exp1 completed: {gcs_summary_exp1.count()} patients\")\n",
    "\n",
    "# 9. 실험 2: GCS + sedation 비율\n",
    "print(\"Running Experiment 2...\")\n",
    "gcs_summary_exp2 = gcs_pivoted.groupBy(\"stay_id\").agg(\n",
    "    mean(\"gcs_total\").alias(\"gcs_mean\"),\n",
    "    spark_max(\"gcs_total\").alias(\"gcs_max\"),\n",
    "    spark_min(\"gcs_total\").alias(\"gcs_min\"),\n",
    "    stddev(\"gcs_total\").alias(\"gcs_std\"),\n",
    "    mean(\"sedated_flag\").alias(\"sedated_ratio\"),\n",
    "    mean(\"220739\").alias(\"eye_mean\"),\n",
    "    mean(\"223900\").alias(\"verbal_mean\"),\n",
    "    mean(\"223901\").alias(\"motor_mean\")\n",
    ")\n",
    "gcs_summary_exp2.write.mode(\"overwrite\").parquet(\"outputs/exp2_gcs_flagged_summary\")\n",
    "print(f\"Exp2 completed: {gcs_summary_exp2.count()} patients\")\n",
    "\n",
    "# 10. 실험 3: sedation 시 verbal 마스킹 (안전한 처리)\n",
    "print(\"Running Experiment 3...\")\n",
    "gcs_masked = gcs_data.withColumn(\n",
    "    \"valuenum_masked\",\n",
    "    when((col(\"sedated_flag\") == 1) & (col(\"gcs_itemid\") == 223900), None)\n",
    "    .otherwise(col(\"valuenum\").cast(DoubleType()))\n",
    ")\n",
    "\n",
    "# 11. 마스킹된 데이터 피벗 (타입 안정성)\n",
    "gcs_masked_pivot = gcs_masked.groupBy(\"stay_id\", \"charttime\").pivot(\"gcs_itemid\") \\\n",
    "    .agg(spark_max(\"valuenum_masked\")) \\\n",
    "    .fillna(0.0)\n",
    "\n",
    "# 컬럼 존재 확인\n",
    "if all(col in gcs_masked_pivot.columns for col in expected_cols):\n",
    "    # 12. 마스킹 총점 계산\n",
    "    gcs_masked_pivot = gcs_masked_pivot.join(sed_flag_summary, on=[\"stay_id\", \"charttime\"], how=\"left\") \\\n",
    "        .fillna({\"sedated_flag\": 0}) \\\n",
    "        .withColumn(\n",
    "            \"gcs_partial\",\n",
    "            when(col(\"sedated_flag\") == 1, col(\"223901\"))  # sedation 시 motor만\n",
    "            .otherwise(col(\"220739\") + col(\"223900\") + col(\"223901\"))  # 아니면 전체\n",
    "        )\n",
    "    \n",
    "    # 13. 실험 3 요약 저장\n",
    "    gcs_summary_exp3 = gcs_masked_pivot.groupBy(\"stay_id\").agg(\n",
    "        mean(\"gcs_partial\").alias(\"gcs_partial_mean\"),\n",
    "        spark_max(\"223901\").alias(\"motor_max\"),\n",
    "        mean(\"223901\").alias(\"motor_mean\"),\n",
    "        stddev(\"223901\").alias(\"motor_std\"),\n",
    "        mean(\"220739\").alias(\"eye_mean\"),\n",
    "        mean(\"223900\").alias(\"verbal_mean\")\n",
    "    )\n",
    "    gcs_summary_exp3.write.mode(\"overwrite\").parquet(\"outputs/exp3_gcs_masked\")\n",
    "    print(f\"Exp3 completed: {gcs_summary_exp3.count()} patients\")\n",
    "    \n",
    "    # 14. 안전한 결측 확인\n",
    "    try:\n",
    "        result = gcs_summary_exp3.agg(\n",
    "            expr(\"count(*) as total\"),\n",
    "            expr(\"count(gcs_partial_mean) as non_null_gcs_partial\"),\n",
    "            expr(\"count(verbal_mean) as non_null_verbal\")\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"Results Summary:\")\n",
    "        print(f\"- Total patients: {result['total']}\")\n",
    "        print(f\"- Non-null GCS partial: {result['non_null_gcs_partial']}\")\n",
    "        print(f\"- Non-null verbal: {result['non_null_verbal']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Result summary failed: {e}\")\n",
    "else:\n",
    "    print(\"Error: Required columns missing in masked pivot\")\n",
    "\n",
    "# 15. 캐시 해제\n",
    "gcs_pivoted.unpersist()\n",
    "sed_flag_summary.unpersist()\n",
    "\n",
    "# 16. Spark 종료\n",
    "spark.stop()\n",
    "print(\"Processing completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ca997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
