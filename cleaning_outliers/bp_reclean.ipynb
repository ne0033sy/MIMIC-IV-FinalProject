{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "609cbc9a",
   "metadata": {},
   "source": [
    "# BP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92faac63",
   "metadata": {},
   "source": [
    "## 0. Spark 세션 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "744a1152",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/11 11:57:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    " from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BP_Cleaning\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3aa663",
   "metadata": {},
   "source": [
    "## 1. 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77227a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BP_Cleaning\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1) chartevents.csv.gz 로딩 (CSV 아님! => csv()로)\n",
    "chartevents_path = \"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\"\n",
    "\n",
    "# 필요한 컬럼 위주 스키마 (속도/메모리 절약)\n",
    "schema = StructType([\n",
    "    StructField(\"subject_id\", IntegerType(), True),\n",
    "    StructField(\"hadm_id\", IntegerType(), True),\n",
    "    StructField(\"stay_id\", IntegerType(), True),\n",
    "    StructField(\"charttime\", TimestampType(), True),\n",
    "    StructField(\"storetime\", TimestampType(), True),   # 없어도 됨\n",
    "    StructField(\"itemid\", IntegerType(), True),\n",
    "    StructField(\"value\", StringType(), True),          # 텍스트 값\n",
    "    StructField(\"valuenum\", DoubleType(), True),       # 수치 값\n",
    "    StructField(\"valueuom\", StringType(), True),       # 단위\n",
    "    # 그 외 컬럼은 필요하면 추가\n",
    "])\n",
    "\n",
    "chartevents_df = spark.read.csv(\n",
    "    chartevents_path,\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    mode=\"DROPMALFORMED\"\n",
    ").select(\"stay_id\",\"charttime\",\"itemid\",\"valuenum\",\"valueuom\")\n",
    "\n",
    "# 2) cohort 로딩\n",
    "stay_id_df = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").select(\"stay_id\").dropDuplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f70605f",
   "metadata": {},
   "source": [
    "## 2. BP Cleaning – Hard Filtering + Fallback (5분 Flat-line)\n",
    "**설명:**  \n",
    "- SBP / DBP / MAP 각각 별도 처리  \n",
    "- IBP 우선, 이상치 시 NIBP로 대체, 둘 다 없으면 결측  \n",
    "- 각 컴포넌트별 정제 결과와 요약 통계 출력  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5c6223c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/11 12:42:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 12:42:56 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:03:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:03:37 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:26:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:26:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:47:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 13:47:08 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:08:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:29:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:29:42 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 14:51:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value\n",
      " Schema: stay_id, charttime, itemid, valuenum\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "25/08/11 15:27:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: stay_id, caregiver_id, storetime, value, valuenum\n",
      " Schema: stay_id, charttime, itemid, valuenum, valueuom\n",
      "Expected: charttime but found: caregiver_id\n",
      "CSV file: file:///Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BP Cleaning – Hard Filtering + Fallback (5min flat-line)\n",
    "# - SBP / DBP / MAP 각각 별도 처리\n",
    "# - IBP 우선, 이상치면 NIBP fallback, 없으면 결측\n",
    "# - 출력: 각 컴포넌트별 정제 결과 + 요약\n",
    "# ============================================================\n",
    "\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# -------------------------\n",
    "# 0) 설정값 (필요시 수정)\n",
    "# -------------------------\n",
    "# MetaVision 기준(자주 쓰는 itemid)\n",
    "ITEMS = {\n",
    "    \"SBP\": {\n",
    "        \"IBP\": [220050],      # Arterial BP Systolic\n",
    "        \"NIBP\": [220179]      # Non Invasive Systolic BP\n",
    "    },\n",
    "    \"DBP\": {\n",
    "        \"IBP\": [220051],      # Arterial BP Diastolic\n",
    "        \"NIBP\": [220180]      # Non Invasive Diastolic BP\n",
    "    },\n",
    "    \"MAP\": {\n",
    "        \"IBP\": [220052],      # Arterial BP Mean\n",
    "        \"NIBP\": [220181]      # Non Invasive Mean BP\n",
    "    }\n",
    "}\n",
    "# CareVue itemid가 필요하면 위 딕셔너리에 추가해서 같이 넣으면 됨.\n",
    "\n",
    "# Extreme outlier 기준\n",
    "EXTREME = {\n",
    "    \"SBP\": (50, 250),\n",
    "    \"DBP\": (30, 150),\n",
    "    \"MAP\": (40, 200)  # MAP은 선택적(원하면 꺼도 됨)\n",
    "}\n",
    "\n",
    "# Flat-line (시간기반 5분 std<2)\n",
    "FLAT_STD_THRESH = 2.0\n",
    "FLAT_WINDOW_SEC = 300  # 5분\n",
    "\n",
    "# -------------------------\n",
    "# 1) 유틸 함수\n",
    "# -------------------------\n",
    "def _prep_component_frames(chartevents_df: DataFrame,\n",
    "                           stay_df: DataFrame,\n",
    "                           itemids_ibp: list,\n",
    "                           itemids_nibp: list,\n",
    "                           value_col: str) -> (DataFrame, DataFrame):\n",
    "    \"\"\"\n",
    "    chartevents에서 해당 컴포넌트의 IBP/NIBP만 추출해 표준 컬럼으로 정리.\n",
    "    반환: (ibp_df, nibp_df) with cols [stay_id, charttime, <value_col>]\n",
    "    \"\"\"\n",
    "    base_cols = [\"stay_id\", \"charttime\", \"itemid\", \"valuenum\"]\n",
    "    # cohort join (stay_id 기준)\n",
    "    df = chartevents_df.select(*base_cols).join(stay_df.select(\"stay_id\").dropDuplicates(), on=\"stay_id\", how=\"inner\")\n",
    "    df = df.filter(F.col(\"valuenum\").isNotNull())\n",
    "\n",
    "    ibp = df.filter(F.col(\"itemid\").isin(itemids_ibp)) \\\n",
    "            .select(\"stay_id\", \"charttime\", F.col(\"valuenum\").alias(value_col)) \\\n",
    "            .withColumn(\"ts\", F.unix_timestamp(\"charttime\"))\n",
    "\n",
    "    nibp = df.filter(F.col(\"itemid\").isin(itemids_nibp)) \\\n",
    "             .select(\"stay_id\", \"charttime\", F.col(\"valuenum\").alias(f\"nibp_{value_col}\"))\n",
    "\n",
    "    return ibp, nibp\n",
    "\n",
    "\n",
    "def _apply_ibp_validity(ibp_df: DataFrame, comp: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    IBP extreme + flat-line(5분 std<2) 적용 → is_valid_ibp 플래그 생성\n",
    "    \"\"\"\n",
    "    lo, hi = EXTREME[comp]\n",
    "    # extreme\n",
    "    ibp = ibp_df.withColumn(\n",
    "        \"is_valid_ext\",\n",
    "        (F.col(comp.lower()).between(lo, hi))\n",
    "    )\n",
    "\n",
    "    # 5분 rolling std (시간기반 윈도우)\n",
    "    w5 = Window.partitionBy(\"stay_id\").orderBy(\"ts\").rangeBetween(-FLAT_WINDOW_SEC, 0)\n",
    "    ibp = ibp.withColumn(f\"std5_{comp.lower()}\", F.stddev(comp.lower()).over(w5))\n",
    "\n",
    "    ibp = ibp.withColumn(\n",
    "        \"flat_artifact\",\n",
    "        (F.col(f\"std5_{comp.lower()}\") < FLAT_STD_THRESH)\n",
    "    )\n",
    "\n",
    "    return ibp.withColumn(\n",
    "        \"is_valid_ibp\",\n",
    "        F.col(\"is_valid_ext\") & (~F.col(\"flat_artifact\"))\n",
    "    )\n",
    "\n",
    "\n",
    "def process_component(chartevents_df: DataFrame,\n",
    "                      stay_df: DataFrame,\n",
    "                      comp: str,\n",
    "                      out_prefix: str = \"outputs\") -> (DataFrame, DataFrame):\n",
    "    \"\"\"\n",
    "    한 컴포넌트(SBP/DBP/MAP)에 대해:\n",
    "      - IBP/NIBP 분리\n",
    "      - IBP extreme + flat-line 적용\n",
    "      - IBP 유효 시 IBP, 아니면 NIBP fallback\n",
    "      - 결과 및 요약 저장\n",
    "    반환: (result_df, summary_df)\n",
    "    \"\"\"\n",
    "    comp = comp.upper()\n",
    "    assert comp in [\"SBP\", \"DBP\", \"MAP\"]\n",
    "    value_col = comp.lower()  # \"sbp\", \"dbp\", \"map\"\n",
    "\n",
    "    ibp_raw, nibp = _prep_component_frames(\n",
    "        chartevents_df,\n",
    "        stay_df,\n",
    "        ITEMS[comp][\"IBP\"],\n",
    "        ITEMS[comp][\"NIBP\"],\n",
    "        value_col\n",
    "    )\n",
    "\n",
    "    ibp = _apply_ibp_validity(ibp_raw, comp)\n",
    "\n",
    "    # merge & fallback\n",
    "    merged = ibp.join(nibp, [\"stay_id\", \"charttime\"], \"outer\")\n",
    "\n",
    "    # 최종값\n",
    "    merged = merged.withColumn(\n",
    "        f\"final_{value_col}\",\n",
    "        F.when(F.col(\"is_valid_ibp\"), F.col(value_col)).otherwise(F.col(f\"nibp_{value_col}\"))\n",
    "    ).withColumn(\n",
    "        f\"{value_col}_source\",\n",
    "        F.when(F.col(\"is_valid_ibp\") & F.col(value_col).isNotNull(), F.lit(\"IBP\"))\n",
    "         .when(F.col(f\"nibp_{value_col}\").isNotNull(), F.lit(\"NIBP\"))\n",
    "         .otherwise(F.lit(\"NA\"))\n",
    "    )\n",
    "\n",
    "    # 저장 (컴포넌트별)\n",
    "    out_path = f\"{out_prefix}/bp_{value_col}_clean.parquet\"\n",
    "    merged.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "# 요약(채택 비율, 유효/아웃라이어/플랫 카운트 등)\n",
    "# ---- 기존 코드 (문제 발생)\n",
    "# summary = merged.select(\n",
    "#     \"stay_id\",\n",
    "#     F.count(F.when(F.col(\"is_valid_ibp\"), True)).alias(\"cnt_valid_ibp\"),\n",
    "#     F.count(F.when(~F.col(\"is_valid_ibp\"), True)).alias(\"cnt_invalid_ibp\"),\n",
    "#     F.count(F.when(F.col(f\"{value_col}_source\") == \"IBP\", True)).alias(\"cnt_final_ibp\"),\n",
    "#     F.count(F.when(F.col(f\"{value_col}_source\") == \"NIBP\", True)).alias(\"cnt_final_nibp\"),\n",
    "#     F.count(F.when(F.col(f\"{value_col}_source\") == \"NA\", True)).alias(\"cnt_final_na\")\n",
    "# ).groupBy().sum()\n",
    "\n",
    "# ---- 수정 코드 (전역 요약)\n",
    "    summary = merged.agg(\n",
    "        F.count(F.when(F.col(\"is_valid_ibp\"), True)).alias(\"cnt_valid_ibp\"),\n",
    "        F.count(F.when(~F.col(\"is_valid_ibp\"), True)).alias(\"cnt_invalid_ibp\"),\n",
    "        F.count(F.when(F.col(f\"{value_col}_source\") == \"IBP\", True)).alias(\"cnt_final_ibp\"),\n",
    "        F.count(F.when(F.col(f\"{value_col}_source\") == \"NIBP\", True)).alias(\"cnt_final_nibp\"),\n",
    "        F.count(F.when(F.col(f\"{value_col}_source\") == \"NA\", True)).alias(\"cnt_final_na\")\n",
    "    )\n",
    "\n",
    "    summary_out = f\"{out_prefix}/bp_{value_col}_summary.parquet\"\n",
    "    summary.write.mode(\"overwrite\").parquet(summary_out)\n",
    "\n",
    "    return merged, summary\n",
    "\n",
    "# -------------------------\n",
    "# 2) 실행부  (경로만 네 환경에 맞게 수정)\n",
    "# -------------------------\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType, StringType\n",
    "\n",
    "# 1) chartevents.csv.gz 로딩 (CSV! => csv() 사용)\n",
    "chartevents_path = \"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\"\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"subject_id\", IntegerType(), True),\n",
    "    StructField(\"hadm_id\", IntegerType(), True),\n",
    "    StructField(\"stay_id\", IntegerType(), True),\n",
    "    StructField(\"charttime\", TimestampType(), True),\n",
    "    StructField(\"storetime\", TimestampType(), True),\n",
    "    StructField(\"itemid\", IntegerType(), True),\n",
    "    StructField(\"value\", StringType(), True),\n",
    "    StructField(\"valuenum\", DoubleType(), True),\n",
    "    StructField(\"valueuom\", StringType(), True),\n",
    "])\n",
    "\n",
    "chartevents_df = spark.read.csv(\n",
    "    chartevents_path,\n",
    "    header=True,\n",
    "    schema=schema,\n",
    "    mode=\"DROPMALFORMED\"\n",
    ").select(\"stay_id\",\"charttime\",\"itemid\",\"valuenum\",\"valueuom\")\n",
    "\n",
    "# 2) cohort 로딩\n",
    "stay_id_df = spark.read.csv(\n",
    "    \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\",\n",
    "    header=True, inferSchema=True\n",
    ").select(\"stay_id\").dropDuplicates()\n",
    "\n",
    "# 3) 실행\n",
    "sbp_res, sbp_sum = process_component(chartevents_df, stay_id_df, \"SBP\", out_prefix=\"outputs\")\n",
    "dbp_res, dbp_sum = process_component(chartevents_df, stay_id_df, \"DBP\", out_prefix=\"outputs\")\n",
    "map_res, map_sum = process_component(chartevents_df, stay_id_df, \"MAP\", out_prefix=\"outputs\")\n",
    "\n",
    "# 4) 최종 병합\n",
    "final_bp = sbp_res.select(\"stay_id\",\"charttime\",\n",
    "                          F.col(\"final_sbp\"), F.col(\"sbp_source\"),\n",
    "                          F.col(\"sbp\").alias(\"ibp_sbp\"),\n",
    "                          F.col(\"nibp_sbp\")) \\\n",
    "    .join(\n",
    "        dbp_res.select(\"stay_id\",\"charttime\",\n",
    "                       F.col(\"final_dbp\"), F.col(\"dbp_source\"),\n",
    "                       F.col(\"dbp\").alias(\"ibp_dbp\"),\n",
    "                       F.col(\"nibp_dbp\")),\n",
    "        [\"stay_id\",\"charttime\"], \"outer\"\n",
    "    ).join(\n",
    "        map_res.select(\"stay_id\",\"charttime\",\n",
    "                       F.col(\"final_map\"), F.col(\"map_source\"),\n",
    "                       F.col(\"map\").alias(\"ibp_map\"),\n",
    "                       F.col(\"nibp_map\")),\n",
    "        [\"stay_id\",\"charttime\"], \"outer\"\n",
    "    )\n",
    "\n",
    "# (참고) MAP 최종값이 없으면 NIBP에서 유도하는 로직 추가하고 싶다면:\n",
    "final_bp = final_bp.withColumn(\n",
    "    \"final_map\",\n",
    "    F.when(F.col(\"final_map\").isNotNull(), F.col(\"final_map\"))\n",
    "     .otherwise(\n",
    "        F.when(F.col(\"nibp_map\").isNotNull(), F.col(\"nibp_map\"))\n",
    "         .otherwise((F.col(\"final_dbp\")*2 + F.col(\"final_sbp\"))/3.0)\n",
    "     )\n",
    ")\n",
    "\n",
    "final_bp.write.mode(\"overwrite\").parquet(\"outputs/bp_final_merged.parquet\")\n",
    "\n",
    "\n",
    "chartevents_df.write.mode(\"overwrite\").parquet(\"outputs/chartevents_cached.parquet\")\n",
    "# 다음 실행부터는\n",
    "# chartevents_df = spark.read.parquet(\"outputs/chartevents_cached.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8b755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
