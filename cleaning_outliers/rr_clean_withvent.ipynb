{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4ca5c6",
   "metadata": {},
   "source": [
    "# RR ì „ì²˜ë¦¬(Vent ì‚¬ìš© ê³ ë ¤)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95f3b7",
   "metadata": {},
   "source": [
    "# RR ì „ì²˜ë¦¬ PySpark ì½”ë“œ ì •ë¦¬ (new_cohort ê¸°ì¤€, ì¡°ê±´ë¶€ ë³´ì¡´ í¬í•¨)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa99f0",
   "metadata": {},
   "source": [
    "## 1. spark ì„¸ì…˜ ìƒì„± & cohort ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24e3d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 21:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, when, lit, concat, max as spark_max  \n",
    "\n",
    "# 1. Spark ì„¸ì…˜ ìƒì„±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIMIC Final - RR Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Cohort íŒŒì¼ ë¡œë”©\n",
    "cohort = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\", header=True, inferSchema=True)\n",
    "cohort = cohort.select(\"subject_id\", \"hadm_id\", \"stay_id\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2bc63",
   "metadata": {},
   "source": [
    "# 2. RR ë°ì´í„° ë¡œë”© ë° í•„í„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535c3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ì²˜ìŒ 1íšŒë§Œ ì‹¤í–‰\n",
    "rr_raw = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\", header=True, inferSchema=True)\n",
    "rr_only = rr_raw.filter(col(\"itemid\") == 220210)\n",
    "rr_only.write.mode(\"overwrite\").parquet(\"/Users/skku_aws165/Documents/MIMIC/preprocessed/rr_only.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a443b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìºì‹œëœ RR parquet íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "|subject_id| hadm_id| stay_id|caregiver_id|          charttime|          storetime|itemid|value|valuenum|valueuom|warning|\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 08:00:00|2150-11-06 09:07:00|220210|   23|    23.0|insp/min|      0|\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 09:00:00|2150-11-06 09:07:00|220210|   26|    26.0|insp/min|      0|\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 10:00:00|2150-11-06 13:15:00|220210|   27|    27.0|insp/min|      0|\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "import os\n",
    "\n",
    "# ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ\n",
    "rr_parquet_path = \"/Users/skku_aws165/Documents/MIMIC/preprocessed/rr_only.parquet\"\n",
    "\n",
    "# RR ë°ì´í„° ë¡œë”©\n",
    "if os.path.exists(rr_parquet_path):\n",
    "    print(\"âœ… ìºì‹œëœ RR parquet íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "    rr_df = spark.read.parquet(rr_parquet_path)\n",
    "else:\n",
    "    print(\"ğŸ”„ RR í•„í„°ë§ ì¤‘ (chartevents.csv.gz â†’ RR only)...\")\n",
    "    rr_raw = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\", header=True, inferSchema=True)\n",
    "    rr_df = rr_raw.filter(col(\"itemid\") == 220210)\n",
    "    rr_df.write.mode(\"overwrite\").parquet(rr_parquet_path)\n",
    "    print(\"âœ… ì €ì¥ ì™„ë£Œ:\", rr_parquet_path)\n",
    "\n",
    "# ê³µí†µ í›„ì²˜ë¦¬ (charttime ë³€í™˜ + cohort í•„í„°)\n",
    "rr_df = rr_df.withColumn(\"charttime\", to_timestamp(\"charttime\"))\n",
    "rr_df = rr_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")\n",
    "\n",
    "# ìƒ˜í”Œ í™•ì¸\n",
    "rr_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e20152",
   "metadata": {},
   "source": [
    "# 3. chart ê¸°ë°˜ Ventilator ì •ë³´ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917f3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. chart eventsì—ì„œ ventilator ê´€ë ¨ itemid ë¡œë”©\n",
    "vent_itemids = [223848, 223849, 223870]\n",
    "chart_vent_df = rr_raw.filter(col(\"itemid\").isin(vent_itemids))\n",
    "chart_vent_df = chart_vent_df.withColumn(\"charttime\", to_timestamp(\"charttime\"))\n",
    "chart_vent_df = chart_vent_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68cce7",
   "metadata": {},
   "source": [
    "# 4. procedureevents ê¸°ë°˜ Ventilation ì •ë³´ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d9d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. procedureevents ë¡œë”© ë° í•„í„°\n",
    "proc_vent_df = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/procedureevents.csv.gz\", header=True, inferSchema=True)\n",
    "proc_vent_df = proc_vent_df.filter(col(\"itemid\").isin([225792, 225794]))\n",
    "proc_vent_df = proc_vent_df.withColumn(\"starttime\", to_timestamp(\"starttime\"))\n",
    "proc_vent_df = proc_vent_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b67ce1",
   "metadata": {},
   "source": [
    "# 5. RR ì¡°ê±´ë¶€ í•„í„°ë§ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a55a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rr_with_ventilator(rr_df, chart_vent_df, proc_vent_df, cohort):\n",
    "    \"\"\"\n",
    "    RR = 0~6 bpm êµ¬ê°„ì— ëŒ€í•´ ventilator ê¸°ë¡ ê¸°ë°˜ ë³´ì¡´ ì—¬ë¶€ ê²°ì •\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import unix_timestamp, abs as spark_abs\n",
    "\n",
    "    # Step 1: RR 0~6 í•„í„°ë§\n",
    "    rr_low_df = rr_df.filter((col(\"valuenum\") > 0) & (col(\"valuenum\") <= 6))\n",
    "\n",
    "    # Step 2: chart events ê¸°ë°˜ ventilator ê¸°ë¡ ì •ë¦¬\n",
    "    vent_itemids = [223848, 223849, 223870]\n",
    "    chart_vent_flag = chart_vent_df.filter(col(\"itemid\").isin(vent_itemids)) \\\n",
    "        .select(\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\").dropDuplicates()\n",
    "\n",
    "    # Step 3: procedure events ê¸°ë°˜ ventilator ê¸°ë¡ ì •ë¦¬\n",
    "    proc_vent_flag = proc_vent_df.select(\n",
    "        \"subject_id\", \"hadm_id\", \"stay_id\", \"starttime\"\n",
    "    ).withColumnRenamed(\"starttime\", \"charttime\").dropDuplicates()\n",
    "\n",
    "    # Step 4: chart events ê¸°ë°˜ RR ë³´ì¡´\n",
    "    rr_keep_chart = rr_low_df.join(\n",
    "        chart_vent_flag,\n",
    "        on=[\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"],\n",
    "        how=\"leftsemi\"\n",
    "    )\n",
    "\n",
    "    # Step 5: procedure events ê¸°ë°˜ RR ë³´ì¡´ (1ì‹œê°„ ì´ë‚´)\n",
    "    rr_proc_join = rr_low_df.alias(\"rr\").join(\n",
    "        proc_vent_flag.alias(\"pv\"),\n",
    "        on=[\"subject_id\", \"hadm_id\", \"stay_id\"],\n",
    "        how=\"inner\"\n",
    "    ).filter(\n",
    "        spark_abs(unix_timestamp(\"rr.charttime\") - unix_timestamp(\"pv.charttime\")) <= 3600\n",
    "    ).select(\"rr.*\")\n",
    "\n",
    "    # âœ… Step 6: ë‘ ì¡°ê±´ ë§Œì¡±í•˜ëŠ” RR ëª¨ë‘ ë³´ì¡´\n",
    "    common_cols = rr_df.columns\n",
    "    rr_keep_chart = rr_keep_chart.select(*common_cols)\n",
    "    rr_proc_join = rr_proc_join.select(*common_cols)\n",
    "    rr_keep_all = rr_keep_chart.union(rr_proc_join).dropDuplicates()\n",
    "\n",
    "    # âœ… Step 7: RR > 6ì€ ëª¨ë‘ ìœ ì§€\n",
    "    rr_normal_df = rr_df.filter(col(\"valuenum\") > 6)\n",
    "\n",
    "    # Step 8: ìµœì¢… ë³‘í•©\n",
    "    rr_final = rr_normal_df.union(rr_keep_all).dropDuplicates()\n",
    "\n",
    "    return rr_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e548cb",
   "metadata": {},
   "source": [
    "# 6. í•¨ìˆ˜ ì ìš© ë° ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8336e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. í•¨ìˆ˜ ì‹¤í–‰\n",
    "rr_final = filter_rr_with_ventilator(rr_df, chart_vent_df, proc_vent_df, cohort)\n",
    "\n",
    "# 8. ì €ì¥\n",
    "rr_final.write.mode(\"overwrite\").parquet(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d59bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CSVë¡œë„ ì €ì¥ (ì£¼ì˜: í° íŒŒì¼ì¼ ê²½ìš° ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŒ)\n",
    "rr_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab517a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë‹¨ì¼ CSV ì €ì¥ ì™„ë£Œ: /Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Spark ì„¸ì…˜ (ì´ë¯¸ ìˆìœ¼ë©´ ìƒëµ)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ë³‘í•©í•  ê²½ë¡œ\n",
    "csv_dir = \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.csv\"\n",
    "merged_csv_path = \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent_merged.csv\"\n",
    "\n",
    "# CSV ë””ë ‰í† ë¦¬ ë¡œë”©\n",
    "df = spark.read.option(\"header\", True).csv(csv_dir)\n",
    "\n",
    "# ë‹¨ì¼ CSVë¡œ ì €ì¥ (SparkëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë³‘ë ¬ ì €ì¥í•˜ë¯€ë¡œ ë‹¤ì‹œ ì €ì¥ + ë³‘í•©)\n",
    "# tmp ê²½ë¡œì— ì €ì¥ í›„, í•˜ë‚˜ë§Œ ë³µì‚¬\n",
    "tmp_path = csv_dir + \"_tmp\"\n",
    "\n",
    "df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(tmp_path)\n",
    "\n",
    "# tmp ë””ë ‰í† ë¦¬ì—ì„œ part-*.csv íŒŒì¼ ì°¾ê¸°\n",
    "for filename in os.listdir(tmp_path):\n",
    "    if filename.startswith(\"part-\") and filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(tmp_path, filename), merged_csv_path)\n",
    "        break\n",
    "\n",
    "# tmp ë””ë ‰í† ë¦¬ ì‚­ì œ\n",
    "shutil.rmtree(tmp_path)\n",
    "\n",
    "print(\"âœ… ë‹¨ì¼ CSV ì €ì¥ ì™„ë£Œ:\", merged_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224d195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
