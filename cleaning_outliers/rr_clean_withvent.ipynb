{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4ca5c6",
   "metadata": {},
   "source": [
    "# RR Ï†ÑÏ≤òÎ¶¨(Vent ÏÇ¨Ïö© Í≥†Î†§)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d95f3b7",
   "metadata": {},
   "source": [
    "# RR Ï†ÑÏ≤òÎ¶¨ PySpark ÏΩîÎìú Ï†ïÎ¶¨ (new_cohort Í∏∞Ï§Ä, Ï°∞Í±¥Î∂Ä Î≥¥Ï°¥ Ìè¨Ìï®)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa99f0",
   "metadata": {},
   "source": [
    "## 1. spark ÏÑ∏ÏÖò ÏÉùÏÑ± & cohort Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e24e3d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/06 21:11:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, expr, when, lit, concat, max as spark_max  \n",
    "\n",
    "# 1. Spark ÏÑ∏ÏÖò ÏÉùÏÑ±\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MIMIC Final - RR Preprocessing\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Cohort ÌååÏùº Î°úÎî©\n",
    "cohort = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/notebooks/final/new_cohort.csv\", header=True, inferSchema=True)\n",
    "cohort = cohort.select(\"subject_id\", \"hadm_id\", \"stay_id\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2bc63",
   "metadata": {},
   "source": [
    "# 2. RR Îç∞Ïù¥ÌÑ∞ Î°úÎî© Î∞è ÌïÑÌÑ∞ÎßÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "535c3528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Ï≤òÏùå 1ÌöåÎßå Ïã§Ìñâ\n",
    "rr_raw = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\", header=True, inferSchema=True)\n",
    "rr_only = rr_raw.filter(col(\"itemid\") == 220210)\n",
    "rr_only.write.mode(\"overwrite\").parquet(\"/Users/skku_aws165/Documents/MIMIC/preprocessed/rr_only.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a443b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ï∫êÏãúÎêú RR parquet ÌååÏùº Î°úÎî© Ï§ë...\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "|subject_id| hadm_id| stay_id|caregiver_id|          charttime|          storetime|itemid|value|valuenum|valueuom|warning|\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 08:00:00|2150-11-06 09:07:00|220210|   23|    23.0|insp/min|      0|\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 09:00:00|2150-11-06 09:07:00|220210|   26|    26.0|insp/min|      0|\n",
      "|  10000690|25860671|37081114|        8787|2150-11-06 10:00:00|2150-11-06 13:15:00|220210|   27|    27.0|insp/min|      0|\n",
      "+----------+--------+--------+------------+-------------------+-------------------+------+-----+--------+--------+-------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp\n",
    "import os\n",
    "\n",
    "# Ï†ÄÏû•Îêú ÌååÏùº Í≤ΩÎ°ú\n",
    "rr_parquet_path = \"/Users/skku_aws165/Documents/MIMIC/preprocessed/rr_only.parquet\"\n",
    "\n",
    "# RR Îç∞Ïù¥ÌÑ∞ Î°úÎî©\n",
    "if os.path.exists(rr_parquet_path):\n",
    "    print(\"‚úÖ Ï∫êÏãúÎêú RR parquet ÌååÏùº Î°úÎî© Ï§ë...\")\n",
    "    rr_df = spark.read.parquet(rr_parquet_path)\n",
    "else:\n",
    "    print(\"üîÑ RR ÌïÑÌÑ∞ÎßÅ Ï§ë (chartevents.csv.gz ‚Üí RR only)...\")\n",
    "    rr_raw = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/chartevents.csv.gz\", header=True, inferSchema=True)\n",
    "    rr_df = rr_raw.filter(col(\"itemid\") == 220210)\n",
    "    rr_df.write.mode(\"overwrite\").parquet(rr_parquet_path)\n",
    "    print(\"‚úÖ Ï†ÄÏû• ÏôÑÎ£å:\", rr_parquet_path)\n",
    "\n",
    "# Í≥µÌÜµ ÌõÑÏ≤òÎ¶¨ (charttime Î≥ÄÌôò + cohort ÌïÑÌÑ∞)\n",
    "rr_df = rr_df.withColumn(\"charttime\", to_timestamp(\"charttime\"))\n",
    "rr_df = rr_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")\n",
    "\n",
    "# ÏÉòÌîå ÌôïÏù∏\n",
    "rr_df.show(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e20152",
   "metadata": {},
   "source": [
    "# 3. chart Í∏∞Î∞ò Ventilator Ï†ïÎ≥¥ Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "917f3359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. chart eventsÏóêÏÑú ventilator Í¥ÄÎ†® itemid Î°úÎî©\n",
    "vent_itemids = [223848, 223849, 223870]\n",
    "chart_vent_df = rr_raw.filter(col(\"itemid\").isin(vent_itemids))\n",
    "chart_vent_df = chart_vent_df.withColumn(\"charttime\", to_timestamp(\"charttime\"))\n",
    "chart_vent_df = chart_vent_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68cce7",
   "metadata": {},
   "source": [
    "# 4. procedureevents Í∏∞Î∞ò Ventilation Ï†ïÎ≥¥ Î°úÎî©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d9d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 6. procedureevents Î°úÎî© Î∞è ÌïÑÌÑ∞\n",
    "proc_vent_df = spark.read.csv(\"/Users/skku_aws165/Documents/MIMIC/icu/procedureevents.csv.gz\", header=True, inferSchema=True)\n",
    "proc_vent_df = proc_vent_df.filter(col(\"itemid\").isin([225792, 225794]))\n",
    "proc_vent_df = proc_vent_df.withColumn(\"starttime\", to_timestamp(\"starttime\"))\n",
    "proc_vent_df = proc_vent_df.join(cohort, on=[\"subject_id\", \"hadm_id\", \"stay_id\"], how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b67ce1",
   "metadata": {},
   "source": [
    "# 5. RR Ï°∞Í±¥Î∂Ä ÌïÑÌÑ∞ÎßÅ Ìï®Ïàò Ï†ïÏùò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a55a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rr_with_ventilator(rr_df, chart_vent_df, proc_vent_df, cohort):\n",
    "    \"\"\"\n",
    "    RR = 0~6 bpm Íµ¨Í∞ÑÏóê ÎåÄÌï¥ ventilator Í∏∞Î°ù Í∏∞Î∞ò Î≥¥Ï°¥ Ïó¨Î∂Ä Í≤∞Ï†ï\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import unix_timestamp, abs as spark_abs\n",
    "\n",
    "    # Step 1: RR 0~6 ÌïÑÌÑ∞ÎßÅ\n",
    "    rr_low_df = rr_df.filter((col(\"valuenum\") > 0) & (col(\"valuenum\") <= 6))\n",
    "\n",
    "    # Step 2: chart events Í∏∞Î∞ò ventilator Í∏∞Î°ù Ï†ïÎ¶¨\n",
    "    vent_itemids = [223848, 223849, 223870]\n",
    "    chart_vent_flag = chart_vent_df.filter(col(\"itemid\").isin(vent_itemids)) \\\n",
    "        .select(\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\").dropDuplicates()\n",
    "\n",
    "    # Step 3: procedure events Í∏∞Î∞ò ventilator Í∏∞Î°ù Ï†ïÎ¶¨\n",
    "    proc_vent_flag = proc_vent_df.select(\n",
    "        \"subject_id\", \"hadm_id\", \"stay_id\", \"starttime\"\n",
    "    ).withColumnRenamed(\"starttime\", \"charttime\").dropDuplicates()\n",
    "\n",
    "    # Step 4: chart events Í∏∞Î∞ò RR Î≥¥Ï°¥\n",
    "    rr_keep_chart = rr_low_df.join(\n",
    "        chart_vent_flag,\n",
    "        on=[\"subject_id\", \"hadm_id\", \"stay_id\", \"charttime\"],\n",
    "        how=\"leftsemi\"\n",
    "    )\n",
    "\n",
    "    # Step 5: procedure events Í∏∞Î∞ò RR Î≥¥Ï°¥ (1ÏãúÍ∞Ñ Ïù¥ÎÇ¥)\n",
    "    rr_proc_join = rr_low_df.alias(\"rr\").join(\n",
    "        proc_vent_flag.alias(\"pv\"),\n",
    "        on=[\"subject_id\", \"hadm_id\", \"stay_id\"],\n",
    "        how=\"inner\"\n",
    "    ).filter(\n",
    "        spark_abs(unix_timestamp(\"rr.charttime\") - unix_timestamp(\"pv.charttime\")) <= 3600\n",
    "    ).select(\"rr.*\")\n",
    "\n",
    "    # ‚úÖ Step 6: Îëê Ï°∞Í±¥ ÎßåÏ°±ÌïòÎäî RR Î™®Îëê Î≥¥Ï°¥\n",
    "    common_cols = rr_df.columns\n",
    "    rr_keep_chart = rr_keep_chart.select(*common_cols)\n",
    "    rr_proc_join = rr_proc_join.select(*common_cols)\n",
    "    rr_keep_all = rr_keep_chart.union(rr_proc_join).dropDuplicates()\n",
    "\n",
    "    # ‚úÖ Step 7: RR > 6ÏùÄ Î™®Îëê Ïú†ÏßÄ\n",
    "    rr_normal_df = rr_df.filter(col(\"valuenum\") > 6)\n",
    "\n",
    "    # Step 8: ÏµúÏ¢Ö Î≥ëÌï©\n",
    "    rr_final = rr_normal_df.union(rr_keep_all).dropDuplicates()\n",
    "\n",
    "    return rr_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e548cb",
   "metadata": {},
   "source": [
    "# 6. Ìï®Ïàò Ï†ÅÏö© Î∞è Ï†ÄÏû•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8336e8e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:26 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 76.00% for 10 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 84.44% for 9 writers\n",
      "25/08/07 13:43:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 7. Ìï®Ïàò Ïã§Ìñâ\n",
    "rr_final = filter_rr_with_ventilator(rr_df, chart_vent_df, proc_vent_df, cohort)\n",
    "\n",
    "# 8. Ï†ÄÏû•\n",
    "rr_final.write.mode(\"overwrite\").parquet(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59d59bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# CSVÎ°úÎèÑ Ï†ÄÏû• (Ï£ºÏùò: ÌÅ∞ ÌååÏùºÏùº Í≤ΩÏö∞ Ïò§Îûò Í±∏Î¶¥ Ïàò ÏûàÏùå)\n",
    "rr_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", True) \\\n",
    "    .csv(\"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ab517a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Îã®Ïùº CSV Ï†ÄÏû• ÏôÑÎ£å: /Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent_merged.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Spark ÏÑ∏ÏÖò (Ïù¥ÎØ∏ ÏûàÏúºÎ©¥ ÏÉùÎûµ)\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Î≥ëÌï©Ìï† Í≤ΩÎ°ú\n",
    "csv_dir = \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent.csv\"\n",
    "merged_csv_path = \"/Users/skku_aws165/Documents/MIMIC/MIMIC-IV-Project/outputs/rr_filtered_with_vent_merged.csv\"\n",
    "\n",
    "# CSV ÎîîÎ†âÌÜ†Î¶¨ Î°úÎî©\n",
    "df = spark.read.option(\"header\", True).csv(csv_dir)\n",
    "\n",
    "# Îã®Ïùº CSVÎ°ú Ï†ÄÏû• (SparkÎäî Í∏∞Î≥∏Ï†ÅÏúºÎ°ú Î≥ëÎ†¨ Ï†ÄÏû•ÌïòÎØÄÎ°ú Îã§Ïãú Ï†ÄÏû• + Î≥ëÌï©)\n",
    "# tmp Í≤ΩÎ°úÏóê Ï†ÄÏû• ÌõÑ, ÌïòÎÇòÎßå Î≥µÏÇ¨\n",
    "tmp_path = csv_dir + \"_tmp\"\n",
    "\n",
    "df.coalesce(1).write.option(\"header\", True).mode(\"overwrite\").csv(tmp_path)\n",
    "\n",
    "# tmp ÎîîÎ†âÌÜ†Î¶¨ÏóêÏÑú part-*.csv ÌååÏùº Ï∞æÍ∏∞\n",
    "for filename in os.listdir(tmp_path):\n",
    "    if filename.startswith(\"part-\") and filename.endswith(\".csv\"):\n",
    "        shutil.move(os.path.join(tmp_path, filename), merged_csv_path)\n",
    "        break\n",
    "\n",
    "# tmp ÎîîÎ†âÌÜ†Î¶¨ ÏÇ≠Ï†ú\n",
    "shutil.rmtree(tmp_path)\n",
    "\n",
    "print(\"‚úÖ Îã®Ïùº CSV Ï†ÄÏû• ÏôÑÎ£å:\", merged_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3224d195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
