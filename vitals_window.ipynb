{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a9cd2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "컬럼별 결측치 수:\n",
      " subject_id    0\n",
      "hadm_id       0\n",
      "stay_id       0\n",
      "charttime     0\n",
      "valuenum      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 파일 경로 지정\n",
    "file_path = \"/Users/skku_aws30/Desktop/ne0033sy/MIMIC-IV-Preprocessing/vitals/rr_filtered_with_vent_merged.csv\"\n",
    "\n",
    "# CSV 파일 로드 및 필요한 컬럼만 선택\n",
    "df = pd.read_csv(file_path, parse_dates=['charttime'])\n",
    "df_filtered = df[['subject_id', 'hadm_id', 'stay_id', 'charttime', 'valuenum']]\n",
    "\n",
    "# subject_id 기준으로 charttime 정렬\n",
    "df_sorted = df_filtered.sort_values(by=['subject_id', 'charttime'])\n",
    "\n",
    "# 같은 파일명으로 저장\n",
    "df_sorted.to_csv(file_path, index=False)\n",
    "\n",
    "# 결측치 확인 (전체 컬럼 대상)\n",
    "missing_summary = df_sorted.isnull().sum()\n",
    "print(\"컬럼별 결측치 수:\\n\", missing_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f8d7a",
   "metadata": {},
   "source": [
    "## RR 균등시계열(bin) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce50a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생존 환자는 outtime, 사망환자는 deathtime을 기준으로 bin 생성\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "rr_data = pd.read_csv('vitals/rr_filtered_with_vent_merged.csv')\n",
    "# 2. datetime 변환\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "rr_data['charttime'] = pd.to_datetime(rr_data['charttime']).dt.tz_localize(None)\n",
    "# 3. 코호트와 RR 데이터 병합\n",
    "rr_merged = rr_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (rr_merged['charttime'] >= rr_merged['intime']) & (rr_merged['charttime'] <= rr_merged['outtime'])\n",
    "rr_filtered = rr_merged[mask].copy()\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in rr_filtered['stay_id'].unique():\n",
    "    patient_data = rr_filtered[rr_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "    \n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 RR 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]['valuenum']\n",
    "        \n",
    "        # 대표값 계산\n",
    "        if len(hour_data) > 0:\n",
    "            rr_mean = hour_data.mean()\n",
    "            rr_last = hour_data.iloc[-1]\n",
    "            rr_min = hour_data.min()\n",
    "            rr_max = hour_data.max()\n",
    "        else:\n",
    "            rr_mean = np.nan\n",
    "            rr_last = np.nan\n",
    "            rr_min = np.nan\n",
    "            rr_max = np.nan\n",
    "        \n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'rr_mean': rr_mean,\n",
    "            'rr_last': rr_last,\n",
    "            'rr_min': rr_min,\n",
    "            'rr_max': rr_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('rr_hourly_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c02a0b",
   "metadata": {},
   "source": [
    "## GCS 균등시계열(bin) 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36fb674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "gcs_data = pd.read_csv('vitals/gcs_filtered_with_sadatedflag.csv')\n",
    "# 2. datetime 변환\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "gcs_data['charttime'] = pd.to_datetime(gcs_data['charttime']).dt.tz_localize(None)\n",
    "# 3. 코호트와 GCS 데이터 병합\n",
    "gcs_merged = gcs_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (gcs_merged['charttime'] >= gcs_merged['intime']) & (gcs_merged['charttime'] <= gcs_merged['outtime'])\n",
    "gcs_filtered = gcs_merged[mask].copy()\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in gcs_filtered['stay_id'].unique():\n",
    "    patient_data = gcs_filtered[gcs_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "    \n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 GCS 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]\n",
    "        \n",
    "        # 대표값 계산\n",
    "        if len(hour_data) > 0:\n",
    "            gcs_mean = hour_data['gcs_total'].mean()\n",
    "            gcs_last = hour_data['gcs_total'].iloc[-1]\n",
    "            gcs_min = hour_data['gcs_total'].min()\n",
    "            gcs_max = hour_data['gcs_total'].max()\n",
    "            # sedated_flag가 한번이라도 1이면 1, 아니면 0\n",
    "            sedated_flag = 1 if (hour_data['sedated_flag'] == 1).any() else 0\n",
    "        else:\n",
    "            gcs_mean = np.nan\n",
    "            gcs_last = np.nan\n",
    "            gcs_min = np.nan\n",
    "            gcs_max = np.nan\n",
    "            sedated_flag = np.nan\n",
    "        \n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'gcs_mean': gcs_mean,\n",
    "            'gcs_last': gcs_last,\n",
    "            'gcs_min': gcs_min,\n",
    "            'gcs_max': gcs_max,\n",
    "            'sedated_flag': sedated_flag\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/gcs_hourly_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158d8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "bp_data = pd.read_csv('vitals/----.csv')\n",
    "# 2. datetime 변환\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "bp_data['charttime'] = pd.to_datetime(bp_data['charttime']).dt.tz_localize(None)\n",
    "# 3. 코호트와 BP 데이터 병합\n",
    "bp_merged = bp_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (bp_merged['charttime'] >= bp_merged['intime']) & (bp_merged['charttime'] <= bp_merged['outtime'])\n",
    "bp_filtered = bp_merged[mask].copy()\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in bp_filtered['stay_id'].unique():\n",
    "    patient_data = bp_filtered[bp_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "    \n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 SBP, DBP, MAP 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]\n",
    "        \n",
    "        # 대표값 계산\n",
    "        if len(hour_data) > 0:\n",
    "            sbp_mean = hour_data['final_sbp'].mean()\n",
    "            sbp_last = hour_data['final_sbp'].iloc[-1]\n",
    "            sbp_min = hour_data['final_sbp'].min()\n",
    "            sbp_max = hour_data['final_sbp'].max()\n",
    "            dbp_mean = hour_data['final_dbp'].mean()\n",
    "            dbp_last = hour_data['final_dbp'].iloc[-1]\n",
    "            dbp_min = hour_data['final_dbp'].min()\n",
    "            dbp_max = hour_data['final_dbp'].max()\n",
    "            map_mean = hour_data['final_map'].mean()\n",
    "            map_last = hour_data['final_map'].iloc[-1]\n",
    "            map_min = hour_data['final_map'].min()\n",
    "            map_max = hour_data['final_map'].max()\n",
    "        else:\n",
    "            sbp_mean = np.nan\n",
    "            sbp_last = np.nan\n",
    "            sbp_min = np.nan\n",
    "            sbp_max = np.nan\n",
    "            dbp_mean = np.nan\n",
    "            dbp_last = np.nan\n",
    "            dbp_min = np.nan\n",
    "            dbp_max = np.nan\n",
    "            map_mean = np.nan\n",
    "            map_last = np.nan\n",
    "            map_min = np.nan\n",
    "            map_max = np.nan\n",
    "            \n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'sbp_mean': sbp_mean,\n",
    "            'sbp_last': sbp_last,\n",
    "            'sbp_min': sbp_min,\n",
    "            'sbp_max': sbp_max,\n",
    "            'dbp_mean': dbp_mean,\n",
    "            'dbp_last': dbp_last,\n",
    "            'dbp_min': dbp_min,\n",
    "            'dbp_max': dbp_max,\n",
    "            'map_mean': map_mean,\n",
    "            'map_last': map_last,\n",
    "            'map_min': map_min,\n",
    "            'map_max': map_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/bp_hourly_bins.csv', index=False)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5691a62d",
   "metadata": {},
   "source": [
    "## HR 균등시계열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfe52f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "hr_data = pd.read_csv('vitals/hr_filtered_merged.csv')\n",
    "# 2. 데이터 전처리\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "hr_data['charttime'] = pd.to_datetime(hr_data['charttime']).dt.tz_localize(None)\n",
    "# 3. 코호트와 HR 데이터 병합\n",
    "hr_merged = hr_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (hr_merged['charttime'] >= hr_merged['intime']) & (hr_merged['charttime'] <= hr_merged['outtime'])\n",
    "hr_filtered = hr_merged[mask].copy()\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in hr_filtered['stay_id'].unique():\n",
    "    patient_data = hr_filtered[hr_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "\n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 HR 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]['valuenum']\n",
    "        \n",
    "        # 대표값 계산\n",
    "        if len(hour_data) > 0:\n",
    "            hr_mean = hour_data.mean()\n",
    "            hr_last = hour_data.iloc[-1]\n",
    "            hr_min = hour_data.min()\n",
    "            hr_max = hour_data.max()\n",
    "        else:\n",
    "            hr_mean = np.nan\n",
    "            hr_last = np.nan\n",
    "            hr_min = np.nan\n",
    "            hr_max = np.nan\n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'hr_mean': hr_mean,\n",
    "            'hr_last': hr_last,\n",
    "            'hr_min': hr_min,\n",
    "            'hr_max': hr_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/hr_hourly_bins.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f64d3f5",
   "metadata": {},
   "source": [
    "## Temp 균등 시계열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de9911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "temp_data = pd.read_csv('vitals/temp_filtered_merged.csv')\n",
    "# 2. 데이터 전처리\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "temp_data['charttime'] = pd.to_datetime(temp_data['charttime']).dt.tz_localize(None)\n",
    "# 3. 코호트와 TEMP 데이터 병합\n",
    "temp_merged = temp_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (temp_merged['charttime'] >= temp_merged['intime']) & (temp_merged['charttime'] <= temp_merged['outtime'])\n",
    "temp_filtered = temp_merged[mask].copy()\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in temp_filtered['stay_id'].unique():\n",
    "    patient_data = temp_filtered[temp_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "\n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 TEMP 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]['temperature_celsius']\n",
    "        \n",
    "        # 대표값 계산 (소수점 셋째자리까지)\n",
    "        if len(hour_data) > 0:\n",
    "            temp_mean = round(hour_data.mean(), 3)\n",
    "            temp_last = round(hour_data.iloc[-1], 3)\n",
    "            temp_min = round(hour_data.min(), 3)\n",
    "            temp_max = round(hour_data.max(), 3)\n",
    "        else:\n",
    "            temp_mean = np.nan\n",
    "            temp_last = np.nan\n",
    "            temp_min = np.nan\n",
    "            temp_max = np.nan\n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'temp_mean': temp_mean,\n",
    "            'temp_last': temp_last,\n",
    "            'temp_min': temp_min,\n",
    "            'temp_max': temp_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/temp_hourly_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c50f7",
   "metadata": {},
   "source": [
    "## SpO2 균등 시계열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c81bba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "spo2_data = pd.read_csv('vitals/spo2_filtered_merged.csv')\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "spo2_data['charttime'] = pd.to_datetime(spo2_data['charttime']).dt.tz_localize(None)\n",
    "\n",
    "# 3. 코호트와 SpO2 데이터 병합\n",
    "spo2_merged = spo2_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (spo2_merged['charttime'] >= spo2_merged['intime']) & (spo2_merged['charttime'] <= spo2_merged['outtime'])\n",
    "spo2_filtered = spo2_merged[mask].copy()\n",
    "\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in spo2_filtered['stay_id'].unique():\n",
    "    patient_data = spo2_filtered[spo2_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "\n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 SpO2 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]['valuenum']\n",
    "        \n",
    "        # 대표값 계산 (소수점 셋째자리까지)\n",
    "        if len(hour_data) > 0:\n",
    "            spo2_mean = hour_data.mean()\n",
    "            spo2_last = hour_data.iloc[-1]\n",
    "            spo2_min = hour_data.min()\n",
    "            spo2_max = hour_data.max()\n",
    "        else:\n",
    "            spo2_mean = np.nan\n",
    "            spo2_last = np.nan\n",
    "            spo2_min = np.nan\n",
    "            spo2_max = np.nan\n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'spo2_mean': spo2_mean,\n",
    "            'spo2_last': spo2_last,\n",
    "            'spo2_min': spo2_min,\n",
    "            'spo2_max': spo2_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/spo2_hourly_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ea015",
   "metadata": {},
   "source": [
    "## BP (SBP/DBP/MAP) 균등 시계열 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "572752f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# 1. 데이터 로드\n",
    "cohort = pd.read_csv('cohort.csv')\n",
    "bp_data = pd.read_csv('vitals/bp_final_merged.csv')\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "bp_data['charttime'] = pd.to_datetime(bp_data['charttime']).dt.tz_localize(None)\n",
    "\n",
    "# 3. 코호트와 BP 데이터 병합\n",
    "bp_merged = bp_data.merge(cohort[['stay_id', 'intime', 'outtime', 'deathtime']], on='stay_id', how='inner')\n",
    "\n",
    "# 4. ICU 재원 기간 내 데이터만 필터링\n",
    "mask = (bp_merged['charttime'] >= bp_merged['intime']) & (bp_merged['charttime'] <= bp_merged['outtime'])\n",
    "bp_filtered = bp_merged[mask].copy()\n",
    "\n",
    "# 5. 1시간 단위 bin 생성 및 대표값 계산\n",
    "result_list = []\n",
    "for stay_id in bp_filtered['stay_id'].unique():\n",
    "    patient_data = bp_filtered[bp_filtered['stay_id'] == stay_id].copy()\n",
    "    patient_cohort = cohort[cohort['stay_id'] == stay_id].iloc[0]\n",
    "\n",
    "    intime = patient_cohort['intime']\n",
    "    outtime = patient_cohort['outtime']\n",
    "    deathtime = patient_cohort['deathtime']\n",
    "    icu_los_hours = patient_cohort['icu_los_hours']\n",
    "    \n",
    "    # bin 생성 종료 시간 결정\n",
    "    if pd.notna(deathtime):\n",
    "        end_time = deathtime  # 사망 환자는 deathtime까지\n",
    "    else:\n",
    "        end_time = outtime    # 생존 환자는 outtime까지\n",
    "    \n",
    "    # intime을 기준으로 1시간 단위 bin 생성\n",
    "    total_hours = int(np.ceil(icu_los_hours))\n",
    "    \n",
    "    for hour in range(total_hours):\n",
    "        # 시간 범위 정의\n",
    "        bin_start = intime + timedelta(hours=hour)\n",
    "        bin_end = intime + timedelta(hours=hour+1)\n",
    "        \n",
    "        # end_time을 넘지 않도록 조정\n",
    "        if bin_end > end_time:\n",
    "            bin_end = end_time\n",
    "        \n",
    "        # bin_start가 end_time을 넘으면 중단\n",
    "        if bin_start >= end_time:\n",
    "            break\n",
    "        \n",
    "        # 해당 시간 범위의 BP 데이터 추출\n",
    "        hour_mask = (patient_data['charttime'] >= bin_start) & (patient_data['charttime'] < bin_end)\n",
    "        hour_data = patient_data[hour_mask]\n",
    "        \n",
    "        # 대표값 계산 (반올림 없이 원본 값 사용)\n",
    "        if len(hour_data) > 0:\n",
    "            sbp_mean = hour_data['final_sbp'].mean()\n",
    "            sbp_last = hour_data['final_sbp'].iloc[-1]\n",
    "            sbp_min = hour_data['final_sbp'].min()\n",
    "            sbp_max = hour_data['final_sbp'].max()\n",
    "            \n",
    "            dbp_mean = hour_data['final_dbp'].mean()\n",
    "            dbp_last = hour_data['final_dbp'].iloc[-1]\n",
    "            dbp_min = hour_data['final_dbp'].min()\n",
    "            dbp_max = hour_data['final_dbp'].max()\n",
    "            \n",
    "            map_mean = hour_data['final_map'].mean()\n",
    "            map_last = hour_data['final_map'].iloc[-1]\n",
    "            map_min = hour_data['final_map'].min()\n",
    "            map_max = hour_data['final_map'].max()\n",
    "        else:\n",
    "            sbp_mean = sbp_last = sbp_min = sbp_max = np.nan\n",
    "            dbp_mean = dbp_last = dbp_min = dbp_max = np.nan\n",
    "            map_mean = map_last = map_min = map_max = np.nan\n",
    "        # 결과 저장\n",
    "        result_list.append({\n",
    "            'subject_id': patient_cohort['subject_id'],\n",
    "            'hadm_id': patient_cohort['hadm_id'],\n",
    "            'stay_id': stay_id,\n",
    "            'hour_from_intime': hour,\n",
    "            'bin_start': bin_start,\n",
    "            'bin_end': bin_end,\n",
    "            'sbp_mean': sbp_mean,\n",
    "            'sbp_last': sbp_last,\n",
    "            'sbp_min': sbp_min,\n",
    "            'sbp_max': sbp_max,\n",
    "            'dbp_mean': dbp_mean,\n",
    "            'dbp_last': dbp_last,\n",
    "            'dbp_min': dbp_min,\n",
    "            'dbp_max': dbp_max,\n",
    "            'map_mean': map_mean,\n",
    "            'map_last': map_last,\n",
    "            'map_min': map_min,\n",
    "            'map_max': map_max\n",
    "        })\n",
    "# 6. 결과를 DataFrame으로 변환 및 저장\n",
    "result_df = pd.DataFrame(result_list)\n",
    "result_df.to_csv('hourly_bins/bp_hourly_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07b7c0",
   "metadata": {},
   "source": [
    "## Vitals 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f141203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "병합 완료! 결과 파일: hourly_bins/all_features_hourly_bins_inner.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# 파일 경로와 피쳐 접두어 매핑\n",
    "file_info = [\n",
    "    ('hourly_bins/bp_hourly_bins.csv', ['sbp', 'dbp', 'map']),\n",
    "    ('hourly_bins/hr_hourly_bins.csv', ['hr']),\n",
    "    ('hourly_bins/temp_hourly_bins.csv', ['temp']),\n",
    "    ('hourly_bins/spo2_hourly_bins.csv', ['spo2']),\n",
    "    ('hourly_bins/rr_hourly_bins.csv', ['rr']),\n",
    "    ('hourly_bins/gcs_hourly_bins.csv', ['gcs']) # sedated_flag 포함\n",
    "]\n",
    "\n",
    "# 공통 key\n",
    "merge_keys = ['subject_id', 'hadm_id', 'stay_id', 'hour_from_intime', 'bin_start', 'bin_end']\n",
    "\n",
    "# 파일별 데이터프레임 로드\n",
    "dfs = []\n",
    "for path, _ in file_info:\n",
    "    df = pd.read_csv(path)\n",
    "    dfs.append(df)\n",
    "\n",
    "# 순차적으로 inner 병합\n",
    "merged = reduce(lambda left, right: pd.merge(left, right, on=merge_keys, how='inner'), dfs)\n",
    "\n",
    "# 저장\n",
    "merged.to_csv('hourly_bins/all_features_hourly_bins_inner.csv', index=False)\n",
    "print(\"병합 완료! 결과 파일: hourly_bins/all_features_hourly_bins_inner.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a3ca2",
   "metadata": {},
   "source": [
    "# sliding window 구성\n",
    "- 관찰 윈도우 18h, 예측 윈도우 6h, 예측 간격(시작시점) 8h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b3f22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 데이터 로딩...\n",
      "   - 전체 bin 수: 4414240\n",
      "   - 환자 수: 47339\n",
      "2. 데이터 그룹화 및 정렬...\n",
      "3. 슬라이딩 윈도우 생성...\n",
      "   진행률: 1000/47339 (2.1%)\n",
      "   진행률: 2000/47339 (4.2%)\n",
      "   진행률: 3000/47339 (6.3%)\n",
      "   진행률: 4000/47339 (8.4%)\n",
      "   진행률: 5000/47339 (10.6%)\n",
      "   진행률: 6000/47339 (12.7%)\n",
      "   진행률: 7000/47339 (14.8%)\n",
      "   진행률: 8000/47339 (16.9%)\n",
      "   진행률: 9000/47339 (19.0%)\n",
      "   진행률: 10000/47339 (21.1%)\n",
      "   진행률: 11000/47339 (23.2%)\n",
      "   진행률: 12000/47339 (25.3%)\n",
      "   진행률: 13000/47339 (27.5%)\n",
      "   진행률: 14000/47339 (29.6%)\n",
      "   진행률: 15000/47339 (31.7%)\n",
      "   진행률: 16000/47339 (33.8%)\n",
      "   진행률: 17000/47339 (35.9%)\n",
      "   진행률: 18000/47339 (38.0%)\n",
      "   진행률: 19000/47339 (40.1%)\n",
      "   진행률: 20000/47339 (42.2%)\n",
      "   진행률: 21000/47339 (44.4%)\n",
      "   진행률: 22000/47339 (46.5%)\n",
      "   진행률: 23000/47339 (48.6%)\n",
      "   진행률: 24000/47339 (50.7%)\n",
      "   진행률: 25000/47339 (52.8%)\n",
      "   진행률: 26000/47339 (54.9%)\n",
      "   진행률: 27000/47339 (57.0%)\n",
      "   진행률: 28000/47339 (59.1%)\n",
      "   진행률: 29000/47339 (61.3%)\n",
      "   진행률: 30000/47339 (63.4%)\n",
      "   진행률: 31000/47339 (65.5%)\n",
      "   진행률: 32000/47339 (67.6%)\n",
      "   진행률: 33000/47339 (69.7%)\n",
      "   진행률: 34000/47339 (71.8%)\n",
      "   진행률: 35000/47339 (73.9%)\n",
      "   진행률: 36000/47339 (76.0%)\n",
      "   진행률: 37000/47339 (78.2%)\n",
      "   진행률: 38000/47339 (80.3%)\n",
      "   진행률: 39000/47339 (82.4%)\n",
      "   진행률: 40000/47339 (84.5%)\n",
      "   진행률: 41000/47339 (86.6%)\n",
      "   진행률: 42000/47339 (88.7%)\n",
      "   진행률: 43000/47339 (90.8%)\n",
      "   진행률: 44000/47339 (92.9%)\n",
      "   진행률: 45000/47339 (95.1%)\n",
      "   진행률: 46000/47339 (97.2%)\n",
      "   진행률: 47000/47339 (99.3%)\n",
      "\n",
      "4. 슬라이딩 윈도우 생성 완료!\n",
      "5. 결과 정리...\n",
      "   - 생성된 총 윈도우 수: 436742\n",
      "   - 윈도우가 있는 환자 수: 47209\n",
      "   - 환자별 평균 윈도우 수: 9.3\n",
      "   - 사망 라벨 1인 윈도우: 433 (0.1%)\n",
      "   - 평균 관찰 윈도우 완성도: 1.000\n",
      "6. 결과 저장...\n",
      "슬라이딩 윈도우 생성 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드\n",
    "print(\"1. 데이터 로딩...\")\n",
    "data = pd.read_csv(\"hourly_bins/all_features_hourly_bins_inner.csv\")\n",
    "cohort = pd.read_csv(\"cohort.csv\")\n",
    "\n",
    "# datetime 변환\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "\n",
    "print(f\"   - 전체 bin 수: {len(data)}\")\n",
    "print(f\"   - 환자 수: {data['subject_id'].nunique()}\")\n",
    "\n",
    "# 2. 윈도우 설정\n",
    "obs_window_hours = 18  # 관찰 윈도우\n",
    "pred_window_hours = 6   # 예측 윈도우\n",
    "pred_interval_hours = 8 # 예측 간격\n",
    "\n",
    "# 3. 코호트 정보와 병합\n",
    "data_with_cohort = data.merge(cohort[['subject_id', 'stay_id', 'intime', 'outtime', 'deathtime']], \n",
    "                              on=['subject_id', 'stay_id'], how='inner')\n",
    "\n",
    "# 4. subject_id 기준으로 그룹화 및 정렬\n",
    "print(\"2. 데이터 그룹화 및 정렬...\")\n",
    "data_grouped = data_with_cohort.groupby('subject_id')\n",
    "\n",
    "# 피처 컬럼 정의\n",
    "feature_cols = [col for col in data.columns \n",
    "               if col not in ['subject_id', 'stay_id', 'hour_from_intime', 'subject_id', 'hadm_id', 'bin_start', 'bin_end']]\n",
    "\n",
    "# 5. 벡터화된 슬라이딩 윈도우 생성\n",
    "print(\"3. 슬라이딩 윈도우 생성...\")\n",
    "\n",
    "windows = []\n",
    "total_subjects = len(data_grouped)\n",
    "processed_subjects = 0\n",
    "\n",
    "for subject_id, subject_data in data_grouped:\n",
    "    processed_subjects += 1\n",
    "    if processed_subjects % 1000 == 0:\n",
    "        print(f\"   진행률: {processed_subjects}/{total_subjects} ({processed_subjects/total_subjects*100:.1f}%)\")\n",
    "    \n",
    "    # 환자별 데이터 정렬\n",
    "    subject_data = subject_data.sort_values('hour_from_intime').reset_index(drop=True)\n",
    "    \n",
    "    # 환자별 코호트 정보\n",
    "    stay_id = subject_data['stay_id'].iloc[0]\n",
    "    intime = subject_data['intime'].iloc[0]\n",
    "    outtime = subject_data['outtime'].iloc[0]  \n",
    "    deathtime = subject_data['deathtime'].iloc[0]\n",
    "    \n",
    "    # 최대 시간\n",
    "    max_hour = subject_data['hour_from_intime'].max()\n",
    "    \n",
    "    # 가능한 모든 윈도우 시작점 계산 (벡터화)\n",
    "    possible_starts = np.arange(0, max_hour + 1 - obs_window_hours - pred_window_hours + 1, pred_interval_hours)\n",
    "    \n",
    "    # 각 시작점에 대해 윈도우 생성\n",
    "    for window_id, start_hour in enumerate(possible_starts):\n",
    "        obs_start = start_hour\n",
    "        obs_end = start_hour + obs_window_hours\n",
    "        pred_start = obs_end\n",
    "        pred_end = obs_end + pred_window_hours\n",
    "        \n",
    "        # 관찰 윈도우 데이터 추출 (벡터화)\n",
    "        obs_mask = (subject_data['hour_from_intime'] >= obs_start) & (subject_data['hour_from_intime'] < obs_end)\n",
    "        obs_window = subject_data[obs_mask]\n",
    "        \n",
    "        # 관찰 윈도우 완성도 검사\n",
    "        expected_obs_hours = obs_window_hours\n",
    "        actual_obs_hours = len(obs_window)\n",
    "        obs_completeness = actual_obs_hours / expected_obs_hours\n",
    "        \n",
    "        if obs_completeness < 0.3:\n",
    "            continue\n",
    "        \n",
    "        # 사망 라벨 결정 (벡터화)\n",
    "        pred_window_start_time = intime + pd.Timedelta(hours=pred_start)\n",
    "        pred_window_end_time = intime + pd.Timedelta(hours=pred_end)\n",
    "        \n",
    "        death_in_pred_window = 0\n",
    "        if pd.notna(deathtime):\n",
    "            if pred_window_start_time <= deathtime <= pred_window_end_time:\n",
    "                death_in_pred_window = 1\n",
    "        \n",
    "        # 시퀀스 데이터 생성 (벡터화)\n",
    "        # 18시간 범위의 hour_from_intime 생성\n",
    "        hour_range = np.arange(obs_start, obs_end)\n",
    "        \n",
    "        # 실제 데이터와 매핑을 위한 인덱스 생성\n",
    "        obs_window_indexed = obs_window.set_index('hour_from_intime')\n",
    "        \n",
    "        # 18시간 시퀀스 데이터 생성\n",
    "        sequence_data = []\n",
    "        for hour in hour_range:\n",
    "            if hour in obs_window_indexed.index:\n",
    "                # 해당 시간의 데이터가 있음\n",
    "                hour_features = obs_window_indexed.loc[hour, feature_cols].values\n",
    "            else:\n",
    "                # 해당 시간의 데이터가 없음 - NaN으로 채움\n",
    "                hour_features = np.full(len(feature_cols), np.nan)\n",
    "            \n",
    "            sequence_data.append(hour_features.tolist())\n",
    "        \n",
    "        # 윈도우 정보 저장\n",
    "        window_info = {\n",
    "            'subject_id': subject_id,\n",
    "            'stay_id': stay_id,\n",
    "            'window_id': window_id,\n",
    "            'obs_start_hour': obs_start,\n",
    "            'obs_end_hour': obs_end,\n",
    "            'pred_start_hour': pred_start,\n",
    "            'pred_end_hour': pred_end,\n",
    "            'obs_completeness': obs_completeness,\n",
    "            'death_in_pred_window': death_in_pred_window,\n",
    "            'sequence': sequence_data,\n",
    "            'feature_names': feature_cols\n",
    "        }\n",
    "        \n",
    "        windows.append(window_info)\n",
    "\n",
    "print(f\"\\n4. 슬라이딩 윈도우 생성 완료!\")\n",
    "\n",
    "# 5. 결과를 DataFrame으로 변환\n",
    "print(\"5. 결과 정리...\")\n",
    "windows_df = pd.DataFrame(windows)\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"   - 생성된 총 윈도우 수: {len(windows_df)}\")\n",
    "print(f\"   - 윈도우가 있는 환자 수: {windows_df['subject_id'].nunique()}\")\n",
    "print(f\"   - 환자별 평균 윈도우 수: {len(windows_df) / windows_df['subject_id'].nunique():.1f}\")\n",
    "print(f\"   - 사망 라벨 1인 윈도우: {windows_df['death_in_pred_window'].sum()} ({windows_df['death_in_pred_window'].mean()*100:.1f}%)\")\n",
    "print(f\"   - 평균 관찰 윈도우 완성도: {windows_df['obs_completeness'].mean():.3f}\")\n",
    "\n",
    "# 6. 결과 저장\n",
    "print(\"6. 결과 저장...\")\n",
    "windows_df.to_csv('sliding_windows_18h_6h.csv', index=False)\n",
    "print(\"슬라이딩 윈도우 생성 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98838fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 데이터 로딩...\n",
      "   - 전체 bin 수: 4414240\n",
      "   - 환자 수: 47339\n",
      "2. 데이터 그룹화 및 정렬...\n",
      "3. 슬라이딩 윈도우 생성...\n",
      "   진행률: 1000/47339 (2.1%)\n",
      "   진행률: 2000/47339 (4.2%)\n",
      "   진행률: 3000/47339 (6.3%)\n",
      "   진행률: 4000/47339 (8.4%)\n",
      "   진행률: 5000/47339 (10.6%)\n",
      "   진행률: 6000/47339 (12.7%)\n",
      "   진행률: 7000/47339 (14.8%)\n",
      "   진행률: 8000/47339 (16.9%)\n",
      "   진행률: 9000/47339 (19.0%)\n",
      "   진행률: 10000/47339 (21.1%)\n",
      "   진행률: 11000/47339 (23.2%)\n",
      "   진행률: 12000/47339 (25.3%)\n",
      "   진행률: 13000/47339 (27.5%)\n",
      "   진행률: 14000/47339 (29.6%)\n",
      "   진행률: 15000/47339 (31.7%)\n",
      "   진행률: 16000/47339 (33.8%)\n",
      "   진행률: 17000/47339 (35.9%)\n",
      "   진행률: 18000/47339 (38.0%)\n",
      "   진행률: 19000/47339 (40.1%)\n",
      "   진행률: 20000/47339 (42.2%)\n",
      "   진행률: 21000/47339 (44.4%)\n",
      "   진행률: 22000/47339 (46.5%)\n",
      "   진행률: 23000/47339 (48.6%)\n",
      "   진행률: 24000/47339 (50.7%)\n",
      "   진행률: 25000/47339 (52.8%)\n",
      "   진행률: 26000/47339 (54.9%)\n",
      "   진행률: 27000/47339 (57.0%)\n",
      "   진행률: 28000/47339 (59.1%)\n",
      "   진행률: 29000/47339 (61.3%)\n",
      "   진행률: 30000/47339 (63.4%)\n",
      "   진행률: 31000/47339 (65.5%)\n",
      "   진행률: 32000/47339 (67.6%)\n",
      "   진행률: 33000/47339 (69.7%)\n",
      "   진행률: 34000/47339 (71.8%)\n",
      "   진행률: 35000/47339 (73.9%)\n",
      "   진행률: 36000/47339 (76.0%)\n",
      "   진행률: 37000/47339 (78.2%)\n",
      "   진행률: 38000/47339 (80.3%)\n",
      "   진행률: 39000/47339 (82.4%)\n",
      "   진행률: 40000/47339 (84.5%)\n",
      "   진행률: 41000/47339 (86.6%)\n",
      "   진행률: 42000/47339 (88.7%)\n",
      "   진행률: 43000/47339 (90.8%)\n",
      "   진행률: 44000/47339 (92.9%)\n",
      "   진행률: 45000/47339 (95.1%)\n",
      "   진행률: 46000/47339 (97.2%)\n",
      "   진행률: 47000/47339 (99.3%)\n",
      "\n",
      "4. 슬라이딩 윈도우 생성 완료!\n",
      "5. 결과 정리...\n",
      "   - 생성된 총 윈도우 수: 433024\n",
      "   - 윈도우가 있는 환자 수: 47173\n",
      "   - 환자별 평균 윈도우 수: 9.2\n",
      "   - 사망 라벨 1인 윈도우: 427 (0.1%)\n",
      "   - 평균 관찰 윈도우 완성도: 0.747\n",
      "   - 완성도 분포:\n",
      "     * 0.9 이상: 6512 (1.5%)\n",
      "     * 0.7-0.9: 345320\n",
      "     * 0.5-0.7: 55995\n",
      "     * 0.3-0.5: 25197\n",
      "6. 결과 저장...\n",
      "슬라이딩 윈도우 생성 완료!\n",
      "   - 시퀀스 데이터 shape: (433024, 18, 33)\n",
      "\n",
      "7. 샘플 윈도우 정보:\n",
      "   - Subject ID: 10000690.0\n",
      "   - 완성도: 0.753\n",
      "   - 총 값 수: 594.0\n",
      "   - 결측 값 수: 147.0\n",
      "   - 첫 번째 윈도우 시퀀스 shape: (18, 33)\n",
      "   - 피처 수: 33\n",
      "\n",
      "8. 데이터 검증:\n",
      "   - 윈도우 개수 일치: 433024 == 433024\n",
      "   - 시퀀스 길이 확인: 18 == 18\n",
      "   - 피처 수 확인: 33 == 33\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. 데이터 로드\n",
    "print(\"1. 데이터 로딩...\")\n",
    "data = pd.read_csv(\"hourly_bins/all_features_hourly_bins_inner.csv\")\n",
    "cohort = pd.read_csv(\"cohort.csv\")\n",
    "\n",
    "# datetime 변환\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "\n",
    "print(f\"   - 전체 bin 수: {len(data)}\")\n",
    "print(f\"   - 환자 수: {data['subject_id'].nunique()}\")\n",
    "\n",
    "# 2. 윈도우 설정\n",
    "obs_window_hours = 18  # 관찰 윈도우\n",
    "pred_window_hours = 6   # 예측 윈도우\n",
    "pred_interval_hours = 8 # 예측 간격\n",
    "\n",
    "# 3. 코호트 정보와 병합\n",
    "data_with_cohort = data.merge(cohort[['subject_id', 'stay_id', 'intime', 'outtime', 'deathtime']], \n",
    "                              on=['subject_id', 'stay_id'], how='inner')\n",
    "\n",
    "# 4. subject_id 기준으로 그룹화 및 정렬\n",
    "print(\"2. 데이터 그룹화 및 정렬...\")\n",
    "data_grouped = data_with_cohort.groupby('subject_id')\n",
    "\n",
    "# 피처 컬럼 정의\n",
    "feature_cols = [col for col in data.columns \n",
    "               if col not in ['subject_id', 'stay_id', 'hour_from_intime', 'subject_id', 'hadm_id', 'bin_start', 'bin_end']]\n",
    "\n",
    "# 5. 벡터화된 슬라이딩 윈도우 생성\n",
    "print(\"3. 슬라이딩 윈도우 생성...\")\n",
    "\n",
    "windows = []\n",
    "sequences = []  # ✅ sequences 리스트 초기화\n",
    "total_subjects = len(data_grouped)\n",
    "processed_subjects = 0\n",
    "\n",
    "for subject_id, subject_data in data_grouped:\n",
    "    processed_subjects += 1\n",
    "    if processed_subjects % 1000 == 0:\n",
    "        print(f\"   진행률: {processed_subjects}/{total_subjects} ({processed_subjects/total_subjects*100:.1f}%)\")\n",
    "    \n",
    "    # 환자별 데이터 정렬\n",
    "    subject_data = subject_data.sort_values('hour_from_intime').reset_index(drop=True)\n",
    "    \n",
    "    # 환자별 코호트 정보\n",
    "    stay_id = subject_data['stay_id'].iloc[0]\n",
    "    intime = subject_data['intime'].iloc[0]\n",
    "    outtime = subject_data['outtime'].iloc[0]  \n",
    "    deathtime = subject_data['deathtime'].iloc[0]\n",
    "    \n",
    "    # 최대 시간\n",
    "    max_hour = subject_data['hour_from_intime'].max()\n",
    "    \n",
    "    # 가능한 모든 윈도우 시작점 계산 (벡터화)\n",
    "    possible_starts = np.arange(0, max_hour + 1 - obs_window_hours - pred_window_hours + 1, pred_interval_hours)\n",
    "    \n",
    "    # 각 시작점에 대해 윈도우 생성\n",
    "    for window_id, start_hour in enumerate(possible_starts):\n",
    "        obs_start = start_hour\n",
    "        obs_end = start_hour + obs_window_hours\n",
    "        pred_start = obs_end\n",
    "        pred_end = obs_end + pred_window_hours\n",
    "        \n",
    "        # 관찰 윈도우 데이터 추출 (벡터화)\n",
    "        obs_mask = (subject_data['hour_from_intime'] >= obs_start) & (subject_data['hour_from_intime'] < obs_end)\n",
    "        obs_window = subject_data[obs_mask]\n",
    "        \n",
    "        # ===== 수정된 결측률 계산 부분 =====\n",
    "        # 시퀀스 데이터 생성을 위한 준비\n",
    "        hour_range = np.arange(obs_start, obs_end)\n",
    "        obs_window_indexed = obs_window.set_index('hour_from_intime')\n",
    "        \n",
    "        # 18시간 시퀀스 데이터 생성 및 결측률 계산\n",
    "        sequence_data = []\n",
    "        total_values = 0\n",
    "        missing_values = 0\n",
    "        \n",
    "        for hour in hour_range:\n",
    "            if hour in obs_window_indexed.index:\n",
    "                # 해당 시간의 데이터가 있음\n",
    "                hour_features = obs_window_indexed.loc[hour, feature_cols].values\n",
    "                # NaN 개수 계산\n",
    "                missing_in_hour = pd.isna(hour_features).sum()\n",
    "                missing_values += missing_in_hour\n",
    "                total_values += len(hour_features)\n",
    "            else:\n",
    "                # 해당 시간의 데이터가 없음 - 전체가 결측\n",
    "                hour_features = np.full(len(feature_cols), np.nan)\n",
    "                missing_values += len(feature_cols)\n",
    "                total_values += len(feature_cols)\n",
    "            \n",
    "            sequence_data.append(hour_features.tolist())\n",
    "        \n",
    "        # 실제 결측률 계산\n",
    "        obs_completeness = 1 - (missing_values / total_values) if total_values > 0 else 0\n",
    "        \n",
    "        # 결측률 기준으로 필터링 (70% 이상 결측이면 제외)\n",
    "        if obs_completeness < 0.3:\n",
    "            continue\n",
    "        \n",
    "        # 사망 라벨 결정 (벡터화)\n",
    "        pred_window_start_time = intime + pd.Timedelta(hours=pred_start)\n",
    "        pred_window_end_time = intime + pd.Timedelta(hours=pred_end)\n",
    "        \n",
    "        death_in_pred_window = 0\n",
    "        if pd.notna(deathtime):\n",
    "            if pred_window_start_time <= deathtime <= pred_window_end_time:\n",
    "                death_in_pred_window = 1\n",
    "        \n",
    "        # ✅ 윈도우 메타데이터 저장 (sequence 제거)\n",
    "        window_info = {\n",
    "            'subject_id': subject_id,\n",
    "            'stay_id': stay_id,\n",
    "            'window_id': window_id,\n",
    "            'obs_start_hour': obs_start,\n",
    "            'obs_end_hour': obs_end,\n",
    "            'pred_start_hour': pred_start,\n",
    "            'pred_end_hour': pred_end,\n",
    "            'obs_completeness': obs_completeness,\n",
    "            'total_values': total_values,\n",
    "            'missing_values': missing_values,\n",
    "            'death_in_pred_window': death_in_pred_window\n",
    "            # ✅ 'sequence' 제거 - 메타데이터만 저장\n",
    "        }\n",
    "        \n",
    "        windows.append(window_info)\n",
    "        \n",
    "        # ✅ 시퀀스 데이터를 별도 리스트에 저장 (윈도우별로)\n",
    "        sequences.append(np.array(sequence_data))  # shape: (18, n_features)\n",
    "\n",
    "print(f\"\\n4. 슬라이딩 윈도우 생성 완료!\")\n",
    "\n",
    "# 5. 결과를 DataFrame으로 변환\n",
    "print(\"5. 결과 정리...\")\n",
    "windows_df = pd.DataFrame(windows)\n",
    "\n",
    "# 통계 출력\n",
    "print(f\"   - 생성된 총 윈도우 수: {len(windows_df)}\")\n",
    "print(f\"   - 윈도우가 있는 환자 수: {windows_df['subject_id'].nunique()}\")\n",
    "print(f\"   - 환자별 평균 윈도우 수: {len(windows_df) / windows_df['subject_id'].nunique():.1f}\")\n",
    "print(f\"   - 사망 라벨 1인 윈도우: {windows_df['death_in_pred_window'].sum()} ({windows_df['death_in_pred_window'].mean()*100:.1f}%)\")\n",
    "print(f\"   - 평균 관찰 윈도우 완성도: {windows_df['obs_completeness'].mean():.3f}\")\n",
    "print(f\"   - 완성도 분포:\")\n",
    "print(f\"     * 0.9 이상: {(windows_df['obs_completeness'] >= 0.9).sum()} ({(windows_df['obs_completeness'] >= 0.9).mean()*100:.1f}%)\")\n",
    "print(f\"     * 0.7-0.9: {((windows_df['obs_completeness'] >= 0.7) & (windows_df['obs_completeness'] < 0.9)).sum()}\")\n",
    "print(f\"     * 0.5-0.7: {((windows_df['obs_completeness'] >= 0.5) & (windows_df['obs_completeness'] < 0.7)).sum()}\")\n",
    "print(f\"     * 0.3-0.5: {((windows_df['obs_completeness'] >= 0.3) & (windows_df['obs_completeness'] < 0.5)).sum()}\")\n",
    "\n",
    "# 6. 결과 저장\n",
    "print(\"6. 결과 저장...\")\n",
    "\n",
    "# ✅ 메타데이터만 CSV로 저장 (sequence 컬럼 제거됨)\n",
    "windows_df.to_csv('sliding_windows_metadata.csv', index=False)\n",
    "\n",
    "# ✅ 시퀀스 데이터를 3D numpy array로 저장\n",
    "sequences_array = np.array(sequences)  # shape: (n_windows, 18, n_features)\n",
    "np.save('sliding_windows_sequences.npy', sequences_array)\n",
    "\n",
    "# ✅ 피처 이름 저장\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_cols))\n",
    "\n",
    "print(\"슬라이딩 윈도우 생성 완료!\")\n",
    "print(f\"   - 시퀀스 데이터 shape: {sequences_array.shape}\")\n",
    "\n",
    "# 7. 추가 분석을 위한 샘플 출력\n",
    "if len(windows_df) > 0:\n",
    "    print(\"\\n7. 샘플 윈도우 정보:\")\n",
    "    sample_window = windows_df.iloc[0]\n",
    "    print(f\"   - Subject ID: {sample_window['subject_id']}\")\n",
    "    print(f\"   - 완성도: {sample_window['obs_completeness']:.3f}\")\n",
    "    print(f\"   - 총 값 수: {sample_window['total_values']}\")\n",
    "    print(f\"   - 결측 값 수: {sample_window['missing_values']}\")\n",
    "    print(f\"   - 첫 번째 윈도우 시퀀스 shape: {sequences_array[0].shape}\")\n",
    "    print(f\"   - 피처 수: {len(feature_cols)}\")\n",
    "\n",
    "# 8. 데이터 검증\n",
    "print(\"\\n8. 데이터 검증:\")\n",
    "print(f\"   - 윈도우 개수 일치: {len(windows_df)} == {len(sequences_array)}\")\n",
    "print(f\"   - 시퀀스 길이 확인: {sequences_array.shape[1]} == {obs_window_hours}\")\n",
    "print(f\"   - 피처 수 확인: {sequences_array.shape[2]} == {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eacfa4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed_due_to_completeness: 3073\n"
     ]
    }
   ],
   "source": [
    "death_windows = windows_df[windows_df['death_in_pred_window']==1]\n",
    "removed_due_to_completeness = 3500 - death_windows['subject_id'].nunique()\n",
    "print(f\"removed_due_to_completeness: {removed_due_to_completeness}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ce774e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 데이터 로딩...\n",
      "\n",
      "===== 라벨 변경 리포트 =====\n",
      "기존 라벨 1 윈도우 수: 433\n",
      "새 로직 라벨 1 윈도우 수: 1276\n",
      "증가 수: 843 (194.7%)\n",
      "전체 윈도우 수: 352165\n",
      "라벨 1 비율(기존): 0.12%\n",
      "라벨 1 비율(새로): 0.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== 파라미터 =====\n",
    "obs_window_hours = 18\n",
    "pred_window_hours = 6\n",
    "pred_interval_hours = 8\n",
    "extra_hours_after_pred = 2  # 예측 구간 종료 직후 추가 허용 시간\n",
    "\n",
    "# ===== 데이터 로드 =====\n",
    "print(\"1. 데이터 로딩...\")\n",
    "data = pd.read_csv(\"hourly_bins/all_features_hourly_bins_inner.csv\")\n",
    "cohort = pd.read_csv(\"cohort.csv\")\n",
    "\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "\n",
    "# 병합\n",
    "data_with_cohort = data.merge(\n",
    "    cohort[['subject_id','stay_id','intime','outtime','deathtime']],\n",
    "    on=['subject_id','stay_id'], how='inner'\n",
    ")\n",
    "\n",
    "# 피처 컬럼 (메타데이터 제외)\n",
    "exclude_cols = ['subject_id', 'stay_id', 'hour_from_intime', 'hadm_id', 'bin_start', 'bin_end']\n",
    "feature_cols = [c for c in data.columns if c not in exclude_cols]\n",
    "\n",
    "# 그룹화\n",
    "data_grouped = data_with_cohort.groupby('subject_id')\n",
    "\n",
    "# ===== 윈도우 루프 =====\n",
    "windows = []\n",
    "sequences = []\n",
    "old_label_count = 0\n",
    "new_label_count = 0\n",
    "\n",
    "for subject_id, subject_data in data_grouped:\n",
    "    subject_data = subject_data.sort_values('hour_from_intime').reset_index(drop=True)\n",
    "    \n",
    "    stay_id = subject_data['stay_id'].iloc[0]\n",
    "    intime = subject_data['intime'].iloc[0]\n",
    "    deathtime = subject_data['deathtime'].iloc[0]\n",
    "    max_hour = subject_data['hour_from_intime'].max()\n",
    "\n",
    "    possible_starts = np.arange(0, max_hour + 1 - obs_window_hours - pred_window_hours + 1,\n",
    "                                pred_interval_hours)\n",
    "\n",
    "    for window_id, obs_start in enumerate(possible_starts):\n",
    "        obs_end = obs_start + obs_window_hours\n",
    "        pred_start = obs_end\n",
    "        pred_end = obs_end + pred_window_hours\n",
    "        \n",
    "        # 시간 계산\n",
    "        obs_start_time  = intime + pd.Timedelta(hours=obs_start)\n",
    "        obs_end_time    = intime + pd.Timedelta(hours=obs_end)\n",
    "        pred_start_time = intime + pd.Timedelta(hours=pred_start)\n",
    "        pred_end_time   = intime + pd.Timedelta(hours=pred_end)\n",
    "        \n",
    "        # ========== 기존 라벨 (old_label) ==========\n",
    "        old_label = 0\n",
    "        if pd.notna(deathtime):\n",
    "            if pred_start_time <= deathtime <= pred_end_time:\n",
    "                old_label = 1\n",
    "        \n",
    "        # ========== 새로운 라벨링 로직 ==========\n",
    "        remove_window = False\n",
    "        new_label = 0\n",
    "        if pd.notna(deathtime):\n",
    "            # 관찰 중 사망 → 제거\n",
    "            if obs_start_time <= deathtime <= obs_end_time:\n",
    "                remove_window = True\n",
    "            # 예측 구간 내 사망\n",
    "            elif pred_start_time <= deathtime <= pred_end_time:\n",
    "                new_label = 1\n",
    "            # 예측 구간 종료 직후 2시간 내 사망\n",
    "            elif pred_end_time < deathtime <= pred_end_time + pd.Timedelta(hours=extra_hours_after_pred):\n",
    "                new_label = 1\n",
    "        \n",
    "        if remove_window:\n",
    "            continue\n",
    "        \n",
    "        # ===== 관찰 데이터 & completeness 계산 =====\n",
    "        obs_mask = (subject_data['hour_from_intime'] >= obs_start) & \\\n",
    "                   (subject_data['hour_from_intime'] < obs_end)\n",
    "        obs_window = subject_data[obs_mask]\n",
    "        \n",
    "        hour_range = np.arange(obs_start, obs_end)\n",
    "        obs_window_indexed = obs_window.set_index('hour_from_intime')\n",
    "        \n",
    "        sequence_data = []\n",
    "        total_values = 0\n",
    "        missing_values = 0\n",
    "        \n",
    "        for hour in hour_range:\n",
    "            if hour in obs_window_indexed.index:\n",
    "                hour_features = obs_window_indexed.loc[hour, feature_cols].values\n",
    "                missing_in_hour = pd.isna(hour_features).sum()\n",
    "            else:\n",
    "                hour_features = np.full(len(feature_cols), np.nan)\n",
    "                missing_in_hour = len(feature_cols)\n",
    "            sequence_data.append(hour_features.tolist())\n",
    "            missing_values += missing_in_hour\n",
    "            total_values += len(feature_cols)\n",
    "        \n",
    "        obs_completeness = 1 - (missing_values / total_values) if total_values > 0 else 0\n",
    "        \n",
    "        # 라벨 0인 경우에만 completeness 기준 적용\n",
    "        if (new_label == 0) and (obs_completeness < 0.7):\n",
    "            continue\n",
    "        \n",
    "        # 저장\n",
    "        windows.append({\n",
    "            'subject_id': subject_id,\n",
    "            'stay_id': stay_id,\n",
    "            'window_id': window_id,\n",
    "            'obs_start_hour': obs_start,\n",
    "            'obs_end_hour': obs_end,\n",
    "            'pred_start_hour': pred_start,\n",
    "            'pred_end_hour': pred_end,\n",
    "            'obs_completeness': obs_completeness,\n",
    "            'death_in_pred_window_old': old_label,\n",
    "            'death_in_pred_window_new': new_label\n",
    "        })\n",
    "        sequences.append(np.array(sequence_data))\n",
    "        \n",
    "        old_label_count += old_label\n",
    "        new_label_count += new_label\n",
    "\n",
    "# ===== 결과 저장 및 리포트 =====\n",
    "windows_df = pd.DataFrame(windows)\n",
    "sequences_array = np.array(sequences)\n",
    "\n",
    "windows_df.to_csv('sliding_windows_metadata.csv', index=False)\n",
    "np.save('sliding_windows_sequences.npy', sequences_array)\n",
    "\n",
    "print(\"\\n===== 라벨 변경 리포트 =====\")\n",
    "print(f\"기존 라벨 1 윈도우 수: {old_label_count}\")\n",
    "print(f\"새 로직 라벨 1 윈도우 수: {new_label_count}\")\n",
    "print(f\"증가 수: {new_label_count - old_label_count} ({(new_label_count - old_label_count) / old_label_count * 100 if old_label_count else 0:.1f}%)\")\n",
    "print(f\"전체 윈도우 수: {len(windows_df)}\")\n",
    "print(f\"라벨 1 비율(기존): {old_label_count / len(windows_df) * 100:.2f}%\")\n",
    "print(f\"라벨 1 비율(새로): {new_label_count / len(windows_df) * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5af0d46",
   "metadata": {},
   "source": [
    "# 결측값 확인 및 컬럼 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2d6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 생성된 파일 분석 =====\n",
      "메타데이터 파일 크기: (352165, 10)\n",
      "컬럼: ['subject_id', 'stay_id', 'window_id', 'obs_start_hour', 'obs_end_hour', 'pred_start_hour', 'pred_end_hour', 'obs_completeness', 'death_in_pred_window_old', 'death_in_pred_window_new']\n",
      "\n",
      "메타데이터 샘플:\n",
      "   subject_id   stay_id  window_id  obs_start_hour  obs_end_hour  \\\n",
      "0    10000690  37081114          0               0            18   \n",
      "1    10000690  37081114          1               8            26   \n",
      "2    10000690  37081114          2              16            34   \n",
      "3    10000690  37081114          3              24            42   \n",
      "4    10000690  37081114          4              32            50   \n",
      "\n",
      "   pred_start_hour  pred_end_hour  obs_completeness  death_in_pred_window_old  \\\n",
      "0               18             24          0.752525                         0   \n",
      "1               26             32          0.750842                         0   \n",
      "2               34             40          0.791246                         0   \n",
      "3               42             48          0.803030                         0   \n",
      "4               50             56          0.796296                         0   \n",
      "\n",
      "   death_in_pred_window_new  \n",
      "0                         0  \n",
      "1                         0  \n",
      "2                         0  \n",
      "3                         0  \n",
      "4                         0  \n",
      "\n",
      "시퀀스 데이터 형태: (352165, 18, 33)\n",
      "시퀀스 데이터 타입: float64\n",
      "각 윈도우 크기: 18시간 × 33피처\n",
      "\n",
      "===== 메타데이터 통계 =====\n",
      "총 윈도우 수: 352165\n",
      "환자 수: 42979\n",
      "평균 관찰 완전성: 0.790\n",
      "기존 라벨 1 비율: 0.001\n",
      "새 라벨 1 비율: 0.004\n",
      "\n",
      "관찰 완전성 분포:\n",
      "count    352165.000000\n",
      "mean          0.789517\n",
      "std           0.041907\n",
      "min           0.000000\n",
      "25%           0.765993\n",
      "50%           0.787879\n",
      "75%           0.804714\n",
      "max           1.000000\n",
      "Name: obs_completeness, dtype: float64\n",
      "\n",
      "===== 결측값 처리 시작 =====\n",
      "원본 데이터 크기: (4414240, 39)\n",
      "처리할 피처 수: 33\n",
      "처리 전 결측값: 39,866,717 / 145,669,920 (27.37%)\n",
      "\n",
      "환자별 결측값 보간 중...\n",
      "처리 후 결측값: 0 / 145,669,920 (0.00%)\n",
      "제거된 결측값: 39,866,717\n",
      "\n",
      "===== 컬럼 정리 =====\n",
      "제거할 컬럼들: ['temp_min', 'temp_max', 'gcs_min', 'gcs_max']\n",
      "제거된 컬럼 수: 4\n",
      "최종 데이터 크기: (4414240, 35)\n",
      "\n",
      "메타데이터에서 old_label 관련 컬럼 제거...\n",
      "메타데이터에서 제거된 컬럼: ['death_in_pred_window_old']\n",
      "\n",
      "===== 새로운 시퀀스 데이터 생성 =====\n",
      "새로운 피처 수: 29\n",
      "새로운 시퀀스 데이터 형태: (352165, 18, 29)\n",
      "\n",
      "===== 최종 파일 저장 =====\n",
      "처리된 시간별 데이터 저장: processed_hourly_data.csv\n",
      "정리된 메타데이터 저장: cleaned_sliding_windows_metadata.csv\n",
      "처리된 시퀀스 데이터 저장: processed_sliding_windows_sequences.npy\n",
      "피처 이름 저장: feature_names.txt\n",
      "\n",
      "===== 최종 처리 요약 =====\n",
      "원본 데이터: 4,414,240행 × 39열\n",
      "처리된 데이터: 4,414,240행 × 35열\n",
      "제거된 컬럼 수: 4\n",
      "남은 피처 수: 29\n",
      "결측값 감소: 39,866,717 → 0\n",
      "윈도우 수: 352165\n",
      "최종 시퀀스 형태: 352165윈도우 × 18시간 × 29피처\n",
      "\n",
      "===== 최종 결측값 현황 =====\n",
      "최종 시퀀스 결측값: 0 / 183,830,130 (0.00%)\n",
      "\n",
      "모든 처리가 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"===== 생성된 파일 분석 =====\")\n",
    "\n",
    "# ===== 1. 생성된 파일들 로드 및 분석 =====\n",
    "# 메타데이터 파일 로드\n",
    "metadata = pd.read_csv('sliding_windows_metadata.csv')\n",
    "print(f\"메타데이터 파일 크기: {metadata.shape}\")\n",
    "print(f\"컬럼: {list(metadata.columns)}\")\n",
    "print(\"\\n메타데이터 샘플:\")\n",
    "print(metadata.head())\n",
    "\n",
    "# 시퀀스 데이터 로드\n",
    "sequences = np.load('sliding_windows_sequences.npy')\n",
    "print(f\"\\n시퀀스 데이터 형태: {sequences.shape}\")\n",
    "print(f\"시퀀스 데이터 타입: {sequences.dtype}\")\n",
    "print(f\"각 윈도우 크기: {sequences.shape[1]}시간 × {sequences.shape[2]}피처\")\n",
    "\n",
    "# 메타데이터 통계\n",
    "print(f\"\\n===== 메타데이터 통계 =====\")\n",
    "print(f\"총 윈도우 수: {len(metadata)}\")\n",
    "print(f\"환자 수: {metadata['subject_id'].nunique()}\")\n",
    "print(f\"평균 관찰 완전성: {metadata['obs_completeness'].mean():.3f}\")\n",
    "print(f\"기존 라벨 1 비율: {metadata['death_in_pred_window_old'].mean():.3f}\")\n",
    "print(f\"새 라벨 1 비율: {metadata['death_in_pred_window_new'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n관찰 완전성 분포:\")\n",
    "print(metadata['obs_completeness'].describe())\n",
    "\n",
    "# ===== 2. 원본 데이터 로드 (결측값 처리를 위해) =====\n",
    "print(\"\\n===== 결측값 처리 시작 =====\")\n",
    "original_data = pd.read_csv(\"hourly_bins/all_features_hourly_bins_inner.csv\")\n",
    "print(f\"원본 데이터 크기: {original_data.shape}\")\n",
    "\n",
    "# 메타데이터 컬럼 제외하고 피처 컬럼만 추출\n",
    "exclude_cols = ['subject_id', 'stay_id', 'hour_from_intime', 'hadm_id', 'bin_start', 'bin_end']\n",
    "feature_cols = [c for c in original_data.columns if c not in exclude_cols]\n",
    "print(f\"처리할 피처 수: {len(feature_cols)}\")\n",
    "\n",
    "# 결측값 현황 확인\n",
    "missing_before = original_data[feature_cols].isnull().sum().sum()\n",
    "total_values = len(original_data) * len(feature_cols)\n",
    "print(f\"처리 전 결측값: {missing_before:,} / {total_values:,} ({missing_before/total_values*100:.2f}%)\")\n",
    "\n",
    "# ===== 3. 환자별 결측값 보간 =====\n",
    "print(\"\\n환자별 결측값 보간 중...\")\n",
    "processed_data = original_data.copy()\n",
    "\n",
    "# 환자별로 그룹화하여 처리\n",
    "for subject_id in processed_data['subject_id'].unique():\n",
    "    patient_mask = processed_data['subject_id'] == subject_id\n",
    "    patient_data = processed_data[patient_mask].copy()\n",
    "    \n",
    "    # 시간 순서로 정렬\n",
    "    patient_data = patient_data.sort_values('hour_from_intime')\n",
    "    \n",
    "    # 각 피처별로 ffill 적용\n",
    "    for col in feature_cols:\n",
    "        # ffill (forward fill) 적용\n",
    "        patient_data[col] = patient_data[col].ffill()\n",
    "        \n",
    "        # ffill로도 채워지지 않은 값들 (첫 번째 값이 NaN인 경우)은 해당 환자의 평균으로 채움\n",
    "        if patient_data[col].isnull().any():\n",
    "            patient_mean = patient_data[col].mean()\n",
    "            if not np.isnan(patient_mean):  # 해당 환자에게 해당 피처의 값이 하나라도 있는 경우\n",
    "                patient_data[col] = patient_data[col].fillna(patient_mean)\n",
    "            else:  # 해당 환자에게 해당 피처의 값이 전혀 없는 경우, 전체 평균 사용\n",
    "                global_mean = processed_data[col].mean()\n",
    "                patient_data[col] = patient_data[col].fillna(global_mean)\n",
    "    \n",
    "    # 원본 데이터에 반영\n",
    "    processed_data.loc[patient_mask, feature_cols] = patient_data[feature_cols].values\n",
    "\n",
    "# 결측값 처리 결과 확인\n",
    "missing_after = processed_data[feature_cols].isnull().sum().sum()\n",
    "print(f\"처리 후 결측값: {missing_after:,} / {total_values:,} ({missing_after/total_values*100:.2f}%)\")\n",
    "print(f\"제거된 결측값: {missing_before - missing_after:,}\")\n",
    "\n",
    "# ===== 4. 불필요한 컬럼 제거 =====\n",
    "print(\"\\n===== 컬럼 정리 =====\")\n",
    "\n",
    "# 제거할 컬럼들 정의\n",
    "cols_to_remove = ['temp_min', 'temp_max', 'gcs_min', 'gcs_max']\n",
    "\n",
    "# old_label 관련 컬럼들도 찾아서 제거 (컬럼명에 'old' 또는 'old_label'이 포함된 것들)\n",
    "old_label_cols = [col for col in processed_data.columns if 'old' in col.lower()]\n",
    "cols_to_remove.extend(old_label_cols)\n",
    "\n",
    "# 실제 존재하는 컬럼만 필터링\n",
    "existing_cols_to_remove = [col for col in cols_to_remove if col in processed_data.columns]\n",
    "print(f\"제거할 컬럼들: {existing_cols_to_remove}\")\n",
    "\n",
    "# 컬럼 제거\n",
    "if existing_cols_to_remove:\n",
    "    processed_data = processed_data.drop(columns=existing_cols_to_remove)\n",
    "    print(f\"제거된 컬럼 수: {len(existing_cols_to_remove)}\")\n",
    "else:\n",
    "    print(\"제거할 컬럼이 존재하지 않습니다.\")\n",
    "\n",
    "print(f\"최종 데이터 크기: {processed_data.shape}\")\n",
    "\n",
    "# ===== 5. 메타데이터에서도 old_label 관련 컬럼 제거 =====\n",
    "print(\"\\n메타데이터에서 old_label 관련 컬럼 제거...\")\n",
    "metadata_old_cols = [col for col in metadata.columns if 'old' in col.lower()]\n",
    "if metadata_old_cols:\n",
    "    metadata_cleaned = metadata.drop(columns=metadata_old_cols)\n",
    "    print(f\"메타데이터에서 제거된 컬럼: {metadata_old_cols}\")\n",
    "else:\n",
    "    metadata_cleaned = metadata.copy()\n",
    "    print(\"메타데이터에서 제거할 old_label 관련 컬럼이 없습니다.\")\n",
    "\n",
    "# ===== 6. 처리된 데이터로 새로운 시퀀스 생성 =====\n",
    "print(\"\\n===== 새로운 시퀀스 데이터 생성 =====\")\n",
    "\n",
    "# 새로운 피처 컬럼 목록 (제거된 컬럼 반영)\n",
    "new_exclude_cols = ['subject_id', 'stay_id', 'hour_from_intime', 'hadm_id', 'bin_start', 'bin_end']\n",
    "new_feature_cols = [c for c in processed_data.columns if c not in new_exclude_cols]\n",
    "print(f\"새로운 피처 수: {len(new_feature_cols)}\")\n",
    "\n",
    "# 코호트 데이터 다시 로드\n",
    "cohort = pd.read_csv(\"cohort.csv\")\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "\n",
    "# 병합\n",
    "processed_data_with_cohort = processed_data.merge(\n",
    "    cohort[['subject_id','stay_id','intime','outtime','deathtime']],\n",
    "    on=['subject_id','stay_id'], how='inner'\n",
    ")\n",
    "\n",
    "# 새로운 시퀀스 생성\n",
    "obs_window_hours = 18\n",
    "new_sequences = []\n",
    "\n",
    "processed_grouped = processed_data_with_cohort.groupby('subject_id')\n",
    "\n",
    "for _, row in metadata_cleaned.iterrows():\n",
    "    subject_id = row['subject_id']\n",
    "    obs_start = row['obs_start_hour']\n",
    "    obs_end = row['obs_end_hour']\n",
    "    \n",
    "    # 해당 환자 데이터 가져오기\n",
    "    subject_data = processed_grouped.get_group(subject_id).sort_values('hour_from_intime').reset_index(drop=True)\n",
    "    \n",
    "    # 관찰 구간 데이터 추출\n",
    "    obs_mask = (subject_data['hour_from_intime'] >= obs_start) & \\\n",
    "               (subject_data['hour_from_intime'] < obs_end)\n",
    "    obs_window = subject_data[obs_mask]\n",
    "    \n",
    "    # 시간별 데이터 생성\n",
    "    hour_range = np.arange(obs_start, obs_end)\n",
    "    obs_window_indexed = obs_window.set_index('hour_from_intime')\n",
    "    \n",
    "    sequence_data = []\n",
    "    for hour in hour_range:\n",
    "        if hour in obs_window_indexed.index:\n",
    "            hour_features = obs_window_indexed.loc[hour, new_feature_cols].values\n",
    "        else:\n",
    "            # 해당 시간의 데이터가 없는 경우 NaN으로 채움 (이미 보간했으므로 거의 발생하지 않음)\n",
    "            hour_features = np.full(len(new_feature_cols), np.nan)\n",
    "        sequence_data.append(hour_features.tolist())\n",
    "    \n",
    "    new_sequences.append(np.array(sequence_data))\n",
    "\n",
    "new_sequences_array = np.array(new_sequences)\n",
    "print(f\"새로운 시퀀스 데이터 형태: {new_sequences_array.shape}\")\n",
    "\n",
    "# ===== 7. 최종 파일 저장 =====\n",
    "print(\"\\n===== 최종 파일 저장 =====\")\n",
    "\n",
    "# 처리된 원본 데이터 저장\n",
    "processed_data.to_csv('processed_hourly_data.csv', index=False)\n",
    "print(\"처리된 시간별 데이터 저장: processed_hourly_data.csv\")\n",
    "\n",
    "# 정리된 메타데이터 저장\n",
    "metadata_cleaned.to_csv('cleaned_sliding_windows_metadata.csv', index=False)\n",
    "print(\"정리된 메타데이터 저장: cleaned_sliding_windows_metadata.csv\")\n",
    "\n",
    "# 새로운 시퀀스 데이터 저장\n",
    "np.save('processed_sliding_windows_sequences.npy', new_sequences_array)\n",
    "print(\"처리된 시퀀스 데이터 저장: processed_sliding_windows_sequences.npy\")\n",
    "\n",
    "# 피처 이름 저장\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    for feature in new_feature_cols:\n",
    "        f.write(feature + '\\n')\n",
    "print(\"피처 이름 저장: feature_names.txt\")\n",
    "\n",
    "# ===== 8. 최종 요약 =====\n",
    "print(f\"\\n===== 최종 처리 요약 =====\")\n",
    "print(f\"원본 데이터: {original_data.shape[0]:,}행 × {original_data.shape[1]}열\")\n",
    "print(f\"처리된 데이터: {processed_data.shape[0]:,}행 × {processed_data.shape[1]}열\")\n",
    "print(f\"제거된 컬럼 수: {len(existing_cols_to_remove)}\")\n",
    "print(f\"남은 피처 수: {len(new_feature_cols)}\")\n",
    "print(f\"결측값 감소: {missing_before:,} → {missing_after:,}\")\n",
    "print(f\"윈도우 수: {len(metadata_cleaned)}\")\n",
    "print(f\"최종 시퀀스 형태: {new_sequences_array.shape[0]}윈도우 × {new_sequences_array.shape[1]}시간 × {new_sequences_array.shape[2]}피처\")\n",
    "\n",
    "# 각 파일별 결측값 현황\n",
    "print(f\"\\n===== 최종 결측값 현황 =====\")\n",
    "final_missing = new_sequences_array.size - np.count_nonzero(~np.isnan(new_sequences_array))\n",
    "final_total = new_sequences_array.size\n",
    "print(f\"최종 시퀀스 결측값: {final_missing:,} / {final_total:,} ({final_missing/final_total*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n모든 처리가 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757aa7b",
   "metadata": {},
   "source": [
    "### 결측값 보간 없는 수정 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15eea1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 생성된 파일 분석 =====\n",
      "메타데이터 파일 크기: (352165, 10)\n",
      "컬럼: ['subject_id', 'stay_id', 'window_id', 'obs_start_hour', 'obs_end_hour', 'pred_start_hour', 'pred_end_hour', 'obs_completeness', 'death_in_pred_window_old', 'death_in_pred_window_new']\n",
      "\n",
      "메타데이터 샘플:\n",
      "   subject_id   stay_id  window_id  obs_start_hour  obs_end_hour  \\\n",
      "0    10000690  37081114          0               0            18   \n",
      "1    10000690  37081114          1               8            26   \n",
      "2    10000690  37081114          2              16            34   \n",
      "3    10000690  37081114          3              24            42   \n",
      "4    10000690  37081114          4              32            50   \n",
      "\n",
      "   pred_start_hour  pred_end_hour  obs_completeness  death_in_pred_window_old  \\\n",
      "0               18             24          0.752525                         0   \n",
      "1               26             32          0.750842                         0   \n",
      "2               34             40          0.791246                         0   \n",
      "3               42             48          0.803030                         0   \n",
      "4               50             56          0.796296                         0   \n",
      "\n",
      "   death_in_pred_window_new  \n",
      "0                         0  \n",
      "1                         0  \n",
      "2                         0  \n",
      "3                         0  \n",
      "4                         0  \n",
      "\n",
      "시퀀스 데이터 형태: (352165, 18, 33)\n",
      "시퀀스 데이터 타입: float64\n",
      "각 윈도우 크기: 18시간 × 33피처\n",
      "\n",
      "===== 메타데이터 통계 =====\n",
      "총 윈도우 수: 352165\n",
      "환자 수: 42979\n",
      "평균 관찰 완전성: 0.790\n",
      "기존 라벨 1 비율: 0.001\n",
      "새 라벨 1 비율: 0.004\n",
      "\n",
      "관찰 완전성 분포:\n",
      "count    352165.000000\n",
      "mean          0.789517\n",
      "std           0.041907\n",
      "min           0.000000\n",
      "25%           0.765993\n",
      "50%           0.787879\n",
      "75%           0.804714\n",
      "max           1.000000\n",
      "Name: obs_completeness, dtype: float64\n",
      "\n",
      "===== 원본 데이터 로드 =====\n",
      "원본 데이터 크기: (4414240, 39)\n",
      "처리할 피처 수: 33\n",
      "결측값 현황: 39,866,717 / 145,669,920 (27.37%)\n",
      "\n",
      "===== 컬럼 정리 =====\n",
      "제거할 컬럼들: ['temp_min', 'temp_max', 'gcs_min', 'gcs_max']\n",
      "제거된 컬럼 수: 4\n",
      "최종 데이터 크기: (4414240, 35)\n",
      "\n",
      "메타데이터에서 old_label 관련 컬럼 제거...\n",
      "메타데이터에서 제거된 컬럼: ['death_in_pred_window_old']\n",
      "\n",
      "===== 새로운 시퀀스 데이터 생성 =====\n",
      "새로운 피처 수: 29\n",
      "새로운 시퀀스 데이터 형태: (352165, 18, 29)\n",
      "\n",
      "===== 최종 파일 저장 =====\n",
      "처리된 시간별 데이터 저장: processed_hourly_data.csv\n",
      "정리된 메타데이터 저장: cleaned_sliding_windows_metadata.csv\n",
      "처리된 시퀀스 데이터 저장: processed_sliding_windows_sequences.npy\n",
      "피처 이름 저장: feature_names.txt\n",
      "\n",
      "===== 최종 처리 요약 =====\n",
      "원본 데이터: 4,414,240행 × 39열\n",
      "처리된 데이터: 4,414,240행 × 35열\n",
      "제거된 컬럼 수: 4\n",
      "남은 피처 수: 29\n",
      "결측값 현황: 39,866,717 (보간 없이 유지)\n",
      "윈도우 수: 352165\n",
      "최종 시퀀스 형태: 352165윈도우 × 18시간 × 29피처\n",
      "\n",
      "===== 최종 결측값 현황 =====\n",
      "최종 시퀀스 결측값: 26,174,390 / 183,830,130 (14.24%)\n",
      "\n",
      "모든 처리가 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "#결측값 보간 없는 버전\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"===== 생성된 파일 분석 =====\")\n",
    "\n",
    "# ===== 1. 생성된 파일들 로드 및 분석 =====\n",
    "# 메타데이터 파일 로드\n",
    "metadata = pd.read_csv('sliding_windows_metadata.csv')\n",
    "print(f\"메타데이터 파일 크기: {metadata.shape}\")\n",
    "print(f\"컬럼: {list(metadata.columns)}\")\n",
    "print(\"\\n메타데이터 샘플:\")\n",
    "print(metadata.head())\n",
    "\n",
    "# 시퀀스 데이터 로드\n",
    "sequences = np.load('sliding_windows_sequences.npy')\n",
    "print(f\"\\n시퀀스 데이터 형태: {sequences.shape}\")\n",
    "print(f\"시퀀스 데이터 타입: {sequences.dtype}\")\n",
    "print(f\"각 윈도우 크기: {sequences.shape[1]}시간 × {sequences.shape[2]}피처\")\n",
    "\n",
    "# 메타데이터 통계\n",
    "print(f\"\\n===== 메타데이터 통계 =====\")\n",
    "print(f\"총 윈도우 수: {len(metadata)}\")\n",
    "print(f\"환자 수: {metadata['subject_id'].nunique()}\")\n",
    "print(f\"평균 관찰 완전성: {metadata['obs_completeness'].mean():.3f}\")\n",
    "print(f\"기존 라벨 1 비율: {metadata['death_in_pred_window_old'].mean():.3f}\")\n",
    "print(f\"새 라벨 1 비율: {metadata['death_in_pred_window_new'].mean():.3f}\")\n",
    "\n",
    "print(\"\\n관찰 완전성 분포:\")\n",
    "print(metadata['obs_completeness'].describe())\n",
    "\n",
    "# ===== 2. 원본 데이터 로드 =====\n",
    "print(\"\\n===== 원본 데이터 로드 =====\")\n",
    "original_data = pd.read_csv(\"hourly_bins/all_features_hourly_bins_inner.csv\")\n",
    "print(f\"원본 데이터 크기: {original_data.shape}\")\n",
    "\n",
    "# 메타데이터 컬럼 제외하고 피처 컬럼만 추출\n",
    "exclude_cols = ['subject_id', 'stay_id', 'hour_from_intime', 'hadm_id', 'bin_start', 'bin_end']\n",
    "feature_cols = [c for c in original_data.columns if c not in exclude_cols]\n",
    "print(f\"처리할 피처 수: {len(feature_cols)}\")\n",
    "\n",
    "# 결측값 현황 확인\n",
    "missing_before = original_data[feature_cols].isnull().sum().sum()\n",
    "total_values = len(original_data) * len(feature_cols)\n",
    "print(f\"결측값 현황: {missing_before:,} / {total_values:,} ({missing_before/total_values*100:.2f}%)\")\n",
    "\n",
    "# ===== 3. 불필요한 컬럼 제거 =====\n",
    "print(\"\\n===== 컬럼 정리 =====\")\n",
    "\n",
    "# 처리된 데이터는 원본 데이터를 그대로 사용 (결측값 보간 제거)\n",
    "processed_data = original_data.copy()\n",
    "\n",
    "# 제거할 컬럼들 정의\n",
    "cols_to_remove = ['temp_min', 'temp_max', 'gcs_min', 'gcs_max']\n",
    "\n",
    "# old_label 관련 컬럼들도 찾아서 제거 (컬럼명에 'old' 또는 'old_label'이 포함된 것들)\n",
    "old_label_cols = [col for col in processed_data.columns if 'old' in col.lower()]\n",
    "cols_to_remove.extend(old_label_cols)\n",
    "\n",
    "# 실제 존재하는 컬럼만 필터링\n",
    "existing_cols_to_remove = [col for col in cols_to_remove if col in processed_data.columns]\n",
    "print(f\"제거할 컬럼들: {existing_cols_to_remove}\")\n",
    "\n",
    "# 컬럼 제거\n",
    "if existing_cols_to_remove:\n",
    "    processed_data = processed_data.drop(columns=existing_cols_to_remove)\n",
    "    print(f\"제거된 컬럼 수: {len(existing_cols_to_remove)}\")\n",
    "else:\n",
    "    print(\"제거할 컬럼이 존재하지 않습니다.\")\n",
    "\n",
    "print(f\"최종 데이터 크기: {processed_data.shape}\")\n",
    "\n",
    "# ===== 4. 메타데이터에서도 old_label 관련 컬럼 제거 =====\n",
    "print(\"\\n메타데이터에서 old_label 관련 컬럼 제거...\")\n",
    "metadata_old_cols = [col for col in metadata.columns if 'old' in col.lower()]\n",
    "if metadata_old_cols:\n",
    "    metadata_cleaned = metadata.drop(columns=metadata_old_cols)\n",
    "    print(f\"메타데이터에서 제거된 컬럼: {metadata_old_cols}\")\n",
    "else:\n",
    "    metadata_cleaned = metadata.copy()\n",
    "    print(\"메타데이터에서 제거할 old_label 관련 컬럼이 없습니다.\")\n",
    "\n",
    "# ===== 5. 처리된 데이터로 새로운 시퀀스 생성 =====\n",
    "print(\"\\n===== 새로운 시퀀스 데이터 생성 =====\")\n",
    "\n",
    "# 새로운 피처 컬럼 목록 (제거된 컬럼 반영)\n",
    "new_exclude_cols = ['subject_id', 'stay_id', 'hour_from_intime', 'hadm_id', 'bin_start', 'bin_end']\n",
    "new_feature_cols = [c for c in processed_data.columns if c not in new_exclude_cols]\n",
    "print(f\"새로운 피처 수: {len(new_feature_cols)}\")\n",
    "\n",
    "# 코호트 데이터 다시 로드\n",
    "cohort = pd.read_csv(\"cohort.csv\")\n",
    "cohort['intime'] = pd.to_datetime(cohort['intime'])\n",
    "cohort['outtime'] = pd.to_datetime(cohort['outtime'])\n",
    "cohort['deathtime'] = pd.to_datetime(cohort['deathtime'])\n",
    "\n",
    "# 병합\n",
    "processed_data_with_cohort = processed_data.merge(\n",
    "    cohort[['subject_id','stay_id','intime','outtime','deathtime']],\n",
    "    on=['subject_id','stay_id'], how='inner'\n",
    ")\n",
    "\n",
    "# 새로운 시퀀스 생성\n",
    "obs_window_hours = 18\n",
    "new_sequences = []\n",
    "\n",
    "processed_grouped = processed_data_with_cohort.groupby('subject_id')\n",
    "\n",
    "for _, row in metadata_cleaned.iterrows():\n",
    "    subject_id = row['subject_id']\n",
    "    obs_start = row['obs_start_hour']\n",
    "    obs_end = row['obs_end_hour']\n",
    "    \n",
    "    # 해당 환자 데이터 가져오기\n",
    "    subject_data = processed_grouped.get_group(subject_id).sort_values('hour_from_intime').reset_index(drop=True)\n",
    "    \n",
    "    # 관찰 구간 데이터 추출\n",
    "    obs_mask = (subject_data['hour_from_intime'] >= obs_start) & \\\n",
    "               (subject_data['hour_from_intime'] < obs_end)\n",
    "    obs_window = subject_data[obs_mask]\n",
    "    \n",
    "    # 시간별 데이터 생성\n",
    "    hour_range = np.arange(obs_start, obs_end)\n",
    "    obs_window_indexed = obs_window.set_index('hour_from_intime')\n",
    "    \n",
    "    sequence_data = []\n",
    "    for hour in hour_range:\n",
    "        if hour in obs_window_indexed.index:\n",
    "            hour_features = obs_window_indexed.loc[hour, new_feature_cols].values\n",
    "        else:\n",
    "            # 해당 시간의 데이터가 없는 경우 NaN으로 채움\n",
    "            hour_features = np.full(len(new_feature_cols), np.nan)\n",
    "        sequence_data.append(hour_features.tolist())\n",
    "    \n",
    "    new_sequences.append(np.array(sequence_data))\n",
    "\n",
    "new_sequences_array = np.array(new_sequences)\n",
    "print(f\"새로운 시퀀스 데이터 형태: {new_sequences_array.shape}\")\n",
    "\n",
    "# ===== 6. 최종 파일 저장 =====\n",
    "print(\"\\n===== 최종 파일 저장 =====\")\n",
    "\n",
    "# 처리된 원본 데이터 저장\n",
    "processed_data.to_csv('processed_hourly_data.csv', index=False)\n",
    "print(\"처리된 시간별 데이터 저장: processed_hourly_data.csv\")\n",
    "\n",
    "# 정리된 메타데이터 저장\n",
    "metadata_cleaned.to_csv('cleaned_sliding_windows_metadata.csv', index=False)\n",
    "print(\"정리된 메타데이터 저장: cleaned_sliding_windows_metadata.csv\")\n",
    "\n",
    "# 새로운 시퀀스 데이터 저장\n",
    "np.save('processed_sliding_windows_sequences.npy', new_sequences_array)\n",
    "print(\"처리된 시퀀스 데이터 저장: processed_sliding_windows_sequences.npy\")\n",
    "\n",
    "# 피처 이름 저장\n",
    "with open('feature_names.txt', 'w') as f:\n",
    "    for feature in new_feature_cols:\n",
    "        f.write(feature + '\\n')\n",
    "print(\"피처 이름 저장: feature_names.txt\")\n",
    "\n",
    "# ===== 7. 최종 요약 =====\n",
    "print(f\"\\n===== 최종 처리 요약 =====\")\n",
    "print(f\"원본 데이터: {original_data.shape[0]:,}행 × {original_data.shape[1]}열\")\n",
    "print(f\"처리된 데이터: {processed_data.shape[0]:,}행 × {processed_data.shape[1]}열\")\n",
    "print(f\"제거된 컬럼 수: {len(existing_cols_to_remove)}\")\n",
    "print(f\"남은 피처 수: {len(new_feature_cols)}\")\n",
    "print(f\"결측값 현황: {missing_before:,} (보간 없이 유지)\")\n",
    "print(f\"윈도우 수: {len(metadata_cleaned)}\")\n",
    "print(f\"최종 시퀀스 형태: {new_sequences_array.shape[0]}윈도우 × {new_sequences_array.shape[1]}시간 × {new_sequences_array.shape[2]}피처\")\n",
    "\n",
    "# 각 파일별 결측값 현황\n",
    "print(f\"\\n===== 최종 결측값 현황 =====\")\n",
    "final_missing = new_sequences_array.size - np.count_nonzero(~np.isnan(new_sequences_array))\n",
    "final_total = new_sequences_array.size\n",
    "print(f\"최종 시퀀스 결측값: {final_missing:,} / {final_total:,} ({final_missing/final_total*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n모든 처리가 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb74e88",
   "metadata": {},
   "source": [
    "## 시계열 + 정적 피쳐 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192dbb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시계열 데이터 로드...\n",
      "시계열 shape: (352165, 18, 29)\n",
      "메타데이터, cohort 로드...\n",
      "정적 피처 병합...\n",
      "정적 피처 시간축 복사...\n",
      "시계열 + 정적 피처 통합...\n",
      "\n",
      "===== 저장 완료 =====\n",
      "통합 입력 shape: (352165, 18, 32)  # (윈도우 수, 시간스텝 수, 전체피처수)\n",
      "라벨 shape: (352165,)\n",
      "피처 이름 수: 32\n",
      "파일:\n",
      " - tcn_input_combined.npy          (X)\n",
      " - tcn_labels.npy                  (y)\n",
      " - tcn_metadata_with_static.csv    (메타데이터 + 정적)\n",
      " - tcn_feature_names.txt           (피처 이름 목록)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ===== 1. 파라미터 설정 =====\n",
    "seq_file = 'processed_sliding_windows_sequences.npy'  # 기존 시계열 .npy\n",
    "meta_file = 'cleaned_sliding_windows_metadata.csv'    # 메타데이터\n",
    "cohort_file = 'cohort.csv'                            # 정적 피처 파일\n",
    "static_cols = ['age', 'gender_M', 'gender_F']  # 병합할 정적 피처 컬럼명\n",
    "\n",
    "# ===== 2. 시계열 데이터 로드 =====\n",
    "print(\"시계열 데이터 로드...\")\n",
    "X_seq = np.load(seq_file)  # shape: (N, T, F_temporal)\n",
    "N, T, F_temporal = X_seq.shape\n",
    "print(f\"시계열 shape: {X_seq.shape}\")\n",
    "\n",
    "# ===== 3. 메타데이터 & cohort 로드 =====\n",
    "print(\"메타데이터, cohort 로드...\")\n",
    "meta_df = pd.read_csv(meta_file)   # (N행, subject_id & stay_id 포함)\n",
    "cohort_df = pd.read_csv(cohort_file)\n",
    "\n",
    "# ===== 4. cohort에서 필요한 정적 피처만 추출 =====\n",
    "static_df = cohort_df[['subject_id', 'stay_id'] + static_cols]\n",
    "\n",
    "# ===== 5. subject_id, stay_id 기준 병합 =====\n",
    "print(\"정적 피처 병합...\")\n",
    "meta_with_static = meta_df.merge(\n",
    "    static_df,\n",
    "    on=['subject_id', 'stay_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# ===== 6. 정적 피처 행렬 생성 =====\n",
    "X_static = meta_with_static[static_cols].to_numpy()  # (N, F_static)\n",
    "F_static = X_static.shape[1]\n",
    "\n",
    "# ===== 7. 모든 시간스텝에 복사\n",
    "print(\"정적 피처 시간축 복사...\")\n",
    "X_static_expanded = np.repeat(X_static[:, np.newaxis, :], T, axis=1)  # (N, T, F_static)\n",
    "\n",
    "# ===== 8. 시계열 + 정적 concat\n",
    "print(\"시계열 + 정적 피처 통합...\")\n",
    "X_combined = np.concatenate([X_seq, X_static_expanded], axis=2)  # (N, T, F_temporal+F_static)\n",
    "\n",
    "# ===== 9. 라벨 추출 =====\n",
    "y = meta_with_static['death_in_pred_window_new'].to_numpy()  # (N,)\n",
    "\n",
    "# ===== 10. 저장 =====\n",
    "np.save('tcn_input_combined.npy', X_combined)\n",
    "np.save('tcn_labels.npy', y)\n",
    "meta_with_static.to_csv('tcn_metadata_with_static.csv', index=False)\n",
    "\n",
    "# ===== 11. 피처 이름 저장 =====\n",
    "# 기존 temporal feature 이름 불러오기\n",
    "with open('feature_names.txt', 'r') as f:\n",
    "    temporal_feature_names = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "# 정적 피처 포함\n",
    "combined_feature_names = temporal_feature_names + static_cols\n",
    "\n",
    "# 저장\n",
    "with open('tcn_feature_names.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(combined_feature_names))\n",
    "\n",
    "\n",
    "print(\"\\n===== 저장 완료 =====\")\n",
    "print(f\"통합 입력 shape: {X_combined.shape}  # (윈도우 수, 시간스텝 수, 전체피처수)\")\n",
    "print(f\"라벨 shape: {y.shape}\")\n",
    "print(f\"피처 이름 수: {len(combined_feature_names)}\")\n",
    "print(\"파일:\")\n",
    "print(\" - tcn_input_combined.npy          (X)\")\n",
    "print(\" - tcn_labels.npy                  (y)\")\n",
    "print(\" - tcn_metadata_with_static.csv    (메타데이터 + 정적)\")\n",
    "print(\" - tcn_feature_names.txt           (피처 이름 목록)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e4f96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 파일 로드 ===\n",
      "X shape: (352165, 18, 32)  # (윈도우 수, 시간스텝 수, 전체 피처 수)\n",
      "y shape: (352165,)  # (윈도우 수,)\n",
      "\n",
      "=== 결측값 및 무한대 값 검증 ===\n",
      "X NaN 개수: 26,174,390\n",
      "X inf 개수: 0\n",
      "y NaN 개수: 0\n",
      "y inf 개수: 0\n",
      "\n",
      "=== 값 범위 ===\n",
      "X min: -32.0000, X max: 8999090.0000\n",
      "y unique 값: [0 1]\n",
      "\n",
      "=== 샘플 확인 (첫 윈도우 첫 타임스텝) ===\n",
      "[106.         105.         105.         107.          56.5\n",
      "  50.          50.          63.          67.5         64.\n",
      "  64.          71.          78.          80.          75.\n",
      "  80.          36.5         36.5         98.          94.\n",
      "  94.         100.          24.33333333  27.          23.\n",
      "  27.          15.          15.           0.          86.\n",
      "   0.           1.        ]\n",
      "y[0]: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ===== 1. 파일 로드 =====\n",
    "print(\"=== 파일 로드 ===\")\n",
    "X = np.load('tcn_input_combined.npy')\n",
    "y = np.load('tcn_labels.npy')\n",
    "\n",
    "# ===== 2. 기본 정보 출력 =====\n",
    "print(f\"X shape: {X.shape}  # (윈도우 수, 시간스텝 수, 전체 피처 수)\")\n",
    "print(f\"y shape: {y.shape}  # (윈도우 수,)\")\n",
    "\n",
    "# ===== 3. NaN/inf 여부 확인 =====\n",
    "print(\"\\n=== 결측값 및 무한대 값 검증 ===\")\n",
    "nan_count_X = np.isnan(X).sum()\n",
    "nan_count_y = np.isnan(y).sum()\n",
    "inf_count_X = np.isinf(X).sum()\n",
    "inf_count_y = np.isinf(y).sum()\n",
    "\n",
    "print(f\"X NaN 개수: {nan_count_X:,}\")\n",
    "print(f\"X inf 개수: {inf_count_X:,}\")\n",
    "print(f\"y NaN 개수: {nan_count_y:,}\")\n",
    "print(f\"y inf 개수: {inf_count_y:,}\")\n",
    "\n",
    "# ===== 4. 값 범위 간단 확인 =====\n",
    "print(\"\\n=== 값 범위 ===\")\n",
    "print(f\"X min: {np.nanmin(X):.4f}, X max: {np.nanmax(X):.4f}\")\n",
    "print(f\"y unique 값: {np.unique(y)}\")\n",
    "\n",
    "# ===== 5. 샘플 데이터 출력 =====\n",
    "print(\"\\n=== 샘플 확인 (첫 윈도우 첫 타임스텝) ===\")\n",
    "print(X[0, 0, :])\n",
    "print(f\"y[0]: {y[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4ac42a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== NaN 결측값 확인 =====\n",
      "시퀀스 데이터 형태: (352165, 18, 32)\n",
      "NaN 값 존재 여부: True\n",
      "전체 값 개수: 202,847,040\n",
      "NaN 값 개수: 26,174,390\n",
      "NaN 비율: 12.90%\n",
      "\n",
      "=== 차원별 NaN 분포 ===\n",
      "윈도우별 NaN 개수 (상위 5개):\n",
      "  윈도우 50158: 522개\n",
      "  윈도우 130954: 498개\n",
      "  윈도우 199100: 492개\n",
      "  윈도우 146266: 478개\n",
      "  윈도우 332756: 461개\n",
      "\n",
      "시간별 NaN 개수 (상위 5개):\n",
      "  시간 0: 1,583,524개\n",
      "  시간 1: 1,508,025개\n",
      "  시간 16: 1,498,222개\n",
      "  시간 17: 1,494,653개\n",
      "  시간 15: 1,476,256개\n",
      "\n",
      "피처별 NaN 개수 (상위 5개):\n",
      "  피처 28: 4,497,147개\n",
      "  피처 27: 4,497,147개\n",
      "  피처 26: 4,497,147개\n",
      "  피처 16: 4,430,759개\n",
      "  피처 17: 4,430,759개\n",
      "\n",
      "*** 결론: 마스킹이 필수입니다! ***\n",
      "이유: NaN 값이 존재하면 PyTorch/TensorFlow에서 gradient 계산 시 문제가 발생합니다.\n",
      "\n",
      "=== 샘플 데이터 확인 ===\n",
      "처음 3개 윈도우의 첫 번째 시간 스텝:\n",
      "윈도우 0: [106.  105.  105.  107.   56.5]\n",
      "  -> 정상 데이터\n",
      "윈도우 1: [138. 138. 138. 138.  71.]\n",
      "  -> 정상 데이터\n",
      "윈도우 2: [114.  nan 114. 114.  56.]\n",
      "  -> NaN 포함!\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"===== NaN 결측값 확인 =====\")\n",
    "\n",
    "# 시퀀스 데이터 로드\n",
    "sequences = np.load('tcn_input_combined.npy')\n",
    "print(f\"시퀀스 데이터 형태: {sequences.shape}\")\n",
    "\n",
    "# 1. NaN 값 존재 여부 확인\n",
    "has_nan = np.isnan(sequences).any()\n",
    "print(f\"NaN 값 존재 여부: {has_nan}\")\n",
    "\n",
    "if has_nan:\n",
    "    # 2. NaN 값의 개수와 비율\n",
    "    total_values = sequences.size\n",
    "    nan_count = np.isnan(sequences).sum()\n",
    "    nan_ratio = nan_count / total_values * 100\n",
    "    \n",
    "    print(f\"전체 값 개수: {total_values:,}\")\n",
    "    print(f\"NaN 값 개수: {nan_count:,}\")\n",
    "    print(f\"NaN 비율: {nan_ratio:.2f}%\")\n",
    "    \n",
    "    # 3. 각 차원별 NaN 분포\n",
    "    print(f\"\\n=== 차원별 NaN 분포 ===\")\n",
    "    print(f\"윈도우별 NaN 개수 (상위 5개):\")\n",
    "    nan_per_window = np.isnan(sequences).sum(axis=(1,2))\n",
    "    top_5_windows = np.argsort(nan_per_window)[-5:][::-1]\n",
    "    for i, window_idx in enumerate(top_5_windows):\n",
    "        print(f\"  윈도우 {window_idx}: {nan_per_window[window_idx]:,}개\")\n",
    "    \n",
    "    print(f\"\\n시간별 NaN 개수 (상위 5개):\")\n",
    "    nan_per_timestep = np.isnan(sequences).sum(axis=(0,2))\n",
    "    top_5_timesteps = np.argsort(nan_per_timestep)[-5:][::-1]\n",
    "    for i, time_idx in enumerate(top_5_timesteps):\n",
    "        print(f\"  시간 {time_idx}: {nan_per_timestep[time_idx]:,}개\")\n",
    "    \n",
    "    print(f\"\\n피처별 NaN 개수 (상위 5개):\")\n",
    "    nan_per_feature = np.isnan(sequences).sum(axis=(0,1))\n",
    "    top_5_features = np.argsort(nan_per_feature)[-5:][::-1]\n",
    "    for i, feature_idx in enumerate(top_5_features):\n",
    "        print(f\"  피처 {feature_idx}: {nan_per_feature[feature_idx]:,}개\")\n",
    "    \n",
    "    print(f\"\\n*** 결론: 마스킹이 필수입니다! ***\")\n",
    "    print(\"이유: NaN 값이 존재하면 PyTorch/TensorFlow에서 gradient 계산 시 문제가 발생합니다.\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n*** 결론: 마스킹이 필수가 아닙니다 ***\")\n",
    "    print(\"이유: NaN 값이 없으므로 직접 모델에 입력 가능합니다.\")\n",
    "\n",
    "# 4. 샘플 데이터 확인 (처음 3개 윈도우의 첫 번째 시간 스텝)\n",
    "print(f\"\\n=== 샘플 데이터 확인 ===\")\n",
    "print(\"처음 3개 윈도우의 첫 번째 시간 스텝:\")\n",
    "for i in range(min(3, sequences.shape[0])):\n",
    "    sample_data = sequences[i, 0, :5]  # 첫 5개 피처만 출력\n",
    "    print(f\"윈도우 {i}: {sample_data}\")\n",
    "    if np.isnan(sample_data).any():\n",
    "        print(f\"  -> NaN 포함!\")\n",
    "    else:\n",
    "        print(f\"  -> 정상 데이터\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ffca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== 환자별 클래스 불균형 확인 =====\n",
      "전체 윈도우 수: 352,165\n",
      "고유 환자 수: 42,979\n",
      "\n",
      "=== 환자별 클래스 개수 ===\n",
      "생존 환자: 41,703명 (97.0%)\n",
      "사망 환자: 1,276명 (3.0%)\n",
      "\n",
      "불균형 비율: 32.7:1 (생존:사망)\n",
      "\n",
      "=== 윈도우 vs 환자 비교 ===\n",
      "윈도우 기준 사망 비율: 0.4%\n",
      "환자 기준 사망 비율: 3.0%\n",
      "환자당 평균 윈도우 수: 8.2개\n",
      "\n",
      "=== 환자군별 윈도우 분포 ===\n",
      "생존 환자 윈도우: 333,495개 (평균 8.0개/환자)\n",
      "사망 환자 윈도우: 18,670개 (평균 14.6개/환자)\n",
      "\n",
      "=== 환자 기준 불균형 평가 ===\n",
      "소수 클래스(환자) 비율: 3.0%\n",
      "평가: 심각한 불균형\n",
      "\n",
      "=== 추천 처리 방법 ===\n",
      "- 환자 레벨에서 SMOTE 적용\n",
      "- 강한 클래스 가중치 필요\n",
      "- Focal Loss 권장\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 클래스 불균형 확인\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(\"===== 환자별 클래스 불균형 확인 =====\")\n",
    "\n",
    "# 1. 메타데이터 로드 (환자 ID 정보 포함)\n",
    "meta_df = pd.read_csv('tcn_metadata_with_static.csv')\n",
    "print(f\"전체 윈도우 수: {len(meta_df):,}\")\n",
    "\n",
    "# 2. 환자별 라벨 집계 (환자당 하나의 라벨만)\n",
    "# 같은 환자의 여러 윈도우 중 하나라도 사망이면 사망으로 분류\n",
    "patient_labels = meta_df.groupby('subject_id')['death_in_pred_window_new'].max()\n",
    "print(f\"고유 환자 수: {len(patient_labels):,}\")\n",
    "\n",
    "# 3. 환자별 클래스 개수 계산\n",
    "survival_patients = np.sum(patient_labels == 0)  # 생존 환자\n",
    "death_patients = np.sum(patient_labels == 1)     # 사망 환자\n",
    "\n",
    "print(f\"\\n=== 환자별 클래스 개수 ===\")\n",
    "print(f\"생존 환자: {survival_patients:,}명 ({survival_patients/len(patient_labels)*100:.1f}%)\")\n",
    "print(f\"사망 환자: {death_patients:,}명 ({death_patients/len(patient_labels)*100:.1f}%)\")\n",
    "\n",
    "# 4. 불균형 비율 계산\n",
    "if death_patients > 0:\n",
    "    imbalance_ratio = survival_patients / death_patients\n",
    "    print(f\"\\n불균형 비율: {imbalance_ratio:.1f}:1 (생존:사망)\")\n",
    "else:\n",
    "    print(\"\\n사망 환자가 없습니다!\")\n",
    "\n",
    "# 5. 윈도우 vs 환자 비교\n",
    "y = np.load('tcn_labels.npy')\n",
    "window_death_ratio = np.mean(y) * 100\n",
    "patient_death_ratio = (death_patients / len(patient_labels)) * 100\n",
    "\n",
    "print(f\"\\n=== 윈도우 vs 환자 비교 ===\")\n",
    "print(f\"윈도우 기준 사망 비율: {window_death_ratio:.1f}%\")\n",
    "print(f\"환자 기준 사망 비율: {patient_death_ratio:.1f}%\")\n",
    "\n",
    "# 6. 환자당 평균 윈도우 수\n",
    "windows_per_patient = meta_df.groupby('subject_id').size()\n",
    "avg_windows = windows_per_patient.mean()\n",
    "print(f\"환자당 평균 윈도우 수: {avg_windows:.1f}개\")\n",
    "\n",
    "# 7. 생존/사망 환자별 윈도우 수 비교\n",
    "survival_patient_ids = patient_labels[patient_labels == 0].index\n",
    "death_patient_ids = patient_labels[patient_labels == 1].index\n",
    "\n",
    "survival_windows = meta_df[meta_df['subject_id'].isin(survival_patient_ids)].shape[0]\n",
    "death_windows = meta_df[meta_df['subject_id'].isin(death_patient_ids)].shape[0]\n",
    "\n",
    "print(f\"\\n=== 환자군별 윈도우 분포 ===\")\n",
    "print(f\"생존 환자 윈도우: {survival_windows:,}개 (평균 {survival_windows/survival_patients:.1f}개/환자)\")\n",
    "print(f\"사망 환자 윈도우: {death_windows:,}개 (평균 {death_windows/death_patients:.1f}개/환자)\")\n",
    "\n",
    "# 8. 불균형 정도 판단 (환자 기준)\n",
    "minority_ratio = min(survival_patients, death_patients) / len(patient_labels)\n",
    "print(f\"\\n=== 환자 기준 불균형 평가 ===\")\n",
    "if minority_ratio < 0.1:\n",
    "    balance_level = \"심각한 불균형\"\n",
    "elif minority_ratio < 0.2:\n",
    "    balance_level = \"중간 불균형\"\n",
    "elif minority_ratio < 0.4:\n",
    "    balance_level = \"약간 불균형\"\n",
    "else:\n",
    "    balance_level = \"균형적\"\n",
    "\n",
    "print(f\"소수 클래스(환자) 비율: {minority_ratio*100:.1f}%\")\n",
    "print(f\"평가: {balance_level}\")\n",
    "\n",
    "# 9. 추천 방법 (환자 기준)\n",
    "print(f\"\\n=== 추천 처리 방법 ===\")\n",
    "if minority_ratio < 0.1:\n",
    "    print(\"- 환자 레벨에서 SMOTE 적용\")\n",
    "    print(\"- 강한 클래스 가중치 필요\")\n",
    "    print(\"- Focal Loss 권장\")\n",
    "elif minority_ratio < 0.3:\n",
    "    print(\"- 클래스 가중치 적용\")\n",
    "    print(\"- 환자별 균등 샘플링 고려\")\n",
    "else:\n",
    "    print(\"- 현재 상태로 훈련 가능\")\n",
    "    print(\"- 가벼운 클래스 가중치 적용\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
